import csv
import logging
import os
import random
import time
import requests
import re
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from scr.oc_base_function import oc_log_message, load_oc_settings, load_attributes_csv, save_attributes_csv, \
                                load_category_csv, save_category_csv, load_poznachky_csv

def find_change_art_shtrihcod():
    """
    –ü–µ—Ä–µ–≤—ñ—Ä—è—î —Ä–æ–∑–±—ñ–∂–Ω–æ—Å—Ç—ñ:
    1) –ø–æ —à—Ç—Ä–∏—Ö–∫–æ–¥—É (–ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É)
    2) –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É (–ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –ø–æ —à—Ç—Ä–∏—Ö–∫–æ–¥—É)
    –£—Å—ñ —Ä–æ–∑–±—ñ–∂–Ω–æ—Å—Ç—ñ –∑–∞–ø–∏—Å—É—î —É change_art_shtrihcod
    """

    oc_log_message("‚ñ∂ –°—Ç–∞—Ä—Ç –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –∞—Ä—Ç–∏–∫—É–ª—ñ–≤ —ñ —à—Ç—Ä–∏—Ö–∫–æ–¥—ñ–≤ (2 –Ω–∞–ø—Ä—è–º–∫–∏)")
    logging.info("find_change_art_shtrihcod START")

    settings = load_oc_settings()
    if not settings:
        oc_log_message("‚ùå settings.json –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ")
        return

    site_csv = settings["paths"]["output_file"]
    supplier_csv = settings["suppliers"]["1"]["csv_path"]
    result_csv = settings["paths"]["change_art_shtrihcod"]

    # --------------------------------------------------
    # 1. –ß–∏—Ç–∞—î–º–æ –ø—Ä–∞–π—Å –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞
    # --------------------------------------------------
    supplier_by_artykul = {}   # –ö–æ–¥_—Ç–æ–≤–∞—Ä–∞ -> –®—Ç—Ä–∏—Ö_–∫–æ–¥
    supplier_by_shtrih = {}    # –®—Ç—Ä–∏—Ö_–∫–æ–¥  -> –ö–æ–¥_—Ç–æ–≤–∞—Ä–∞

    with open(supplier_csv, newline="", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f, delimiter=";")

        for row in reader:
            artykul = row.get("–ö–æ–¥_—Ç–æ–≤–∞—Ä–∞", "").strip()
            shtrih = row.get("–®—Ç—Ä–∏—Ö_–∫–æ–¥", "").strip()

            if artykul and shtrih:
                supplier_by_artykul[artykul] = shtrih
                supplier_by_shtrih[shtrih] = artykul

    oc_log_message(
        f"‚Ñπ –¢–æ–≤–∞—Ä—ñ–≤ –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞: "
        f"{len(supplier_by_artykul)} (–ø–æ –∞—Ä—Ç–∏–∫—É–ª—É), "
        f"{len(supplier_by_shtrih)} (–ø–æ —à—Ç—Ä–∏—Ö–∫–æ–¥—É)"
    )

    # --------------------------------------------------
    # 2. –û—á–∏—â–µ–Ω–Ω—è —Ñ–∞–π–ª—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
    # --------------------------------------------------
    headers = [
        "sku",
        "shtrih_cod",
        "artykul_lutsk",
        "–ö–æ–¥_—Ç–æ–≤–∞—Ä–∞",
        "–®—Ç—Ä–∏—Ö_–∫–æ–¥"
    ]

    with open(result_csv, "w", newline="", encoding="utf-8") as f:
        csv.writer(f).writerow(headers)

    oc_log_message("üßπ change_art_shtrihcod –æ—á–∏—â–µ–Ω–æ")

    # --------------------------------------------------
    # 3. –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è (2 –Ω–∞–ø—Ä—è–º–∫–∏)
    # --------------------------------------------------
    checked = 0
    diff_count = 0
    written_keys = set()  # –∑–∞—Ö–∏—Å—Ç –≤—ñ–¥ –¥—É–±–ª—ñ–≤

    with open(site_csv, newline="", encoding="utf-8") as site_f, \
         open(result_csv, "a", newline="", encoding="utf-8") as out_f:

        site_reader = csv.DictReader(site_f)
        writer = csv.writer(out_f)

        for row in site_reader:
            sku = row.get("sku", "").strip()
            site_shtrih = row.get("shtrih_cod", "").strip()
            site_artykul = row.get("artykul_lutsk", "").strip()

            # ---------- –ü–†–ê–í–ò–õ–û 1 ----------
            # –Ñ –∞—Ä—Ç–∏–∫—É–ª ‚Üí –ø–æ—Ä—ñ–≤–Ω—é—î–º–æ —à—Ç—Ä–∏—Ö–∫–æ–¥
            if site_artykul and site_artykul in supplier_by_artykul:
                checked += 1
                supplier_shtrih = supplier_by_artykul[site_artykul]

                if site_shtrih != supplier_shtrih:
                    key = (sku, site_artykul, supplier_shtrih)
                    if key not in written_keys:
                        writer.writerow([
                            sku,
                            site_shtrih,
                            site_artykul,
                            site_artykul,
                            supplier_shtrih
                        ])
                        written_keys.add(key)
                        diff_count += 1

            # ---------- –ü–†–ê–í–ò–õ–û 2 ----------
            # –Ñ —à—Ç—Ä–∏—Ö–∫–æ–¥ ‚Üí –ø–æ—Ä—ñ–≤–Ω—é—î–º–æ –∞—Ä—Ç–∏–∫—É–ª
            if site_shtrih and site_shtrih in supplier_by_shtrih:
                checked += 1
                supplier_artykul = supplier_by_shtrih[site_shtrih]

                if site_artykul != supplier_artykul:
                    key = (sku, site_shtrih, supplier_artykul)
                    if key not in written_keys:
                        writer.writerow([
                            sku,
                            site_shtrih,
                            site_artykul,
                            supplier_artykul,
                            site_shtrih
                        ])
                        written_keys.add(key)
                        diff_count += 1

    # --------------------------------------------------
    # 4. –ü—ñ–¥—Å—É–º–æ–∫
    # --------------------------------------------------
    oc_log_message(f"‚úÖ –ü–µ—Ä–µ–≤—ñ—Ä–µ–Ω–æ –ø–æ–∑–∏—Ü—ñ–π: {checked}")
    oc_log_message(f"‚ö† –ó–Ω–∞–π–¥–µ–Ω–æ —Ä–æ–∑–±—ñ–∂–Ω–æ—Å—Ç–µ–π: {diff_count}")
    logging.info("find_change_art_shtrihcod END")

def find_new_products():
    """
    –ü–æ—Ä—ñ–≤–Ω—é—î –∞—Ä—Ç–∏–∫—É–ª–∏ —Ç–æ–≤–∞—Ä—ñ–≤ –∑ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç–∞ –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ –∑ –∞—Ä—Ç–∏–∫—É–ª–∞–º–∏,
    —â–æ —î –Ω–∞ —Å–∞–π—Ç—ñ, —ñ –∑–∞–ø–∏—Å—É—î –Ω–æ–≤—ñ —Ç–æ–≤–∞—Ä–∏ –≤ –æ–∫—Ä–µ–º–∏–π —Ñ–∞–π–ª.
    """
    # --- 1. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ª–æ–≥—É–≤–∞–Ω–Ω—è ---
    oc_log_message()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 1. –ü–æ—á–∏–Ω–∞—é –ø–æ—à—É–∫ –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤...")
    
    # --- 2. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å –∑ settings.json ---
    settings = load_oc_settings()
    
    # --- 3. –û—Ç—Ä–∏–º–∞–Ω–Ω—è —à–ª—è—Ö—ñ–≤ –¥–æ –ø–æ—Ç—Ä—ñ–±–Ω–∏—Ö —Ñ–∞–π–ª—ñ–≤ ---
    zalishki_path = settings['paths']['output_file']                   # –ë–∞–∑–∞ —ñ—Å–Ω—É—é—á–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤
    supliers_new_path = settings['paths']['csv_path_supliers_1_new']         # –§–∞–π–ª, –∫—É–¥–∏ –±—É–¥–µ –∑–∞–ø–∏—Å–∞–Ω–æ –Ω–æ–≤—ñ —Ç–æ–≤–∞—Ä–∏
    supliers_csv_path = settings['suppliers']['1']['csv_path']               # –ü—Ä–∞–π—Å-–ª–∏—Å—Ç –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ 1
    delimiter = settings['suppliers']['1']['delimiter']                      # –†–æ–∑–¥—ñ–ª—å–Ω–∏–∫ —É CSV
    
    # --- 4. –û—Ç—Ä–∏–º–∞–Ω–Ω—è –¥–æ–ø–æ–º—ñ–∂–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ ---
    sku_prefix = settings['suppliers']['1']['search']                        # –ü—Ä–µ—Ñ—ñ–∫—Å –¥–ª—è –ø–æ—à—É–∫—É
       
    # --- 5. –û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤ –Ω–æ–≤–æ–≥–æ —Ñ–∞–π–ª—É ---
    new_product_headers = [
        settings['column_supliers_1_new_name'][str(i)]
        for i in range(len(settings['column_supliers_1_new_name']))
    ]
    num_new_columns = len(new_product_headers)

    logging.info("–ó—á–∏—Ç—É—é —ñ—Å–Ω—É—é—á—ñ –∞—Ä—Ç–∏–∫—É–ª–∏ –∑ —Ñ–∞–π–ª—É, –≤–∫–∞–∑–∞–Ω–æ–≥–æ –∑–∞ –∫–ª—é—á–µ–º 'csv_path_zalishki'.")

    try:
        # --- 6. –ó—á–∏—Ç—É–≤–∞–Ω–Ω—è —ñ—Å–Ω—É—é—á–∏—Ö –∞—Ä—Ç–∏–∫—É–ª—ñ–≤ —ñ–∑ –±–∞–∑–∏ (zalishki.csv) ---
        with open(zalishki_path, mode='r', encoding='utf-8') as zalishki_file:
            zalishki_reader = csv.reader(zalishki_file)
            next(zalishki_reader, None)  # –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫
            existing_skus = {row[9].strip().lower() for row in zalishki_reader if len(row) > 9}
            logging.info(f"–ó—á–∏—Ç–∞–Ω–æ {len(existing_skus)} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –∞—Ä—Ç–∏–∫—É–ª—ñ–≤ —ñ–∑ –±–∞–∑–∏.")

        # --- 7. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –Ω–æ–≤–æ–≥–æ —Ñ–∞–π–ª—É –¥–ª—è –∑–∞–ø–∏—Å—É –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ ---
        logging.info("–í—ñ–¥–∫—Ä–∏–≤–∞—é —Ñ–∞–π–ª –¥–ª—è –∑–∞–ø–∏—Å—É –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤...")
        with open(supliers_new_path, mode='w', encoding='utf-8', newline='') as new_file:
            writer = csv.writer(new_file)
            writer.writerow(new_product_headers)  # –∑–∞–ø–∏—Å—É—î–º–æ –∑–∞–≥–æ–ª–æ–≤–∫–∏
            
            # --- 8. –ó—á–∏—Ç—É–≤–∞–Ω–Ω—è –ø—Ä–∞–π—Å-–ª–∏—Å—Ç–∞ –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ ---
            logging.info("–ü–æ—Ä—ñ–≤–Ω—é—é –¥–∞–Ω—ñ –∑ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç–æ–º –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ 1...")
            with open(supliers_csv_path, mode='r', encoding='utf-8') as supliers_file:
                supliers_reader = csv.reader(supliers_file, delimiter=delimiter)
                next(supliers_reader, None)  # –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫
                
                # --- 9. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ª—ñ—á–∏–ª—å–Ω–∏–∫–∞ ---
                new_products_count = 0

                # --- 10. –ì–æ–ª–æ–≤–Ω–∏–π —Ü–∏–∫–ª: –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∫–æ–∂–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä—É ---
                for row in supliers_reader:
                    if not row:
                        continue
                    
                    sku = row[0].strip().lower()
                    
                    # --- 11. –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Ç–æ–≤–∞—Ä –Ω–æ–≤–∏–π (–≤—ñ–¥—Å—É—Ç–Ω—ñ–π —É –±–∞–∑—ñ) ---
                    if sku and sku not in existing_skus:
                        
                        # --- 12. –§–æ—Ä–º—É—î–º–æ –Ω–æ–≤–∏–π —Ä—è–¥–æ–∫ –∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ—é SL_new.csv ---
                        new_row = [''] * num_new_columns
                        
                        # –î–æ–¥–∞—î–º–æ –ø—Ä–µ—Ñ—ñ–∫—Å –¥–æ SKU
                        sku_with_prefix = sku_prefix + row[0]
                        new_row[0] = sku_with_prefix

                        # --- 13. –ú–∞–ø—É–≤–∞–Ω–Ω—è –∫–æ–ª–æ–Ω–æ–∫ –∑ –ø—Ä–∞–π—Å—É —É –Ω–æ–≤–∏–π CSV ---
                        column_mapping = [
                            (18, 2),  # s(18) -> —Å(2)
                            (0, 5),   # a(0) -> f(5)
                            (1, 6),   # b(1) -> g(6)
                            (2, 7),   # c(2) -> h(7)
                            (3, 8),   # d(3) -> i(8)
                            (6, 9),   # g(6) -> j(9)
                            (7, 10),  # h(7) -> k(10)
                            (8, 11),  # i(8) -> l(11)
                            (9, 12),  # j(9) -> m(12)
                            (10, 13), # k(10) -> n(13)
                            (11, 14), # l(11) -> o(14)
                        ]
                        for source_index, dest_index in column_mapping:
                            if len(row) > source_index:
                                new_row[dest_index] = row[source_index]
                                
                        # --- 14. –î–æ–¥–∞—î–º–æ —É —Ñ–∞–π–ª –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ ---
                        new_products_count += 1
                        writer.writerow(new_row)

        # --- 17. –ü—ñ–¥—Å—É–º–∫–æ–≤–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è ---
        logging.info(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {new_products_count} –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤.")
        logging.info(f"–î–∞–Ω—ñ –∑–∞–ø–∏—Å–∞–Ω–æ —É —Ñ–∞–π–ª csv 'supliers_new_path'.")

    # --- 18. –û–±—Ä–æ–±–∫–∞ –ø–æ–º–∏–ª–æ–∫ ---
    except FileNotFoundError as e:
        logging.info(f"‚ùå –ü–æ–º–∏–ª–∫–∞: –§–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ - {e}")
    except Exception as e:
        logging.info(f"‚ùå –í–∏–Ω–∏–∫–ª–∞ –Ω–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")

def find_product_url():
    """
    –ó—á–∏—Ç—É—î —Ñ–∞–π–ª –∑ –Ω–æ–≤–∏–º–∏ —Ç–æ–≤–∞—Ä–∞–º–∏, –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç—å –∑–∞ URL-–∞–¥—Ä–µ—Å–æ—é,
    –∑–Ω–∞—Ö–æ–¥–∏—Ç—å URL-–∞–¥—Ä–µ—Å—É –ø—Ä–æ—Å—Ç–æ–≥–æ –∞–±–æ –≤–∞—Ä—ñ–∞—Ç–∏–≤–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä—É,
    —ñ –∑–∞–ø–∏—Å—É—î –∑–Ω–∞–π–¥–µ–Ω—É URL-–∞–¥—Ä–µ—Å—É –≤ –∫–æ–ª–æ–Ω–∫—É B(1) –≤ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª.
    """

    # --- 1. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ª–æ–≥—É–≤–∞–Ω–Ω—è (–ø—ñ–¥–∫–ª—é—á–∞—î–º–æ —ñ—Å–Ω—É—é—á–∏–π –ª–æ–≥-—Ñ–∞–π–ª) ---
    oc_log_message()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 2. –ü–æ—á–∏–Ω–∞—é –ø–æ—à—É–∫ URL-–∞–¥—Ä–µ—Å —Ç–æ–≤–∞—Ä—ñ–≤...")
    
    # --- 2. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å —Ç–∞ —Ñ–æ—Ä–º—É–≤–∞–Ω–Ω—è —à–ª—è—Ö—ñ–≤/—Ç–∏–º—á–∞—Å–æ–≤–æ–≥–æ —Ñ–∞–π–ª—É ---
    settings = load_oc_settings()
    supliers_new_path = settings['paths']['csv_path_supliers_1_new']  # –≤—Ö—ñ–¥–Ω–∏–π CSV (1.csv)
    site_url = settings['suppliers']['1']['site']                     # –±–∞–∑–æ–≤–∏–π URL —Å–∞–π—Ç—É (—â–æ–± –¥–æ–¥–∞–≤–∞—Ç–∏ –≤—ñ–¥–Ω–æ—Å–Ω—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è)
    temp_file_path = supliers_new_path + '.temp'                      # —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª –ø—ñ–¥ —á–∞—Å –∑–∞–ø–∏—Å—É

    # --- –õ—ñ—á–∏–ª—å–Ω–∏–∫–∏ —ñ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ---
    total_rows = 0
    found_variant_count = 0
    found_simple_count = 0
    not_found_count = 0
    found_variant_rows = []
    not_found_rows = []

    try:
        # --- 3. –í—ñ–¥–∫—Ä–∏–≤–∞—î–º–æ –≤—Ö—ñ–¥–Ω–∏–π —Ñ–∞–π–ª –¥–ª—è —á–∏—Ç–∞–Ω–Ω—è ---
        with open(supliers_new_path, mode='r', encoding='utf-8') as input_file:
            reader = csv.reader(input_file)
            headers = next(reader)  # —á–∏—Ç–∞—î–º–æ —ñ –∑–±–µ—Ä—ñ–≥–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–∫–∏ (—â–æ–± –ø–µ—Ä–µ–ø–∏—Å–∞—Ç–∏ –≤ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª)

            # --- 4. –í—ñ–¥–∫—Ä–∏–≤–∞—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª –¥–ª—è –ø–æ—Å—Ç—É–ø–æ–≤–æ–≥–æ –∑–∞–ø–∏—Å—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ ---
            with open(temp_file_path, mode='w', encoding='utf-8', newline='') as output_file:
                writer = csv.writer(output_file)
                writer.writerow(headers) # –∑–∞–ø–∏—Å—É—î–º–æ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —É —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª

                # --- 5. –Ü—Ç–µ—Ä–∞—Ü—ñ—è –ø–æ —Ä—è–¥–∫–∞—Ö –≤—Ö—ñ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª—É ---
                for idx, row in enumerate(reader):
                    total_rows += 1
                    # 5.1. –í–∏—Ç—è–≥—É—î–º–æ –∫–ª—é—á–æ–≤—ñ –ø–æ–ª—è —ñ–∑ —Ä—è–¥–∫–∞
                    search_url = row[0].strip()    # —É –≤–∏—Ö—ñ–¥–Ω–æ–º—É —Ñ–∞–π–ª—ñ —É –∫–æ–ª–æ–Ω—Ü—ñ A –º–æ–∂–µ –±—É—Ç–∏ "–ø–æ—Å–∏–ª–∞–Ω–Ω—è –¥–ª—è –ø–æ—à—É–∫—É"
                    file_sku = row[5].strip()      # –∞—Ä—Ç–∏–∫—É–ª (SKU) –∑ –∫–æ–ª–æ–Ω–∫–∏, —è–∫–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î —ñ–Ω–¥–µ–∫—Å—É 5

                    # --- 6. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –≤–∞–ª—ñ–¥–Ω–æ—Å—Ç—ñ URL –¥–ª—è –ø–æ—à—É–∫—É ---
                    # –Ø–∫—â–æ URL –ø—É—Å—Ç–∏–π –∞–±–æ –≤–∂–µ –ø–æ–∑–Ω–∞—á–µ–Ω–∏–π —è–∫ –ø–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Ç—É, –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ —Ä—è–¥–æ–∫
                    if not search_url or search_url.startswith('–ü–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Ç—É'):
                        writer.writerow(row)
                        continue

                    try:
                        # --- 7. –í–∏–∫–æ–Ω–∞–Ω–Ω—è HTTP-–∑–∞–ø–∏—Ç—É –¥–æ search_url —ñ –ø–∞—Ä—Å–∏–Ω–≥ HTML ---
                        response = requests.get(search_url)
                        response.raise_for_status()
                        soup = BeautifulSoup(response.text, 'html.parser')
                        found_type = None  # 'variant' –∞–±–æ 'simple'
                        found_url = None  # —Å—é–¥–∏ –∑–∞–ø–∏—à–µ–º–æ –∑–Ω–∞–π–¥–µ–Ω—É —Ä–µ–∞–ª—å–Ω—É URL-–∞–¥—Ä–µ—Å—É —Ç–æ–≤–∞—Ä—É
                        
                        # --- 8. –ü–æ—à—É–∫ –≤–∞—Ä—ñ–∞—Ç–∏–≤–Ω–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ (input.variant_control[data-code]) ---
                        # –®—É–∫–∞—î–º–æ input —Ç–µ–≥–∏ –∑ –∫–ª–∞—Å–æ–º variant_control —Ç–∞ –∞—Ç—Ä–∏–±—É—Ç–æ–º data-code,
                        # –ø–æ—Ä—ñ–≤–Ω—é—î–º–æ data-code –∑ file_sku ‚Äî —è–∫—â–æ —Å–ø—ñ–≤–ø–∞–¥—ñ–Ω–Ω—è, –±–µ—Ä–µ–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è —É –±–∞—Ç—å–∫—ñ–≤—Å—å–∫–æ–º—É –±–ª–æ—Ü—ñ.
                        variant_inputs = soup.find_all('input', class_='variant_control', attrs={'data-code': True})
                        for input_tag in variant_inputs:
                            site_sku = input_tag.get('data-code', '').strip()
                            if file_sku == site_sku:
                                parent_div = input_tag.find_parent('div', class_='card-block')
                                if parent_div:
                                    link_tag = parent_div.find('h4', class_='card-title').find('a')
                                    if link_tag and link_tag.has_attr('href'):
                                        # –§–æ—Ä–º—É—î–º–æ –ø–æ–≤–Ω–∏–π URL (–¥–æ–¥–∞—î–º–æ site_url –¥–æ –≤—ñ–¥–Ω–æ—Å–Ω–æ–≥–æ —à–ª—è—Ö—É)
                                        found_url = site_url + link_tag['href']
                                        found_type = 'variant'
                                        break

                        # --- 9. –Ø–∫—â–æ –Ω–µ –∑–Ω–∞–π—à–ª–∏ —Å–µ—Ä–µ–¥ –≤–∞—Ä—ñ–∞–Ω—Ç—ñ–≤ ‚Äî —à—É–∫–∞—î–º–æ –ø—Ä–æ—Å—Ç—ñ —Ç–æ–≤–∞—Ä–∏ ---
                        if not found_url:
                            # –î–ª—è –ø—Ä–æ—Å—Ç–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ —à—É–∫–∞—î–º–æ div –∑ –∫–ª–∞—Å–æ–º 'radio', –±–µ—Ä–µ–º–æ —Ç–µ–∫—Å—Ç —è–∫ SKU,
                            # —ñ –∑–∞ —Ç–∞–∫–∏–º –∂–µ –ø—ñ–¥—Ö–æ–¥–æ–º –∑–Ω–∞—Ö–æ–¥–∏–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è —É –±–ª–æ—Ü—ñ card-block.
                            simple_divs = soup.find_all('div', class_='radio')
                            for div_tag in simple_divs:
                                site_sku = div_tag.get_text(strip=True).strip()
                                if file_sku == site_sku:
                                    parent_div = div_tag.find_parent('div', class_='card-block')
                                    if parent_div:
                                        link_tag = parent_div.find('h4', class_='card-title').find('a')
                                        if link_tag and link_tag.has_attr('href'):
                                            found_url = site_url + link_tag['href']
                                            found_type = 'simple'
                                            break

                        # --- 10. –ó–∞–ø–∏—Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É —É –∫–æ–ª–æ–Ω–∫—É B (—ñ–Ω–¥–µ–∫—Å 1) –∞–±–æ –ª–æ–≥—É–≤–∞–Ω–Ω—è —è–∫—â–æ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ ---
                        if found_url:
                            row[1] = found_url
                            if found_type == 'variant':
                                found_variant_count += 1
                                found_variant_rows.append(idx + 2)  # +2, –±–æ —Ä—è–¥–∫–∏ CSV —Ä–∞—Ö—É—é—Ç—å—Å—è –∑ 1 + –∑–∞–≥–æ–ª–æ–≤–æ–∫
                            elif found_type == 'simple':
                                found_simple_count += 1
                        else:
                            not_found_count += 1
                            not_found_rows.append(idx + 2)

                        # –ó–∞–ø–∏—Å—É—î–º–æ (–∑–Ω–∞–π–¥–µ–Ω–∏–π –∞–±–æ –Ω–µ–∑–º—ñ–Ω–µ–Ω–∏–π) —Ä—è–¥–æ–∫ —É —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª
                        writer.writerow(row)

                    except requests.RequestException as e:
                        # --- 11. –û–±—Ä–æ–±–∫–∞ –ø–æ–º–∏–ª–æ–∫ HTTP-–∑–∞–ø–∏—Ç—É: –ª–æ–≥—É–≤–∞–Ω–Ω—è —Ç–∞ –º–∞—Ä–∫—É–≤–∞–Ω–Ω—è —Ä—è–¥–∫–∞ ---
                        logging.error(f"–†—è–¥–æ–∫ {idx + 2}: –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Ç—ñ –¥–æ —É—Ä–ª: {e}")
                        row[0] = f'–ü–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Ç—É: {e}'  # –ø–æ–∑–Ω–∞—á–∞—î–º–æ –ø–æ–ª–µ –ø–æ—à—É–∫—É —è–∫ –ø–æ–º–∏–ª–∫–æ–≤–µ
                        writer.writerow(row)
                    
                    # --- 12. –î–æ–¥–∞—Ç–∫–æ–≤–∞ –ø–∞—É–∑–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏ (—Ä–∞–Ω–¥–æ–º—ñ–∑–æ–≤–∞–Ω–∞) –¥–ª—è —É–Ω–∏–∫–Ω–µ–Ω–Ω—è –±–∞–Ω–∞/DDOS ---
                    time.sleep(random.uniform(1, 3))
  
        # --- 13. –ü—ñ—Å–ª—è —É—Å–ø—ñ—à–Ω–æ—ó –æ–±—Ä–æ–±–∫–∏: –∑–∞–º—ñ–Ω–∞ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª—É —Ç–∏–º—á–∞—Å–æ–≤–∏–º ---
        os.replace(temp_file_path, supliers_new_path)

        # --- 14. –ó–≤–µ–¥–µ–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ---
        logging.info("=== –ü–Ü–î–°–£–ú–ö–û–í–ê –Ü–ù–§–û–†–ú–ê–¶–Ü–Ø ===")
        logging.info(f"–í—Å—å–æ–≥–æ —Ä—è–¥–∫—ñ–≤ –∑ —Ç–æ–≤–∞—Ä–∞–º–∏: {total_rows}")
        logging.info(
            f"–ó–Ω–∞–π–¥–µ–Ω–æ URL –≤–∞—Ä—ñ–∞—Ç–∏–≤–Ω–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤: {found_variant_count}"
            + (f" (–†—è–¥–∫–∏ {', '.join(map(str, found_variant_rows))})" if found_variant_rows else "")
        )
        logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ URL –ø—Ä–æ—Å—Ç–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤: {found_simple_count}")
        logging.info(
            f"–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ URL: {not_found_count}"
            + (f" (–†—è–¥–∫–∏ {', '.join(map(str, not_found_rows))})" if not_found_rows else "")
        )

    except FileNotFoundError as e:
        # --- 15. –û–±—Ä–æ–±–∫–∞ –ø–æ–º–∏–ª–∫–∏: –≤—Ö—ñ–¥–Ω–∏–π —Ñ–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ ---
        logging.error(f"–ü–æ–º–∏–ª–∫–∞: –§–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ - {e}")
    except Exception as e:
        # --- 16. –ì–∞—Ä–∞–Ω—Ç—ñ–π–Ω–µ –ø—Ä–∏–±–∏—Ä–∞–Ω–Ω—è: –≤–∏–¥–∞–ª—è—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª –ø—Ä–∏ –ø–æ–º–∏–ª—Ü—ñ, —â–æ–± –Ω–µ –∑–∞–ª–∏—à–∏—Ç–∏ —Å–º—ñ—Ç—Ç—è ---
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)
        logging.error(f"–í–∏–Ω–∏–∫–ª–∞ –Ω–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")

def parse_product_attributes():
    """
    –ü–∞—Ä—Å–∏—Ç—å —Å—Ç–æ—Ä—ñ–Ω–∫–∏ —Ç–æ–≤–∞—Ä—ñ–≤, –∑–∞—Å—Ç–æ—Å–æ–≤—É—î –∑–∞–º—ñ–Ω—É –∑ attribute.csv (–±–ª–æ—á–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞)
    —ñ –¥–æ–¥–∞—î –Ω–æ–≤—ñ –Ω–µ–≤—ñ–¥–æ–º—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –æ–¥—Ä–∞–∑—É –ø–µ—Ä–µ–¥ –Ω–∞—Å—Ç—É–ø–Ω–∏–º –±–ª–æ–∫–æ–º-–∑–∞–≥–æ–ª–æ–≤–∫–æ–º.

    """

    # --- 0. –õ–æ–≥ —Å—Ç–∞—Ä—Ç—É ---
    oc_log_message("‚ñ∂ –§–£–ù–ö–¶–Ü–Ø: –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ —Ç–æ–≤–∞—Ä—ñ–≤ (–±–µ–∑ —à—Ç—Ä–∏—Ö-–∫–æ–¥—ñ–≤)")
    logging.info("–ü–æ—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç–æ—Ä—ñ–Ω–æ–∫ —Ç–æ–≤–∞—Ä—ñ–≤ –¥–ª—è –≤–∏–ª—É—á–µ–Ω–Ω—è –∞—Ç—Ä–∏–±—É—Ç—ñ–≤")

    # --- 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å ---
    settings = load_oc_settings()
    try:
        supliers_new_path = settings["paths"]["csv_path_supliers_1_new"]
        product_data_map = settings["suppliers"]["1"]["product_data_columns"]
        other_attrs_index = settings["suppliers"]["1"]["other_attributes_column"]
    except (TypeError, KeyError) as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –¥–æ—Å—Ç—É–ø—É –¥–æ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å settings.json: {e}")
        return

    # --- 2. –ú–∞–ø–∞ –æ–±—Ä–æ–±–ª—é–≤–∞–Ω–∏—Ö –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ (–±–µ–∑ —à—Ç—Ä–∏—Ö-–∫–æ–¥—É) ---
    processing_map = {
        attr_name: col_index
        for attr_name, col_index in product_data_map.items()
        if attr_name != "–®—Ç—Ä–∏—Ö-–∫–æ–¥"
    }

    # --- 3. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è attribute.csv ---
    replacements_map, raw_data = load_attributes_csv()
    changes_made = False
    max_raw_row_len = len(raw_data[0]) if raw_data and raw_data[0] else 10

    # --- 4. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–æ—á–æ–∫ –≤—Å—Ç–∞–≤–∫–∏ –¥–ª—è –Ω–æ–≤–∏—Ö –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ ---
    insertion_points = {}
    current_col_index = None

    for i, row in enumerate(raw_data[1:], start=1):
        if row and row[0].strip().isdigit():
            col_index = int(row[0].strip())
            if current_col_index is not None and current_col_index not in insertion_points:
                insertion_points[current_col_index] = i
            current_col_index = col_index
            insertion_points[col_index] = i + 1
        elif current_col_index is not None:
            insertion_points[current_col_index] = i + 1

    logging.debug(f"–¢–æ—á–∫–∏ –≤—Å—Ç–∞–≤–∫–∏ attribute.csv: {insertion_points}")

    # --- 5. –õ—ñ—á–∏–ª—å–Ω–∏–∫ –Ω–æ–≤–∏—Ö –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ ---
    new_attributes_counter = {}  # {col_index: count}

    # --- 6. –û–±—Ä–æ–±–∫–∞ CSV –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ ---
    temp_file_path = supliers_new_path + ".temp"

    try:
        with open(supliers_new_path, mode="r", encoding="utf-8") as input_file, \
             open(temp_file_path, mode="w", encoding="utf-8", newline="") as output_file:

            reader = csv.reader(input_file)
            writer = csv.writer(output_file)

            headers = next(reader)
            writer.writerow(headers)

            for idx, row in enumerate(reader, start=2):  # start=2 ‚Üí –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –∑–∞–≥–æ–ª–æ–≤–∫–∞
                product_url = row[1].strip() if len(row) > 1 else ""

                # --- –†–æ–∑—à–∏—Ä–µ–Ω–Ω—è —Ä—è–¥–∫–∞ –ø—Ä–∏ –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ—Å—Ç—ñ ---
                max_index = max(
                    max(product_data_map.values(), default=0),
                    other_attrs_index
                )
                if len(row) <= max_index:
                    row.extend([""] * (max_index + 1 - len(row)))

                # --- –ü—Ä–æ–ø—É—Å–∫ –Ω–µ–∫–æ—Ä–µ–∫—Ç–Ω–∏—Ö URL ---
                if not product_url or product_url.startswith("–ü–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Ç—É"):
                    writer.writerow(row)
                    continue

                try:
                    # --- 6.1 –ó–∞–ø–∏—Ç —Å—Ç–æ—Ä—ñ–Ω–∫–∏ ---
                    response = requests.get(product_url, timeout=10)
                    response.raise_for_status()
                    soup = BeautifulSoup(response.text, "html.parser")

                    characteristics_div = soup.find("div", id="w0-tab0")
                    parsed_attributes = {}

                    if characteristics_div and characteristics_div.find("table"):
                        for tr in characteristics_div.find("table").find_all("tr"):
                            cells = tr.find_all("td")
                            if len(cells) == 2:
                                key = cells[0].get_text(strip=True).replace(":", "")
                                value = cells[1].get_text(strip=True)
                                parsed_attributes[key] = value

                    other_attributes = []

                    # --- 6.2 –û–±—Ä–æ–±–∫–∞ –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ ---
                    for attr_name, attr_value in parsed_attributes.items():

                        # ‚ùó –ü–æ–≤–Ω—ñ—Å—Ç—é —ñ–≥–Ω–æ—Ä—É—î–º–æ —à—Ç—Ä–∏—Ö-–∫–æ–¥
                        if attr_name == "–®—Ç—Ä–∏—Ö-–∫–æ–¥":
                            continue

                        target_col_index = processing_map.get(attr_name)
                        original_value_lower = attr_value.strip().lower()

                        if target_col_index is not None:
                            replacement_rules = replacements_map.get(target_col_index, {})
                            new_value = replacement_rules.get(original_value_lower)

                            if new_value:
                                row[target_col_index] = new_value
                            else:
                                if original_value_lower not in replacement_rules:
                                    insert_index = insertion_points.get(target_col_index)

                                    if insert_index is None:
                                        logging.error(
                                            f"–ê—Ç—Ä–∏–±—É—Ç '{attr_value}' (I={target_col_index}) "
                                            f"–Ω–µ –¥–æ–¥–∞–Ω–æ: –≤—ñ–¥—Å—É—Ç–Ω—è —Ç–æ—á–∫–∞ –≤—Å—Ç–∞–≤–∫–∏"
                                        )
                                        row[target_col_index] = attr_value
                                        continue

                                    new_raw_row = [""] * max_raw_row_len
                                    new_raw_row[2] = original_value_lower
                                    raw_data.insert(insert_index, new_raw_row)

                                    replacements_map.setdefault(
                                        target_col_index, {}
                                    )[original_value_lower] = ""

                                    changes_made = True

                                    # –ó—Å—É–≤ —Ç–æ—á–æ–∫ –≤—Å—Ç–∞–≤–∫–∏
                                    for col, point in insertion_points.items():
                                        if point >= insert_index:
                                            insertion_points[col] += 1

                                    new_attributes_counter[target_col_index] = (
                                        new_attributes_counter.get(target_col_index, 0) + 1
                                    )

                                row[target_col_index] = attr_value
                        else:
                            other_attributes.append(f"{attr_name}:{attr_value}")

                    if other_attributes:
                        row[other_attrs_index] = ", ".join(other_attributes)

                    writer.writerow(row)

                except requests.RequestException as req_err:
                    logging.error(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Ç—É URL {product_url}: {req_err}")
                    writer.writerow(row)

                except Exception as e:
                    logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É URL {product_url}: {e}")
                    writer.writerow(row)

                time.sleep(random.uniform(1, 3))

        # --- 7. –ó–∞–º—ñ–Ω–∞ —Ñ–∞–π–ª—É ---
        os.replace(temp_file_path, supliers_new_path)
        logging.info("–ü–∞—Ä—Å–∏–Ω–≥ –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. CSV –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ –æ–Ω–æ–≤–ª–µ–Ω–æ.")

        # --- 8. –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è attribute.csv ---
        if changes_made:
            save_attributes_csv(raw_data)
        else:
            logging.info("attribute.csv –Ω–µ –∑–º—ñ–Ω—é–≤–∞–≤—Å—è.")

        # --- 9. –ü—ñ–¥—Å—É–º–∫–æ–≤–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è ---
        if new_attributes_counter:
            logging.info("–î–æ–¥–∞–Ω—ñ –Ω–æ–≤—ñ –∞—Ç—Ä–∏–±—É—Ç–∏:")
            for col_index, count in sorted(new_attributes_counter.items()):
                logging.info(f"–ö–æ–ª–æ–Ω–∫–∞ {col_index}: +{count}")
        else:
            logging.info("–ù–æ–≤–∏—Ö –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ –Ω–µ –¥–æ–¥–∞–Ω–æ.")

    except Exception as e:
        logging.error(f"–ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è: {e}")
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)

def apply_final_standardization():
    """
    –ó–∞—Å—Ç–æ—Å–æ–≤—É—î —Ñ—ñ–Ω–∞–ª—å–Ω—ñ –ø—Ä–∞–≤–∏–ª–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü—ñ—ó –∑ attribute.csv –¥–æ —Ñ–∞–π–ª—É 1.csv.
    –ó–∞–º—ñ–Ω—é—î –∞—Ç—Ä–∏–±—É—Ç–∏ –Ω–∞ –∑–Ω–∞—á–µ–Ω–Ω—è –∑ –∫–æ–ª–æ–Ω–∫–∏ 'attr_site_name', —è–∫—â–æ –≤–æ–Ω–æ —ñ—Å–Ω—É—î.
    –ü—Ä–æ—ñ–≥–Ω–æ—Ä–æ–≤–∞–Ω—ñ –∞—Ç—Ä–∏–±—É—Ç–∏ (–∑ –ø–æ—Ä–æ–∂–Ω—ñ–º 'attr_site_name') –æ—á–∏—â–∞—é—Ç—å—Å—è.
    –ê—Ç—Ä–∏–±—É—Ç–∏, –¥–ª—è —è–∫–∏—Ö –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –ø—Ä–∞–≤–∏–ª, –∑–∞–ª–∏—à–∞—é—Ç—å—Å—è –±–µ–∑ –∑–º—ñ–Ω.
    –õ–æ–≥—É–≤–∞–Ω–Ω—è –≤–∫–ª—é—á–∞—î —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–º—ñ–Ω —Ç–∞ –æ—á–∏—â–µ–Ω—å.
    """
    oc_log_message()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 4. –ü–æ—á–∏–Ω–∞—é —Ñ—ñ–Ω–∞–ª—å–Ω—É —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü—ñ—é –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ —É 1.csv...")

    # --- 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å ---
    settings = load_oc_settings()
    try:
        csv_path = settings['paths']['csv_path_supliers_1_new']
        product_map = settings['suppliers']['1']['product_data_columns']
    except TypeError as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –¥–æ—Å—Ç—É–ø—É –¥–æ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å: {e}")
        return

    # --- 2. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–∞–ø–∏ –¥–ª—è –æ–±—Ä–æ–±–∫–∏ (–±–µ–∑ –®—Ç—Ä–∏—Ö-–∫–æ–¥—É) ---
    processing_map = {k: v for k, v in product_map.items() if k != "–®—Ç—Ä–∏—Ö-–∫–æ–¥"}

    # --- 3. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø—Ä–∞–≤–∏–ª –∑–∞–º—ñ–Ω–∏ ---
    replacements_map, _ = load_attributes_csv()

    # --- 4. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∑–∞–º—ñ–Ω ---
    replacement_counter = {}  # {col_index: count}
    cleared_counter = {}      # {col_index: count}

    # --- 5. –û–±—Ä–æ–±–∫–∞ CSV ---
    temp_file_path = csv_path + '.final_temp'
    try:
        with open(csv_path, 'r', encoding='utf-8') as infile, \
             open(temp_file_path, 'w', encoding='utf-8', newline='') as outfile:

            reader = csv.reader(infile)
            writer = csv.writer(outfile)
            headers = next(reader)
            writer.writerow(headers)

            # –°–ª–æ–≤–Ω–∏–∫ –¥–ª—è –ª–æ–≥—É–≤–∞–Ω–Ω—è –Ω–∞–∑–≤ –∫–æ–ª–æ–Ω–æ–∫
            column_names = {index: name for name, index in processing_map.items()}

            for idx, row in enumerate(reader):
                max_index = max(product_map.values(), default=0)
                if len(row) <= max_index:
                    row.extend([''] * (max_index + 1 - len(row)))

                for col_index, rules in replacements_map.items():
                    if col_index >= len(row):
                        continue

                    current_value = row[col_index].strip()
                    if not current_value:
                        continue

                    current_lower = current_value.lower()
                    col_name = column_names.get(col_index, f"I={col_index}")
                    new_value = rules.get(current_lower)

                    if new_value is not None:
                        if new_value:
                            if new_value != current_value:
                                row[col_index] = new_value
                                replacement_counter[col_index] = replacement_counter.get(col_index, 0) + 1
                                logging.info(f"–†—è–¥–æ–∫ {idx + 2}: –ó–ê–ú–Ü–ù–ê ({col_name}): '{current_value}' -> '{new_value}'")
                        else:
                            row[col_index] = ""
                            cleared_counter[col_index] = cleared_counter.get(col_index, 0) + 1
                            logging.warning(f"–†—è–¥–æ–∫ {idx + 2}: –Ü–ì–ù–û–†–£–í–ê–ù–ù–Ø/–û–ß–ò–©–ï–ù–ù–Ø ({col_name}): '{current_value}' –æ—á–∏—â–µ–Ω–æ")

                writer.writerow(row)

        os.replace(temp_file_path, csv_path)
        logging.info("–§—ñ–Ω–∞–ª—å–Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü—ñ—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. csv –æ–Ω–æ–≤–ª–µ–Ω–æ.")

        # --- 6. –ü—ñ–¥—Å—É–º–∫–æ–≤–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è ---
        if replacement_counter:
            for col, count in sorted(replacement_counter.items()):
                logging.info(f"–ê—Ç—Ä–∏–±—É—Ç {col}: –≤–∏–∫–æ–Ω–∞–Ω–æ {count} –∑–∞–º—ñ–Ω")
        if cleared_counter:
            for col, count in sorted(cleared_counter.items()):
                logging.info(f"–ê—Ç—Ä–∏–±—É—Ç {col}: –æ—á–∏—â–µ–Ω–æ {count} –∑–Ω–∞—á–µ–Ω—å")

    except FileNotFoundError as e:
        logging.error(f"–§–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {e}")
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)
    except Exception as e:
        logging.error(f"–ù–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü—ñ—ó: {e}")
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)

def fill_product_category():
    """
    –ó–∞–ø–æ–≤–Ω—é—î —Å–ª—É–∂–±–æ–≤—ñ –∫–æ–ª–æ–Ω–∫–∏ —É csv:
    - Q (–∫–∞—Ç–µ–≥–æ—Ä—ñ—è) –Ω–∞ –æ—Å–Ω–æ–≤—ñ M, N, O
    - T (–ø–æ–∑–Ω–∞—á–∫–∏) –Ω–∞ –æ—Å–Ω–æ–≤—ñ –Ω–∞–∑–≤–∏ —Ç–æ–≤–∞—Ä—É G
    - U (Rank Math) –Ω–∞ –æ—Å–Ω–æ–≤—ñ –Ω–∞–∑–≤–∏ —Ç–æ–≤–∞—Ä—É G
    - AV (pa_used) –Ω–∞ –æ—Å–Ω–æ–≤—ñ category.csv
    - V, W, X, Y, AZ —Ñ—ñ–∫—Å–æ–≤–∞–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏
    - Z (–∫–æ—Ä–æ—Ç–∫–∏–π –æ–ø–∏—Å) –∑ H
    - AX (–¥–∞—Ç–∞)
    –ü—Ä–∞—Ü—é—î —Ç—ñ–ª—å–∫–∏ –¥–ª—è –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ –∑ ID=1
    """
    oc_log_message()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 5. –ü–æ—á–∏–Ω–∞—é –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó —Ç–∞ —Å–ª—É–∂–±–æ–≤–∏—Ö –∫–æ–ª–æ–Ω–æ–∫...")

    settings = load_oc_settings()
    try:
        csv_path = settings['paths']['csv_path_supliers_1_new']
        supplier_id = 1
        name_ukr = settings['suppliers']['1']['name_ukr']
    except (TypeError, KeyError) as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å: {e}")
        return

    # –Ü–Ω–¥–µ–∫—Å–∏ –∫–æ–ª–æ–Ω–æ–∫
    M, N, O = 12, 13, 14
    G, H = 6, 7
    Q, T, U = 16, 19, 20
    Z, V, W, X, Y = 25, 21, 22, 23, 24
    AV, AX, AZ = 47, 49, 51

    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø—Ä–∞–≤–∏–ª –∫–∞—Ç–µ–≥–æ—Ä—ñ–π —ñ –ø–æ–∑–Ω–∞—á–æ–∫
    category_map, raw_category = load_category_csv()
    rules_category = category_map.get(supplier_id, {})
    poznachky_list = load_poznachky_csv()
    changes_category = False
    max_row_len_category = len(raw_category[0]) if raw_category else 5

    # –°—Ç–≤–æ—Ä—é—î–º–æ –º–∞–ø—É –¥–ª—è pa_used
    pa_used_map = {}
    for row in raw_category:
        if len(row) > 5 and (row[0].strip() == str(supplier_id) or row[0].strip() == ''):
            key = tuple(v.strip().lower() for v in row[1:4])
            pa_used_map[key] = row[5].strip()

    logging.info(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(pa_used_map)} –ø—Ä–∞–≤–∏–ª pa_used")

    current_date = datetime.now().strftime('%Y-%m-%dT00:00:00')

    # –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –Ω–æ–≤–æ–≥–æ —Ä—è–¥–∫–∞ —É category.csv
    def get_insert_index(supplier_id, raw_data):
        insert_index = len(raw_data)
        found_block = False
        for i, r in enumerate(raw_data):
            if r and r[0].strip().isdigit():
                try:
                    cur_id = int(r[0].strip())
                    if cur_id == supplier_id:
                        found_block = True
                        insert_index = i + 1
                    elif cur_id > supplier_id and found_block:
                        return i
                except ValueError:
                    continue
            elif found_block:
                insert_index = i + 1
        return insert_index

    temp_path = csv_path + '.category_temp'
    try:
        with open(csv_path, 'r', encoding='utf-8') as infile, \
             open(temp_path, 'w', encoding='utf-8', newline='') as outfile:

            reader = csv.reader(infile)
            writer = csv.writer(outfile)
            headers = next(reader)
            writer.writerow(headers)

            for idx, row in enumerate(reader):
                # –†–æ–∑—à–∏—Ä—é—î–º–æ —Ä—è–¥–æ–∫ –∑–∞ –ø–æ—Ç—Ä–µ–±–∏
                max_col = max(M, N, O, Q, T, U, V, W, X, Y, Z, AV, AX, AZ, G, H)
                if len(row) <= max_col:
                    row.extend([''] * (max_col + 1 - len(row)))

                product_name = row[G].strip()
                product_desc = row[H]

                key = tuple(row[i].strip().lower() for i in (M, N, O))

                # --- –ö–∞—Ç–µ–≥–æ—Ä—ñ—è Q ---
                category_val = rules_category.get(key)
                if category_val is not None:
                    row[Q] = category_val or ""
                else:
                    # –î–æ–¥–∞—î–º–æ –Ω–æ–≤–∏–π —Ä—è–¥–æ–∫ —É category.csv
                    insert_idx = get_insert_index(supplier_id, raw_category)
                    new_row = [''] + list(row[M:O+1]) + [''] * (max_row_len_category - 4)
                    raw_category.insert(insert_idx, new_row)
                    rules_category[key] = ""
                    changes_category = True
                    logging.warning(f"–†—è–¥–æ–∫ {idx + 2}: –î–æ–¥–∞–Ω–∞ –Ω–æ–≤–∞ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—è –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó {key}")

                # --- –ü–æ–∑–Ω–∞—á–∫–∏ T ---
                if product_name and poznachky_list:
                    found_tags = []
                    covered = []
                    name_lower = product_name.lower()
                    for tag in poznachky_list:
                        if tag in name_lower:
                            start, end = name_lower.find(tag), name_lower.find(tag) + len(tag)
                            if not any(s <= start and end <= e for s, e in covered):
                                found_tags.append(tag.capitalize())
                                covered.append((start, end))
                                covered.sort(key=lambda x: x[1]-x[0], reverse=True)
                    if found_tags:
                        row[T] = ', '.join(found_tags)

                # --- Rank Math U ---
                if product_name:
                    cleaned = re.sub(r'[–∞-—è–ê-–Ø0-9]', '', product_name)
                    cleaned = re.sub(r'[^a-zA-Z\s]', '', cleaned)
                    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
                    row[U] = cleaned

                # --- pa_used AV ---
                pa_val = pa_used_map.get(key)
                if pa_val:
                    row[AV] = pa_val


                # --- –§—ñ–∫—Å–æ–≤–∞–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏ ---
                row[V] = name_ukr
                row[W] = "draft"
                row[X] = "yes"
                row[Y] = "none"
                row[AZ] = "simple"
                row[AX] = current_date

                # --- –ö–æ—Ä–æ—Ç–∫–∏–π –æ–ø–∏—Å Z ---
                if product_desc:
                    row[Z] = product_desc.split('\\n', 1)[0].strip()
                else:
                    row[Z] = ""

                writer.writerow(row)

        os.replace(temp_path, csv_path)
        logging.info("–ó–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–π —Ç–∞ —Å–ª—É–∂–±–æ–≤–∏—Ö –∫–æ–ª–æ–Ω–æ–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")

        if changes_category:
            save_category_csv(raw_category)
        else:
            logging.info("–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è category.csv –Ω–µ –ø–æ—Ç—Ä—ñ–±–Ω–µ. –ó–º—ñ–Ω: False.")

    except Exception as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—ñ –∫–æ–ª–æ–Ω–æ–∫: {e}")
        if os.path.exists(temp_path):
            os.remove(temp_path)

def refill_product_category():
    """
    –ü–æ–≤—Ç–æ—Ä–Ω–æ –∑–∞–ø–æ–≤–Ω—é—î –∫–æ–ª–æ–Ω–∫–∏ Q (–ö–∞—Ç–µ–≥–æ—Ä—ñ—è) —Ç–∞ AV (pa_used) —É 1.csv
    –Ω–∞ –æ—Å–Ω–æ–≤—ñ –æ–Ω–æ–≤–ª–µ–Ω–∏—Ö –ø—Ä–∞–≤–∏–ª —É category.csv.
    –ù–ï –¥–æ–¥–∞—î –Ω–æ–≤—ñ —Ä—è–¥–∫–∏ —É category.csv.
    –õ–æ–≥—É–≤–∞–Ω–Ω—è –ø–æ–∫–∞–∑—É—î, —è–∫—ñ —Ä—è–¥–∫–∏ –æ–Ω–æ–≤–ª–µ–Ω—ñ.
    """
    oc_log_message()
    logging.info("–§—É–Ω–∫—Ü—ñ—è 6. –ü–æ—á–∏–Ω–∞—é –ø–æ–≤—Ç–æ—Ä–Ω–µ –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–π —Ç–∞ pa_used —É 1.csv...")

    # --- 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å ---
    settings = load_oc_settings()
    try:
        csv_path = settings['paths']['csv_path_supliers_1_new']
        supplier_id = 1
    except (TypeError, KeyError) as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –¥–æ—Å—Ç—É–ø—É –¥–æ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å: {e}")
        return

    # --- 2. –Ü–Ω–¥–µ–∫—Å–∏ –∫–æ–ª–æ–Ω–æ–∫ CSV ---
    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –æ–¥—Ä–∞–∑—É —á–∏—Å–ª–∞, –±–µ–∑ –¥–æ–≤–≥–∏—Ö –∑–º—ñ–Ω–Ω–∏—Ö
    M, N, O = 12, 13, 14        # name_1, name_2, name_3
    Q, AV = 16, 47              # –ö–∞—Ç–µ–≥–æ—Ä—ñ—è —Ç–∞ pa_used
    max_index = max(M, N, O, Q, AV)
    missing_category_rows = []  # —Å–ø–∏—Å–æ–∫ —Ä—è–¥–∫—ñ–≤ –∑ –ø–æ—Ä–æ–∂–Ω—å–æ—é –∫–∞—Ç–µ–≥–æ—Ä—ñ—î—é

    # --- 3. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø—Ä–∞–≤–∏–ª –∫–∞—Ç–µ–≥–æ—Ä—ñ–π —Ç–∞ pa_used ---
    category_map, raw_category = load_category_csv()
    rules_category = {}
    pa_used_map = {}
    supplier_str = str(supplier_id)

    for row in raw_category:
        if len(row) > 5:
            supplier_value = row[0].strip()
            if supplier_value == supplier_str or supplier_value == '':
                key = tuple(v.strip().lower() for v in row[1:4])  # –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—è M,N,O
                rules_category[key] = row[4].strip() if len(row) > 4 else ""
                pa_used_map[key] = row[5].strip() if len(row) > 5 else ""

    logging.info(f"–ó—á–∏—Ç–∞–Ω–æ {len(rules_category)} –ø—Ä–∞–≤–∏–ª –¥–ª—è –ö–∞—Ç–µ–≥–æ—Ä—ñ—ó (Q) —Ç–∞ {len(pa_used_map)} –ø—Ä–∞–≤–∏–ª –¥–ª—è pa_used (AV)")

    # --- 4. –û–±—Ä–æ–±–∫–∞ CSV ---
    temp_path = csv_path + '.refill_temp'
    updated_rows = 0

    try:
        with open(csv_path, 'r', encoding='utf-8') as infile, \
             open(temp_path, 'w', encoding='utf-8', newline='') as outfile:

            reader = csv.reader(infile)
            writer = csv.writer(outfile)

            headers = next(reader)
            writer.writerow(headers)

            for idx, row in enumerate(reader):
                # –†–æ–∑—à–∏—Ä—é—î–º–æ —Ä—è–¥–æ–∫, —â–æ–± –Ω–µ –≤–∏—Ö–æ–¥–∏—Ç–∏ –∑–∞ –º–µ–∂—ñ
                if len(row) <= max_index:
                    row.extend([''] * (max_index + 1 - len(row)))

                # --- 4.1 –ö–ª—é—á –ø–æ—à—É–∫—É ---
                key = tuple(row[i].strip().lower() for i in (M, N, O))
                initial_category = row[Q].strip()
                initial_pa_used = row[AV].strip()
                row_changed = False

                # --- 4.2 –ü–æ–≤—Ç–æ—Ä–Ω–µ –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –ö–∞—Ç–µ–≥–æ—Ä—ñ—ó Q ---
                category_val = rules_category.get(key)
                if category_val and category_val != initial_category:
                    row[Q] = category_val
                    row_changed = True
                    logging.info(f"–†—è–¥–æ–∫ {idx + 2}: Q (–ö–∞—Ç–µ–≥–æ—Ä—ñ—è) –æ–Ω–æ–≤–ª–µ–Ω–æ. –ö–ª—é—á: {key}, –ó–Ω–∞—á–µ–Ω–Ω—è: '{category_val}'")

                # --- 4.3 –ü–æ–≤—Ç–æ—Ä–Ω–µ –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è pa_used AV ---
                pa_val = pa_used_map.get(key)
                if pa_val and pa_val != initial_pa_used:
                    row[AV] = pa_val
                    row_changed = True
                    logging.info(f"–†—è–¥–æ–∫ {idx + 2}: AV (pa_used) –æ–Ω–æ–≤–ª–µ–Ω–æ. –ö–ª—é—á: {key}, –ó–Ω–∞—á–µ–Ω–Ω—è: '{pa_val}'")

                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø–æ—Ä–æ–∂–Ω—å–æ—ó –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó –ø—ñ—Å–ª—è –æ–Ω–æ–≤–ª–µ–Ω–Ω—è
                if not row[Q].strip():
                    missing_category_rows.append(idx + 2)  # –∑–±–µ—Ä—ñ–≥–∞—î–º–æ –Ω–æ–º–µ—Ä —Ä—è–¥–∫–∞ —É —Ñ–∞–π–ª—ñ

                if row_changed:
                    updated_rows += 1

                writer.writerow(row)

        # --- 5. –ó–∞–º—ñ–Ω—é—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π CSV ---
        os.replace(temp_path, csv_path)
        logging.info(f"–ü–æ–≤—Ç–æ—Ä–Ω–µ –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –û–Ω–æ–≤–ª–µ–Ω–æ {updated_rows} —Ä—è–¥–∫—ñ–≤.")

        # --- –õ–æ–≥—É–≤–∞–Ω–Ω—è —Ä—è–¥–∫—ñ–≤ –∑ –ø–æ—Ä–æ–∂–Ω—å–æ—é –∫–∞—Ç–µ–≥–æ—Ä—ñ—î—é ---
        for row_num in missing_category_rows:
            logging.warning(f"–£–í–ê–ì–ê —Ä—è–¥–æ–∫ {row_num} –Ω–µ –∑–∞–ø–æ–≤–Ω–µ–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è!")

    except Exception as e:
        logging.error(f"–ù–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ–º—É –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—ñ: {e}")
        if os.path.exists(temp_path):
            os.remove(temp_path)

def separate_existing_products():
    """
    –ó–≤—ñ—Ä—è—î —à—Ç—Ä–∏—Ö–∫–æ–¥–∏ 1.csv –∑ –±–∞–∑–æ—é (zalishki.csv),
    –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—å –∑–Ω–∞–π–¥–µ–Ω—ñ —Ç–æ–≤–∞—Ä–∏ —É old_prod_new_SHK.csv,
    –≤–∏–¥–∞–ª—è—î —ó—Ö –∑ 1.csv —Ç–∞ —Ñ–æ—Ä–º—É—î –ø—ñ–¥—Å—É–º–∫–æ–≤—É —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É.
    –ö–æ–ª–æ–Ω–∫–∏ —Ç–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ—Å—Ç—ñ old -> new –≤–∏–Ω–µ—Å–µ–Ω—ñ —É settings.json.
    """
    oc_log_message()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 7. –ü–æ—á–∏–Ω–∞—é –∑–≤—ñ—Ä–∫—É 1.csv –∑—ñ —à—Ç—Ä–∏—Ö–∫–æ–¥–∞–º–∏ –±–∞–∑–∏ (zalishki.csv)...")

    settings = load_oc_settings()
    try:
        sl_new_path = settings['paths']['csv_path_supliers_1_new']
        zalishki_path = settings['paths']['output_file']
        sl_old_prod_shk_path = settings['paths']['csv_path_sl_old_prod_new_shk']
        column_mapping = settings['suppliers']['1']['column_mapping_sl_old_to_sl_new']
    except KeyError as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó. –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —à–ª—è—Ö –∞–±–æ –º–∞–ø—É –∫–æ–ª–æ–Ω–æ–∫: {e}")
        return

    # --- 0. –ó—á–∏—Ç—É–≤–∞–Ω–Ω—è —ñ—Å–Ω—É—é—á–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞ old_prod_new_SHK.csv ---
    sl_old_header = []
    try:
        if os.path.exists(sl_old_prod_shk_path):
            with open(sl_old_prod_shk_path, mode='r', encoding='utf-8') as f:
                reader = csv.reader(f)
                sl_old_header = next(reader, [])
        else:
            logging.warning("–§–∞–π–ª old_prod_new_SHK.csv –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ ‚Äî —Å—Ç–≤–æ—Ä—é—é –Ω–æ–≤–∏–π —ñ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º.")
            sl_old_header_base = [
                'id', 'sku', '–ú–µ—Ç–∞: url_lutsk', '–ú–µ—Ç–∞: shtrih_cod', '–ú–µ—Ç–∞: artykul_lutsk', '–ü–æ–∑–Ω–∞—á–∫–∏',
                'rank_math_focus_keyword', '–ú–µ—Ç–∞: postachalnyk', 'manage_stock', 'tax_status', 'excerpt'
            ]
            # –î–æ–¥–∞—î–º–æ –∞—Ç—Ä–∏–±—É—Ç–∏ —Ç–∞ –¥–æ–¥–∞—Ç–∫–æ–≤—ñ –∫–æ–ª–æ–Ω–∫–∏ (–±–µ–∑ attribute_none)
            sl_old_header = sl_old_header_base + [f'attribute_{i}' for i in range(1, 24)] + [
                'content', 'post_date', 'product_type'
            ]

        # –û—á–∏—â–∞—î–º–æ —Ñ–∞–π–ª, –∞–ª–µ –∑–∞–ª–∏—à–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫
        with open(sl_old_prod_shk_path, mode='w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(sl_old_header)

        logging.info("–§–∞–π–ª old_prod_new_SHK.csv –æ—á–∏—â–µ–Ω–æ, –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∑–∞–ª–∏—à–µ–Ω–æ –±–µ–∑ –∑–º—ñ–Ω.")
    except Exception as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó old_prod_new_SHK.csv: {e}")
        return

    # --- 1. –ó—á–∏—Ç—É–≤–∞–Ω–Ω—è –±–∞–∑–∏ —à—Ç—Ä–∏—Ö–∫–æ–¥—ñ–≤ ---
    zalishki_map = {}  # {shk: (id, sku)}
    try:
        with open(zalishki_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            next(reader)  # –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫
            for row in reader:
                if len(row) > 7:
                    shk = row[7].strip()
                    if shk:
                        zalishki_map[shk] = (row[0].strip(), row[1].strip())
        logging.info(f"–ó—á–∏—Ç–∞–Ω–æ {len(zalishki_map)} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —à—Ç—Ä–∏—Ö–∫–æ–¥—ñ–≤ –∑ –±–∞–∑–∏.")
    except Exception as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —á–∏—Ç–∞–Ω–Ω—ñ –±–∞–∑–∏: {e}")
        return

    # --- 2. –û–±—Ä–æ–±–∫–∞ 1.csv —Ç–∞ —Ñ–æ—Ä–º—É–≤–∞–Ω–Ω—è —Å–ø–∏—Å–∫—ñ–≤ ---
    items_to_keep = []
    items_to_move = []

    try:
        with open(sl_new_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            header = next(reader)
            items_to_keep.append(header)

            for row in reader:
                # –†–æ–∑—à–∏—Ä—é—î–º–æ —Ä—è–¥–æ–∫ –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —ñ–Ω–¥–µ–∫—Å—É —É –º–∞–ø—ñ
                max_index = max(column_mapping.values())
                if len(row) <= max_index:
                    row.extend([''] * (max_index + 1 - len(row)))

                shk_value = row[2].strip()  # C (–®—Ç—Ä–∏—Ö–∫–æ–¥)
                if shk_value in zalishki_map:
                    item_id, item_sku = zalishki_map[shk_value]

                    # –§–æ—Ä–º—É—î–º–æ –Ω–æ–≤–∏–π —Ä—è–¥–æ–∫ –¥–ª—è old_prod_new_SHK.csv
                    new_row = [''] * len(sl_old_header)
                    new_row[0] = item_id
                    new_row[1] = item_sku

                    for sl_old_idx_str, sl_new_idx in column_mapping.items():
                        sl_old_idx = int(sl_old_idx_str)  # –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ –∫–ª—é—á —É int
                        if sl_new_idx < len(row):
                            new_row[sl_old_idx] = row[sl_new_idx]

                    items_to_move.append(new_row)
                else:
                    items_to_keep.append(row)

        # --- 3. –ó–∞–ø–∏—Å –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ ---
        if items_to_move:
            with open(sl_old_prod_shk_path, 'a', encoding='utf-8', newline='') as f:
                writer = csv.writer(f)
                writer.writerows(items_to_move)
            logging.info(f"–ü–µ—Ä–µ–Ω–µ—Å–µ–Ω–æ {len(items_to_move)} —ñ—Å–Ω—É—é—á–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ —É old_prod_new_SHK.csv.")
        else:
            logging.info("–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∂–æ–¥–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä—É –∑ —ñ—Å–Ω—É—é—á–∏–º —à—Ç—Ä–∏—Ö–∫–æ–¥–æ–º —É –±–∞–∑—ñ.")

        # --- 4. –ó–∞–ø–∏—Å –æ–Ω–æ–≤–ª–µ–Ω–æ–≥–æ 1.csv ---
        temp_path = sl_new_path + '.temp'
        with open(temp_path, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerows(items_to_keep)
        os.replace(temp_path, sl_new_path)
        logging.info(f"1.csv –æ–Ω–æ–≤–ª–µ–Ω–æ. –ó–∞–ª–∏—à–∏–ª–æ—Å—å {len(items_to_keep)-1} –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ –¥–ª—è —ñ–º–ø–æ—Ä—Ç—É.")

    except Exception as e:
        logging.error(f"–ù–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –æ–±—Ä–æ–±–∫–∏ 1.csv: {e}")
        if 'temp_path' in locals() and os.path.exists(temp_path):
            os.remove(temp_path)