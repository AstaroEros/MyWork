import csv
import logging
import os
import random
import time
import requests
import re
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from scr.oc_base_function import oc_log_message, load_oc_settings, load_attributes_csv, save_attributes_csv, \
                                load_category_csv, save_category_csv, load_poznachky_csv

# –û–°–ù–û–í–ù–ê –§–£–ù–ö–¶–Ü–Ø 1: –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∑–º—ñ–Ω–∏ –∞—Ä—Ç–∏–∫—É–ª—É —ñ —à—Ç—Ä–∏—Ö–∫–æ–¥—É
def find_change_art_shtrihcod():
    """
    –ü–µ—Ä–µ–≤—ñ—Ä—è—î —Ä–æ–∑–±—ñ–∂–Ω–æ—Å—Ç—ñ:
    1) –ø–æ —à—Ç—Ä–∏—Ö–∫–æ–¥—É (–ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É)
    2) –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É (–ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –ø–æ —à—Ç—Ä–∏—Ö–∫–æ–¥—É)
    –£—Å—ñ —Ä–æ–∑–±—ñ–∂–Ω–æ—Å—Ç—ñ –∑–∞–ø–∏—Å—É—î —É change_art_shtrihcod
    """

    oc_log_message()
    logging.info("‚ñ∂ –°—Ç–∞—Ä—Ç –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –∞—Ä—Ç–∏–∫—É–ª—ñ–≤ —ñ —à—Ç—Ä–∏—Ö–∫–æ–¥—ñ–≤ (2 –Ω–∞–ø—Ä—è–º–∫–∏)")

    settings = load_oc_settings()
    if not settings:
        logging.info("‚ùå oc_settings.yaml –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ")
        return

    site_csv = settings["paths"]["output_file"]
    supplier_csv = settings["suppliers"][1]["csv_path"]
    result_csv = settings["paths"]["change_art_shtrihcod"]

    # --------------------------------------------------
    # 1. –ß–∏—Ç–∞—î–º–æ –ø—Ä–∞–π—Å –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞
    # --------------------------------------------------
    supplier_by_artykul = {}   # –ö–æ–¥_—Ç–æ–≤–∞—Ä–∞ -> –®—Ç—Ä–∏—Ö_–∫–æ–¥
    supplier_by_shtrih = {}    # –®—Ç—Ä–∏—Ö_–∫–æ–¥  -> –ö–æ–¥_—Ç–æ–≤–∞—Ä–∞

    with open(supplier_csv, newline="", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f, delimiter=";")

        for row in reader:
            artykul = row.get("–ö–æ–¥_—Ç–æ–≤–∞—Ä–∞", "").strip()
            shtrih = row.get("–®—Ç—Ä–∏—Ö_–∫–æ–¥", "").strip()

            if artykul and shtrih:
                supplier_by_artykul[artykul] = shtrih
                supplier_by_shtrih[shtrih] = artykul

    logging.info(
        f"‚Ñπ –¢–æ–≤–∞—Ä—ñ–≤ –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞: "
        f"{len(supplier_by_artykul)} (–ø–æ –∞—Ä—Ç–∏–∫—É–ª—É), "
        f"{len(supplier_by_shtrih)} (–ø–æ —à—Ç—Ä–∏—Ö–∫–æ–¥—É)"
    )

    # --------------------------------------------------
    # 2. –û—á–∏—â–µ–Ω–Ω—è —Ñ–∞–π–ª—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
    # --------------------------------------------------
    headers = [
        "sku",
        "shtrih_cod",
        "artykul_lutsk",
        "–ö–æ–¥_—Ç–æ–≤–∞—Ä–∞",
        "–®—Ç—Ä–∏—Ö_–∫–æ–¥"
    ]

    with open(result_csv, "w", newline="", encoding="utf-8") as f:
        csv.writer(f).writerow(headers)

    logging.info("üßπ change_art_shtrihcod –æ—á–∏—â–µ–Ω–æ")

    # --------------------------------------------------
    # 3. –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è (2 –Ω–∞–ø—Ä—è–º–∫–∏)
    # --------------------------------------------------
    checked = 0
    diff_count = 0
    written_keys = set()  # –∑–∞—Ö–∏—Å—Ç –≤—ñ–¥ –¥—É–±–ª—ñ–≤

    with open(site_csv, newline="", encoding="utf-8") as site_f, \
         open(result_csv, "a", newline="", encoding="utf-8") as out_f:

        site_reader = csv.DictReader(site_f)
        writer = csv.writer(out_f)

        for row in site_reader:
            sku = row.get("sku", "").strip()
            site_shtrih = row.get("shtrih_cod", "").strip()
            site_artykul = row.get("artykul_lutsk", "").strip()

            # ---------- –ü–†–ê–í–ò–õ–û 1 ----------
            # –Ñ –∞—Ä—Ç–∏–∫—É–ª ‚Üí –ø–æ—Ä—ñ–≤–Ω—é—î–º–æ —à—Ç—Ä–∏—Ö–∫–æ–¥
            if site_artykul and site_artykul in supplier_by_artykul:
                checked += 1
                supplier_shtrih = supplier_by_artykul[site_artykul]

                if site_shtrih != supplier_shtrih:
                    key = (sku, site_artykul, supplier_shtrih)
                    if key not in written_keys:
                        writer.writerow([
                            sku,
                            site_shtrih,
                            site_artykul,
                            site_artykul,
                            supplier_shtrih
                        ])
                        written_keys.add(key)
                        diff_count += 1

            # ---------- –ü–†–ê–í–ò–õ–û 2 ----------
            # –Ñ —à—Ç—Ä–∏—Ö–∫–æ–¥ ‚Üí –ø–æ—Ä—ñ–≤–Ω—é—î–º–æ –∞—Ä—Ç–∏–∫—É–ª
            if site_shtrih and site_shtrih in supplier_by_shtrih:
                checked += 1
                supplier_artykul = supplier_by_shtrih[site_shtrih]

                if site_artykul != supplier_artykul:
                    key = (sku, site_shtrih, supplier_artykul)
                    if key not in written_keys:
                        writer.writerow([
                            sku,
                            site_shtrih,
                            site_artykul,
                            supplier_artykul,
                            site_shtrih
                        ])
                        written_keys.add(key)
                        diff_count += 1

    # --------------------------------------------------
    # 4. –ü—ñ–¥—Å—É–º–æ–∫
    # --------------------------------------------------
    logging.info(f"‚úÖ –ü–µ—Ä–µ–≤—ñ—Ä–µ–Ω–æ –ø–æ–∑–∏—Ü—ñ–π: {checked}")
    logging.info(f"‚ö† –ó–Ω–∞–π–¥–µ–Ω–æ —Ä–æ–∑–±—ñ–∂–Ω–æ—Å—Ç–µ–π: {diff_count}")

# –û–°–ù–û–í–ù–ê –§–£–ù–ö–¶–Ü–Ø 2: –ó–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤
def find_new_products():
    """
    –ü–æ—Ä—ñ–≤–Ω—é—î –∞—Ä—Ç–∏–∫—É–ª–∏ —Ç–æ–≤–∞—Ä—ñ–≤ –∑ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç–∞ –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ –∑ –∞—Ä—Ç–∏–∫—É–ª–∞–º–∏,
    —â–æ —î –Ω–∞ —Å–∞–π—Ç—ñ, —ñ –∑–∞–ø–∏—Å—É—î –Ω–æ–≤—ñ —Ç–æ–≤–∞—Ä–∏ –≤ –æ–∫—Ä–µ–º–∏–π —Ñ–∞–π–ª.
    """
    # --- 1. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ª–æ–≥—É–≤–∞–Ω–Ω—è ---
    oc_log_message()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 1. –ü–æ—á–∏–Ω–∞—é –ø–æ—à—É–∫ –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤...")
    
    # --- 2. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å –∑ settings.json ---
    settings = load_oc_settings()
    if not settings:
        logging.info("‚ùå oc_settings.yaml –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ")
        return
    
    # --- 3. –û—Ç—Ä–∏–º–∞–Ω–Ω—è —à–ª—è—Ö—ñ–≤ –¥–æ –ø–æ—Ç—Ä—ñ–±–Ω–∏—Ö —Ñ–∞–π–ª—ñ–≤ ---
    zalishki_path = settings['paths']['output_file']                         # –ë–∞–∑–∞ —ñ—Å–Ω—É—é—á–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤
    supliers_new_path = settings['paths']['csv_path_new_product']         # –§–∞–π–ª, –∫—É–¥–∏ –±—É–¥–µ –∑–∞–ø–∏—Å–∞–Ω–æ –Ω–æ–≤—ñ —Ç–æ–≤–∞—Ä–∏
    supliers_csv_path = settings['suppliers'][1]['csv_path']                 # –ü—Ä–∞–π—Å-–ª–∏—Å—Ç –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ 1
    delimiter = settings['suppliers'][1]['delimiter']                        # –†–æ–∑–¥—ñ–ª—å–Ω–∏–∫ —É CSV
    
    # --- 4. –û—Ç—Ä–∏–º–∞–Ω–Ω—è –¥–æ–ø–æ–º—ñ–∂–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ ---
    sku_prefix = settings['suppliers'][1]['search']                          # –ü—Ä–µ—Ñ—ñ–∫—Å –¥–ª—è –ø–æ—à—É–∫—É
       
    logging.info("–ó—á–∏—Ç—É—é —ñ—Å–Ω—É—é—á—ñ –∞—Ä—Ç–∏–∫—É–ª–∏ –∑ —Ñ–∞–π–ª—É, –≤–∫–∞–∑–∞–Ω–æ–≥–æ –∑–∞ –∫–ª—é—á–µ–º 'csv_path_zalishki'.")

    try:

        # --- 3. –ó—á–∏—Ç—É–≤–∞–Ω–Ω—è –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤ –∑ —Ñ–∞–π–ª—É –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ ---
        fieldnames = []
        with open(supliers_new_path, mode='r', encoding='utf-8') as f:
            reader = csv.reader(f)
            try:
                fieldnames = next(reader) # –û—Ç—Ä–∏–º—É—î–º–æ —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤ –∫–æ–ª–æ–Ω–æ–∫: ['search', 'url_lutsk', ...]
            except StopIteration:
                logging.info("‚ùå –§–∞–π–ª –¥–ª—è –∑–∞–ø–∏—Å—É –ø–æ—Ä–æ–∂–Ω—ñ–π.")
                return

        logging.info(f"–°—Ç—Ä—É–∫—Ç—É—Ä—É —Ñ–∞–π–ª—É –∑—á–∏—Ç–∞–Ω–æ. –ö–æ–ª–æ–Ω–æ–∫: {len(fieldnames)}")

        # --- 4. –ó—á–∏—Ç—É–≤–∞–Ω–Ω—è —ñ—Å–Ω—É—é—á–∏—Ö –∞—Ä—Ç–∏–∫—É–ª—ñ–≤ —ñ–∑ –±–∞–∑–∏ (–û–ù–û–í–õ–ï–ù–û) ---
        logging.info("–ó—á–∏—Ç—É—é –±–∞–∑—É —ñ—Å–Ω—É—é—á–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤...")
        with open(zalishki_path, mode='r', encoding='utf-8') as zalishki_file:
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ DictReader, —â–æ–± –∑–≤–µ—Ä—Ç–∞—Ç–∏—Å—è –ø–æ –Ω–∞–∑–≤—ñ
            zalishki_reader = csv.DictReader(zalishki_file)

            existing_skus = {
                row.get("artykul_lutsk", "").strip().lower() 
                for row in zalishki_reader 
                if row.get("artykul_lutsk")
            }
            
            logging.info(f"–ó—á–∏—Ç–∞–Ω–æ {len(existing_skus)} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –∞—Ä—Ç–∏–∫—É–ª—ñ–≤ —ñ–∑ –±–∞–∑–∏.")

        # --- 5. –í—ñ–¥–∫—Ä–∏–≤–∞—î–º–æ —Ñ–∞–π–ª –¥–ª—è –∑–∞–ø–∏—Å—É (DictWriter) ---
        logging.info("–í—ñ–¥–∫—Ä–∏–≤–∞—é —Ñ–∞–π–ª –¥–ª—è –∑–∞–ø–∏—Å—É –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤...")
        with open(supliers_new_path, mode='w', encoding='utf-8', newline='') as new_file:
            # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ Writer, –ø–µ—Ä–µ–¥–∞—é—á–∏ –π–æ–º—É —Å–ø–∏—Å–æ–∫ –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤
            writer = csv.DictWriter(new_file, fieldnames=fieldnames)
            writer.writeheader() # –ó–∞–ø–∏—Å—É—î–º–æ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–∞–∑–∞–¥ —É —Ñ–∞–π–ª
            
            # --- 6. –ß–∏—Ç–∞—î–º–æ –ø—Ä–∞–π—Å –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ (DictReader) ---
            with open(supliers_csv_path, mode='r', encoding='utf-8') as supliers_file:
                # DictReader –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤—ñ–∑—å–º–µ –ø–µ—Ä—à–∏–π —Ä—è–¥–æ–∫ –ø—Ä–∞–π—Å—É —è–∫ –∫–ª—é—á—ñ —Å–ª–æ–≤–Ω–∏–∫–∞
                supliers_reader = csv.DictReader(supliers_file, delimiter=delimiter)
                
                new_products_count = 0

                for row in supliers_reader:
                    # –û—Ç—Ä–∏–º—É—î–º–æ –∞—Ä—Ç–∏–∫—É–ª –ø–æ –Ω–∞–∑–≤—ñ –∫–æ–ª–æ–Ω–∫–∏
                    sku = row.get("–ö–æ–¥_—Ç–æ–≤–∞—Ä–∞", "").strip().lower()
                    
                    if sku and sku not in existing_skus:
                        
                        # –°—Ç–≤–æ—Ä—é—î–º–æ –ø—É—Å—Ç–∏–π —Å–ª–æ–≤–Ω–∏–∫ –¥–ª—è –Ω–æ–≤–æ–≥–æ —Ä—è–¥–∫–∞, –∑–∞–ø–æ–≤–Ω—é—î–º–æ –ø–æ—Ä–æ–∂–Ω—ñ–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏
                        new_row = {key: '' for key in fieldnames}
                        
                        # --- 7. –ü–†–Ø–ú–ï –ú–ê–ü–£–í–ê–ù–ù–Ø (–ù–∞–∑–≤–∞ -> –ù–∞–∑–≤–∞) ---
                        
                        # –°–ø–µ—Ü—ñ–∞–ª—å–Ω–µ –ø–æ–ª–µ search (–ø—Ä–µ—Ñ—ñ–∫—Å + –∫–æ–¥)
                        new_row["search"] = sku_prefix + row.get("–ö–æ–¥_—Ç–æ–≤–∞—Ä–∞", "")
                        
                        # –û—Å–Ω–æ–≤–Ω—ñ –ø–æ–ª—è (–±–µ—Ä—É—Ç—å—Å—è –ø—Ä—è–º–æ –∑ row –ø–æ –∫–ª—é—á—É)
                        new_row["shtrih_cod"] = row.get("–®—Ç—Ä–∏—Ö_–∫–æ–¥", "")
                        new_row["–ö–æ–¥_—Ç–æ–≤–∞—Ä–∞"] = row.get("–ö–æ–¥_—Ç–æ–≤–∞—Ä–∞", "")
                        new_row["–ù–∞–∑–≤–∞–Ω–∏–µ_–ø–æ–∑–∏—Ü–∏–∏"] = row.get("–ù–∞–∑–≤–∞–Ω–∏–µ_–ø–æ–∑–∏—Ü–∏–∏", "")
                        new_row["–û–ø–∏—Å–∞–Ω–∏–µ"] = row.get("–û–ø–∏—Å–∞–Ω–∏–µ", "")
                        new_row["–¶–µ–Ω–∞"] = row.get("–¶–µ–Ω–∞", "")
                        new_row["–ù–∞–ª–∏—á–∏–µ"] = row.get("–ù–∞–ª–∏—á–∏–µ", "")
                        new_row["–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å"] = row.get("–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å", "")
                        new_row["–°—Ç—Ä–∞–Ω–∞_–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å"] = row.get("–°—Ç—Ä–∞–Ω–∞_–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å", "")
                        new_row["–ö–∞—Ç–µ–≥–æ—Ä–∏—è"] = row.get("–ö–∞—Ç–µ–≥–æ—Ä–∏—è", "")
                        new_row["–î–æ–ø. –ö–∞—Ç–µ–≥–æ—Ä–∏—è 1"] = row.get("–î–æ–ø. –ö–∞—Ç–µ–≥–æ—Ä–∏—è 1", "")
                        new_row["–î–æ–ø. –ö–∞—Ç–µ–≥–æ—Ä–∏—è 2"] = row.get("–î–æ–ø. –ö–∞—Ç–µ–≥–æ—Ä–∏—è 2", "")

                        # --- 8. –ó–∞–ø–∏—Å —Ä—è–¥–∫–∞ ---
                        writer.writerow(new_row)
                        new_products_count += 1

        # --- 17. –ü—ñ–¥—Å—É–º–∫–æ–≤–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è ---
        logging.info(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {new_products_count} –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤.")
        logging.info(f"–î–∞–Ω—ñ –∑–∞–ø–∏—Å–∞–Ω–æ —É —Ñ–∞–π–ª csv 'supliers_new_path'.")

    # --- 18. –û–±—Ä–æ–±–∫–∞ –ø–æ–º–∏–ª–æ–∫ ---
    except FileNotFoundError as e:
        logging.info(f"‚ùå –ü–æ–º–∏–ª–∫–∞: –§–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ - {e}")
    except Exception as e:
        logging.info(f"‚ùå –í–∏–Ω–∏–∫–ª–∞ –Ω–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")

# –û–°–ù–û–í–ù–ê –§–£–ù–ö–¶–Ü–Ø 3: –ó–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è —É—Ä–ª —Ç–æ–≤–∞—Ä—É
def find_product_url():
    """
    –ó—á–∏—Ç—É—î —Ñ–∞–π–ª –∑ –Ω–æ–≤–∏–º–∏ —Ç–æ–≤–∞—Ä–∞–º–∏, –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç—å –∑–∞ URL-–∞–¥—Ä–µ—Å–æ—é,
    –∑–Ω–∞—Ö–æ–¥–∏—Ç—å URL-–∞–¥—Ä–µ—Å—É –ø—Ä–æ—Å—Ç–æ–≥–æ –∞–±–æ –≤–∞—Ä—ñ–∞—Ç–∏–≤–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä—É,
    —ñ –∑–∞–ø–∏—Å—É—î –∑–Ω–∞–π–¥–µ–Ω—É URL-–∞–¥—Ä–µ—Å—É –≤ –∫–æ–ª–æ–Ω–∫—É B(1) –≤ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª.
    """

    # --- 1. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ª–æ–≥—É–≤–∞–Ω–Ω—è (–ø—ñ–¥–∫–ª—é—á–∞—î–º–æ —ñ—Å–Ω—É—é—á–∏–π –ª–æ–≥-—Ñ–∞–π–ª) ---
    oc_log_message()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 2. –ü–æ—á–∏–Ω–∞—é –ø–æ—à—É–∫ URL-–∞–¥—Ä–µ—Å —Ç–æ–≤–∞—Ä—ñ–≤...")
    
    # --- 2. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å —Ç–∞ —Ñ–æ—Ä–º—É–≤–∞–Ω–Ω—è —à–ª—è—Ö—ñ–≤/—Ç–∏–º—á–∞—Å–æ–≤–æ–≥–æ —Ñ–∞–π–ª—É ---
    settings = load_oc_settings()
    if not settings:
        logging.error("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è. –û–±—Ä–æ–±–∫–∞ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç–∞ –ø–µ—Ä–µ—Ä–≤–∞–Ω–∞.")
        return
    supliers_new_path = settings['paths']['csv_path_new_product']     # –≤—Ö—ñ–¥–Ω–∏–π CSV (1.csv)
    site_url = settings['suppliers'][1]['site']                       # –±–∞–∑–æ–≤–∏–π URL —Å–∞–π—Ç—É (—â–æ–± –¥–æ–¥–∞–≤–∞—Ç–∏ –≤—ñ–¥–Ω–æ—Å–Ω—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è)
    temp_file_path = supliers_new_path + '.temp'                      # —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª –ø—ñ–¥ —á–∞—Å –∑–∞–ø–∏—Å—É

    # --- –õ—ñ—á–∏–ª—å–Ω–∏–∫–∏ —ñ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ---
    total_rows = 0
    found_variant_count = 0
    found_simple_count = 0
    not_found_count = 0
    found_variant_rows = []
    not_found_rows = []

    try:
        # --- 3. –í—ñ–¥–∫—Ä–∏–≤–∞—î–º–æ –≤—Ö—ñ–¥–Ω–∏–π —Ñ–∞–π–ª –¥–ª—è —á–∏—Ç–∞–Ω–Ω—è ---
        with open(supliers_new_path, mode='r', encoding='utf-8') as input_file:
            reader = csv.DictReader(input_file)
            # –í–ê–ñ–õ–ò–í–û: –û—Ç—Ä–∏–º—É—î–º–æ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –æ–¥—Ä–∞–∑—É
            fieldnames = reader.fieldnames
            
            # --- –ü–ï–†–ï–í–Ü–†–ö–ê –ù–ê –ü–û–ú–ò–õ–ö–£: –Ø–∫—â–æ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –ø—É—Å—Ç—ñ ---
            if not fieldnames:
                logging.error("‚ùå –ü–æ–º–∏–ª–∫–∞: –ù–µ –≤–¥–∞–ª–æ—Å—è –ø—Ä–æ—á–∏—Ç–∞—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å—Ç–æ–≤–ø—Ü—ñ–≤. –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ, —á–∏ —Ñ–∞–π–ª –Ω–µ –ø—É—Å—Ç–∏–π —ñ —á–∏ –∫–æ—Ä–µ–∫—Ç–Ω–µ –∫–æ–¥—É–≤–∞–Ω–Ω—è.")
                return
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ –∫—Ä–∏—Ç–∏—á–Ω–∏—Ö –∫–æ–ª–æ–Ω–æ–∫
            required_columns = ['search', 'sku', 'url_lutsk']
            for col in required_columns:
                if col not in fieldnames:
                    logging.error(f"‚ùå –£ —Ñ–∞–π–ª—ñ –≤—ñ–¥—Å—É—Ç–Ω—è –æ–±–æ–≤'—è–∑–∫–æ–≤–∞ –∫–æ–ª–æ–Ω–∫–∞: {col}")
                    return

            # --- 4. –í—ñ–¥–∫—Ä–∏–≤–∞—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª –¥–ª—è –∑–∞–ø–∏—Å—É (DictWriter) ---
            with open(temp_file_path, mode='w', encoding='utf-8', newline='') as output_file:
                writer = csv.DictWriter(output_file, fieldnames=fieldnames)
                writer.writeheader() # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –∑–∞–ø–∏—Å—É—î —Ä—è–¥–æ–∫ –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤
                
                # --- 5. –Ü—Ç–µ—Ä–∞—Ü—ñ—è –ø–æ —Ä—è–¥–∫–∞—Ö ---
                for idx, row in enumerate(reader):
                    total_rows += 1
                    
                    # 5.1. –í–∏—Ç—è–≥—É—î–º–æ –¥–∞–Ω—ñ –ø–æ –Ω–∞–∑–≤–∞—Ö –∫–æ–ª–æ–Ω–æ–∫
                    search_url = row.get('search', '').strip()
                    
                    file_sku = row.get('–ö–æ–¥_—Ç–æ–≤–∞—Ä–∞', '').strip()

                    # --- 6. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –≤–∞–ª—ñ–¥–Ω–æ—Å—Ç—ñ URL ---
                    if not search_url or search_url.startswith('–ü–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Ç—É'):
                        writer.writerow(row)
                        continue

                    try:
                        # --- 7. –í–∏–∫–æ–Ω–∞–Ω–Ω—è HTTP-–∑–∞–ø–∏—Ç—É –¥–æ search_url —ñ –ø–∞—Ä—Å–∏–Ω–≥ HTML ---
                        response = requests.get(search_url)
                        response.raise_for_status()
                        soup = BeautifulSoup(response.text, 'html.parser')
                        found_type = None  # 'variant' –∞–±–æ 'simple'
                        found_url = None  # —Å—é–¥–∏ –∑–∞–ø–∏—à–µ–º–æ –∑–Ω–∞–π–¥–µ–Ω—É —Ä–µ–∞–ª—å–Ω—É URL-–∞–¥—Ä–µ—Å—É —Ç–æ–≤–∞—Ä—É
                        
                        # --- 8. –ü–æ—à—É–∫ –≤–∞—Ä—ñ–∞—Ç–∏–≤–Ω–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ (input.variant_control[data-code]) ---
                        # –®—É–∫–∞—î–º–æ input —Ç–µ–≥–∏ –∑ –∫–ª–∞—Å–æ–º variant_control —Ç–∞ –∞—Ç—Ä–∏–±—É—Ç–æ–º data-code,
                        # –ø–æ—Ä—ñ–≤–Ω—é—î–º–æ data-code –∑ file_sku ‚Äî —è–∫—â–æ —Å–ø—ñ–≤–ø–∞–¥—ñ–Ω–Ω—è, –±–µ—Ä–µ–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è —É –±–∞—Ç—å–∫—ñ–≤—Å—å–∫–æ–º—É –±–ª–æ—Ü—ñ.
                        variant_inputs = soup.find_all('input', class_='variant_control', attrs={'data-code': True})
                        for input_tag in variant_inputs:
                            site_sku = input_tag.get('data-code', '').strip()
                            if file_sku == site_sku:
                                parent_div = input_tag.find_parent('div', class_='card-block')
                                if parent_div:
                                    link_tag = parent_div.find('h4', class_='card-title').find('a')
                                    if link_tag and link_tag.has_attr('href'):
                                        # –§–æ—Ä–º—É—î–º–æ –ø–æ–≤–Ω–∏–π URL (–¥–æ–¥–∞—î–º–æ site_url –¥–æ –≤—ñ–¥–Ω–æ—Å–Ω–æ–≥–æ —à–ª—è—Ö—É)
                                        found_url = site_url + link_tag['href']
                                        found_type = 'variant'
                                        break

                        # --- 9. –Ø–∫—â–æ –Ω–µ –∑–Ω–∞–π—à–ª–∏ —Å–µ—Ä–µ–¥ –≤–∞—Ä—ñ–∞–Ω—Ç—ñ–≤ ‚Äî —à—É–∫–∞—î–º–æ –ø—Ä–æ—Å—Ç—ñ —Ç–æ–≤–∞—Ä–∏ ---
                        if not found_url:
                            # –î–ª—è –ø—Ä–æ—Å—Ç–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ —à—É–∫–∞—î–º–æ div –∑ –∫–ª–∞—Å–æ–º 'radio', –±–µ—Ä–µ–º–æ —Ç–µ–∫—Å—Ç —è–∫ SKU,
                            # —ñ –∑–∞ —Ç–∞–∫–∏–º –∂–µ –ø—ñ–¥—Ö–æ–¥–æ–º –∑–Ω–∞—Ö–æ–¥–∏–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è —É –±–ª–æ—Ü—ñ card-block.
                            simple_divs = soup.find_all('div', class_='radio')
                            for div_tag in simple_divs:
                                site_sku = div_tag.get_text(strip=True).strip()
                                if file_sku == site_sku:
                                    parent_div = div_tag.find_parent('div', class_='card-block')
                                    if parent_div:
                                        link_tag = parent_div.find('h4', class_='card-title').find('a')
                                        if link_tag and link_tag.has_attr('href'):
                                            found_url = site_url + link_tag['href']
                                            found_type = 'simple'
                                            break

                        # --- 10. –ó–∞–ø–∏—Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É –≤ –∫–æ–ª–æ–Ω–∫—É 'url_lutsk' –∞–±–æ –ª–æ–≥—É–≤–∞–Ω–Ω—è —è–∫—â–æ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ ---
                        if found_url:
                            row['url_lutsk'] = found_url
                            if found_type == 'variant':
                                found_variant_count += 1
                                found_variant_rows.append(idx + 2)  # +2, –±–æ —Ä—è–¥–∫–∏ CSV —Ä–∞—Ö—É—é—Ç—å—Å—è –∑ 1 + –∑–∞–≥–æ–ª–æ–≤–æ–∫
                            elif found_type == 'simple':
                                found_simple_count += 1
                        else:
                            not_found_count += 1
                            not_found_rows.append(idx + 2)

                        # –ó–∞–ø–∏—Å—É—î–º–æ (–∑–Ω–∞–π–¥–µ–Ω–∏–π –∞–±–æ –Ω–µ–∑–º—ñ–Ω–µ–Ω–∏–π) —Ä—è–¥–æ–∫ —É —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª
                        writer.writerow(row)

                    except requests.RequestException as e:
                        # --- 11. –û–±—Ä–æ–±–∫–∞ –ø–æ–º–∏–ª–æ–∫ –∑–∞–ø–∏—Ç—É ---
                        logging.error(f"–†—è–¥–æ–∫ {idx + 2}: –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Ç—ñ: {e}")
                        # –ó–∞–ø–∏—Å—É—î–º–æ –ø–æ–º–∏–ª–∫—É –≤ –∫–æ–ª–æ–Ω–∫—É 'search', —è–∫ –±—É–ª–æ —Ä–∞–Ω—ñ—à–µ
                        row['search'] = f'–ü–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Ç—É: {e}'
                        writer.writerow(row)
                    
                    # --- 12. –î–æ–¥–∞—Ç–∫–æ–≤–∞ –ø–∞—É–∑–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏ (—Ä–∞–Ω–¥–æ–º—ñ–∑–æ–≤–∞–Ω–∞) –¥–ª—è —É–Ω–∏–∫–Ω–µ–Ω–Ω—è –±–∞–Ω–∞/DDOS ---
                    time.sleep(random.uniform(1, 3))
  
        # --- 13. –ü—ñ—Å–ª—è —É—Å–ø—ñ—à–Ω–æ—ó –æ–±—Ä–æ–±–∫–∏: –∑–∞–º—ñ–Ω–∞ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª—É —Ç–∏–º—á–∞—Å–æ–≤–∏–º ---
        os.replace(temp_file_path, supliers_new_path)

        # --- 14. –ó–≤–µ–¥–µ–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ---
        logging.info("=== –ü–Ü–î–°–£–ú–ö–û–í–ê –Ü–ù–§–û–†–ú–ê–¶–Ü–Ø ===")
        logging.info(f"–í—Å—å–æ–≥–æ —Ä—è–¥–∫—ñ–≤ –∑ —Ç–æ–≤–∞—Ä–∞–º–∏: {total_rows}")
        logging.info(
            f"–ó–Ω–∞–π–¥–µ–Ω–æ URL –≤–∞—Ä—ñ–∞—Ç–∏–≤–Ω–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤: {found_variant_count}"
            + (f" (–†—è–¥–∫–∏ {', '.join(map(str, found_variant_rows))})" if found_variant_rows else "")
        )
        logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ URL –ø—Ä–æ—Å—Ç–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤: {found_simple_count}")
        logging.info(
            f"–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ URL: {not_found_count}"
            + (f" (–†—è–¥–∫–∏ {', '.join(map(str, not_found_rows))})" if not_found_rows else "")
        )

    except FileNotFoundError as e:
        # --- 15. –û–±—Ä–æ–±–∫–∞ –ø–æ–º–∏–ª–∫–∏: –≤—Ö—ñ–¥–Ω–∏–π —Ñ–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ ---
        logging.error(f"–ü–æ–º–∏–ª–∫–∞: –§–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ - {e}")
    except Exception as e:
        # --- 16. –ì–∞—Ä–∞–Ω—Ç—ñ–π–Ω–µ –ø—Ä–∏–±–∏—Ä–∞–Ω–Ω—è: –≤–∏–¥–∞–ª—è—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª –ø—Ä–∏ –ø–æ–º–∏–ª—Ü—ñ, —â–æ–± –Ω–µ –∑–∞–ª–∏—à–∏—Ç–∏ —Å–º—ñ—Ç—Ç—è ---
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)
        logging.error(f"–í–∏–Ω–∏–∫–ª–∞ –Ω–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")

# –û–°–ù–û–í–ù–ê –§–£–ù–ö–¶–Ü–Ø 4: –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ç—Ä–∏–±—É—Ç—ñ–≤
def parse_product_attributes():
    oc_log_message()
    logging.info("‚ñ∂ –§–£–ù–ö–¶–Ü–Ø: –ü–∞—Ä—Å–∏–Ω–≥ (Mapping + Auto-suffix |ua + Next Col RU)")

    settings = load_oc_settings()
    if not settings: return
    
    supliers_new_path = settings["paths"]["csv_path_new_product"]

    # =========================================================================
    # üîß –°–õ–û–í–ù–ò–ö –ü–ï–†–ï–ô–ú–ï–ù–£–í–ê–ù–ù–Ø –ê–¢–†–ò–ë–£–¢–Ü–í
    # –¢—É—Ç –º–∏ –≤–∫–∞–∑—É—î–º–æ: "–Ø–∫ –Ω–∞–∑–≤–∞–Ω–æ –Ω–∞ —Å–∞–π—Ç—ñ" : "–Ø–∫ –Ω–∞–∑–∏–≤–∞—î—Ç—å—Å—è –∫–æ–ª–æ–Ω–∫–∞ —É —Ñ–∞–π–ª—ñ"
    # =========================================================================
    ATTRIBUTE_NAME_MAPPING = {
        "–ö—Ä–∞—ó–Ω–∞": "–ó—Ä–æ–±–ª–µ–Ω–æ –≤|ua",
        "–ú–∞—Ä–∫–∞/–õ—ñ–Ω—ñ—è": "–ë—Ä–µ–Ω–¥|ua",
        # –î–æ–¥–∞–≤–∞–π—Ç–µ —Å—é–¥–∏ –Ω–æ–≤—ñ, —è–∫—â–æ –±—É–¥—É—Ç—å —Ä–æ–∑–±—ñ–∂–Ω–æ—Å—Ç—ñ
        # "–ú–∞—Ç–µ—Ä—ñ–∞–ª": "–°–∫–ª–∞–¥ —Ç–∫–∞–Ω–∏–Ω–∏|ua", 
    }

    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–∞–ø–∏ –∑–Ω–∞—á–µ–Ω—å (attribute.csv)
    replacements_map, raw_data = load_attributes_csv()
    changes_made = False
    max_raw_row_len = len(raw_data[0]) if raw_data and raw_data[0] else 12

    # –¢–æ—á–∫–∏ –≤—Å—Ç–∞–≤–∫–∏
    insertion_points = {} 
    current_block_name = None
    for i, row_raw in enumerate(raw_data[1:], start=1):
        first_col = row_raw[0].strip()
        if first_col: 
            current_block_name = first_col
            insertion_points[current_block_name] = i + 1
        elif current_block_name:
            insertion_points[current_block_name] = i + 1

    new_attributes_counter = {} 
    temp_file_path = supliers_new_path + ".temp"

    try:
        with open(supliers_new_path, mode="r", encoding="utf-8") as input_file, \
             open(temp_file_path, mode="w", encoding="utf-8", newline="") as output_file:

            reader = csv.DictReader(input_file)
            fieldnames = reader.fieldnames 
            
            if not fieldnames:
                logging.error("‚ùå –§–∞–π–ª –ø–æ—Ä–æ–∂–Ω—ñ–π!")
                return
            
            logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ –∫–æ–ª–æ–Ω–æ–∫ —É —Ñ–∞–π–ª—ñ: {len(fieldnames)}")

            writer = csv.DictWriter(output_file, fieldnames=fieldnames)
            writer.writeheader()

            processed_count = 0

            for row in reader:
                processed_count += 1
                row_values = list(row.values())
                product_url = row_values[1].strip() if len(row_values) > 1 else ""

                if not product_url or product_url.startswith("–ü–æ–º–∏–ª–∫–∞"):
                    writer.writerow(row)
                    continue

                try:
                    response = requests.get(product_url, timeout=10)
                    response.raise_for_status()
                    soup = BeautifulSoup(response.text, "html.parser")

                    parsed_attributes = {}
                    characteristics_div = soup.find("div", id="w0-tab0")
                    if characteristics_div and characteristics_div.find("table"):
                        for tr in characteristics_div.find("table").find_all("tr"):
                            cells = tr.find_all("td")
                            if len(cells) == 2:
                                key = cells[0].get_text(strip=True).replace(":", "")
                                value = cells[1].get_text(strip=True)
                                parsed_attributes[key] = value

                    other_attributes_list = []

                    # --- –û–±—Ä–æ–±–∫–∞ –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ ---
                    for attr_name_site, attr_value in parsed_attributes.items():
                        if attr_name_site == "–®—Ç—Ä–∏—Ö-–∫–æ–¥": continue

                        # === –õ–û–ì–Ü–ö–ê –í–ò–ó–ù–ê–ß–ï–ù–ù–Ø –ö–û–õ–û–ù–ö–ò ===
                        final_col_name = None
                        
                        # 1. –ü—Ä—ñ–æ—Ä–∏—Ç–µ—Ç: –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ä—É—á–Ω–∏–π –º–∞–ø–∏–Ω–≥ (–°–ª–æ–≤–Ω–∏–∫ –∑–≤–µ—Ä—Ö—É)
                        if attr_name_site in ATTRIBUTE_NAME_MAPPING:
                            mapped_name = ATTRIBUTE_NAME_MAPPING[attr_name_site]
                            if mapped_name in row:
                                final_col_name = mapped_name
                        
                        # 2. –ü—Ä—ñ–æ—Ä–∏—Ç–µ—Ç: –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π —Å—É—Ñ—ñ–∫—Å |ua
                        if not final_col_name:
                            target_col_ua = f"{attr_name_site}|ua"
                            if target_col_ua in row:
                                final_col_name = target_col_ua
                            elif attr_name_site in row: # –ü—Ä—è–º–∏–π –∑–±—ñ–≥ (–±–µ–∑ —Å—É—Ñ—ñ–∫—Å—ñ–≤)
                                final_col_name = attr_name_site
                        
                        # === –î–Ü–á –ó –ó–ù–ê–ô–î–ï–ù–û–Æ –ö–û–õ–û–ù–ö–û–Æ ===
                        if final_col_name:
                            original_value_lower = attr_value.strip().lower()
                            rules_for_this_attr = replacements_map.get(final_col_name, {})
                            
                            found_data = rules_for_this_attr.get(original_value_lower)

                            if found_data:
                                # –†–æ–∑–ø–∞–∫–æ–≤–∫–∞ –∑–Ω–∞—á–µ–Ω—å (Tuple check)
                                if isinstance(found_data, tuple) and len(found_data) >= 2:
                                    ua_new, ru_new = found_data
                                else:
                                    ua_new = str(found_data)
                                    ru_new = "" 

                                # –ü–∏—à–µ–º–æ UA
                                row[final_col_name] = ua_new
                                
                                # –ü–∏—à–µ–º–æ RU (–Ω–∞—Å—Ç—É–ø–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞)
                                if ru_new:
                                    try:
                                        current_idx = fieldnames.index(final_col_name)
                                        if current_idx + 1 < len(fieldnames):
                                            next_col_name = fieldnames[current_idx + 1]
                                            row[next_col_name] = ru_new
                                    except ValueError: pass 
                            else:
                                # –ù–æ–≤–µ –∑–Ω–∞—á–µ–Ω–Ω—è -> –ø–∏—à–µ–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª
                                row[final_col_name] = attr_value
                                
                                # –î–æ–¥–∞—î–º–æ –≤ attribute.csv
                                if original_value_lower not in rules_for_this_attr:
                                    insert_index = insertion_points.get(final_col_name)
                                    if insert_index is not None:
                                        new_raw_row = [""] * max_raw_row_len
                                        new_raw_row[2] = original_value_lower
                                        raw_data.insert(insert_index, new_raw_row)
                                        
                                        # –ö–µ—à—É—î–º–æ —è–∫ –∫–æ—Ä—Ç–µ–∂
                                        replacements_map.setdefault(final_col_name, {})[original_value_lower] = (attr_value, "") 
                                        
                                        changes_made = True
                                        new_attributes_counter[final_col_name] = new_attributes_counter.get(final_col_name, 0) + 1
                                        
                                        for k, v in insertion_points.items():
                                            if v >= insert_index: insertion_points[k] += 1
                        else:
                            # –ö–æ–ª–æ–Ω–∫–∏ –Ω–µ–º–∞—î –Ω—ñ –≤ –º–∞–ø–∏–Ω–≥—É, –Ω—ñ –Ω–∞–ø—Ä—è–º—É -> "–ù–æ–≤—ñ –∞—Ç—Ä–∏–±—É—Ç–∏"
                            other_attributes_list.append(f"{attr_name_site}:{attr_value}")

                    # --- –ó–∞–ø–∏—Å "–ù–æ–≤—ñ –∞—Ç—Ä–∏–±—É—Ç–∏" ---
                    if other_attributes_list:
                        new_content = ", ".join(other_attributes_list)
                        if "–ù–æ–≤—ñ –∞—Ç—Ä–∏–±—É—Ç–∏" in row:
                            current = row.get("–ù–æ–≤—ñ –∞—Ç—Ä–∏–±—É—Ç–∏", "")
                            row["–ù–æ–≤—ñ –∞—Ç—Ä–∏–±—É—Ç–∏"] = (current + ", " + new_content) if current else new_content

                    writer.writerow(row)

                except Exception as e:
                    logging.error(f"–ü–æ–º–∏–ª–∫–∞ URL {product_url}: {e}")
                    writer.writerow(row)

                time.sleep(random.uniform(1, 3))

        os.replace(temp_file_path, supliers_new_path)
        logging.info(f"–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –¢–æ–≤–∞—Ä—ñ–≤: {processed_count}")

        if changes_made:
            save_attributes_csv(raw_data)
            logging.info("===== –ó–í–Ü–¢ –ü–†–û –ù–û–í–Ü –ó–ù–ê–ß–ï–ù–ù–Ø =====")
            total_new = 0
            for attr_block, count in sorted(new_attributes_counter.items()):
                logging.info(f"‚Ä¢ {attr_block}: +{count}")
                total_new += count
            logging.info(f"–†–ê–ó–û–ú –¥–æ–¥–∞–Ω–æ: {total_new}")
            logging.info("==================================")
        else:
            logging.info("–ù–æ–≤–∏—Ö –∑–Ω–∞—á–µ–Ω—å –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ –Ω–µ –≤–∏—è–≤–ª–µ–Ω–æ.")
        
    except Exception as e:
        logging.error(f"–ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")
        if os.path.exists(temp_file_path): os.remove(temp_file_path)

# –û–°–ù–û–í–ù–ê –§–£–ù–ö–¶–Ü–Ø 5: –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü—ñ—è –∞—Ç—Ä–∏–±—É—Ç—ñ–≤
def apply_final_standardization():
    """
    –ü—Ä–æ—Ö–æ–¥–∏—Ç—å –ø–æ –≤–∂–µ —Å—Ñ–æ—Ä–º–æ–≤–∞–Ω–æ–º—É —Ñ–∞–π–ª—É —ñ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑—É—î –∑–Ω–∞—á–µ–Ω–Ω—è –∑–≥—ñ–¥–Ω–æ –∑ attribute.csv.
    1. –í–∏–ø—Ä–∞–≤–ª—è—î —Ä–µ–≥—ñ—Å—Ç—Ä (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥ "–≤–æ–¥–Ω–∞" -> "–í–æ–¥–Ω–∞").
    2. –û–Ω–æ–≤–ª—é—î —Ä–æ—Å—ñ–π—Å—å–∫–∏–π –ø–µ—Ä–µ–∫–ª–∞–¥ –≤ —Å—É—Å—ñ–¥–Ω—ñ–π –∫–æ–ª–æ–Ω—Ü—ñ.
    3. –ü—Ä–∞—Ü—é—î –Ω–∞ –æ—Å–Ω–æ–≤—ñ –Ω–∞–∑–≤ –∫–æ–ª–æ–Ω–æ–∫.
    """
    oc_log_message()
    logging.info("‚ñ∂ –§–£–ù–ö–¶–Ü–Ø 5. –§—ñ–Ω–∞–ª—å–Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü—ñ—è (UA + RU update)...")

    # --- 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å ---
    settings = load_oc_settings()
    if not settings: return

    csv_path = settings['paths']['csv_path_new_product']
    
    # --- 2. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø—Ä–∞–≤–∏–ª –∑–∞–º—ñ–Ω–∏ ---
    # –û—Ç—Ä–∏–º—É—î–º–æ —Å–ª–æ–≤–Ω–∏–∫: {'–û—Å–Ω–æ–≤–∞|ua': {'–≤–æ–¥–Ω–∞': ('–í–æ–¥–Ω–∞', '–í–æ–¥–Ω–∞—è')}}
    replacements_map, _ = load_attributes_csv()
    
    if not replacements_map:
        logging.warning("‚ö†Ô∏è attribute.csv –ø–æ—Ä–æ–∂–Ω—ñ–π –∞–±–æ –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏–≤—Å—è. –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü—ñ—è –ø—Ä–æ–ø—É—â–µ–Ω–∞.")
        return

    # --- 3. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ ---
    replacement_counter = {}  # {col_name: count}

    # --- 4. –û–±—Ä–æ–±–∫–∞ CSV ---
    temp_file_path = csv_path + '.final_temp'
    
    try:
        with open(csv_path, 'r', encoding='utf-8') as infile, \
             open(temp_file_path, 'w', encoding='utf-8', newline='') as outfile:

            reader = csv.DictReader(infile)
            fieldnames = reader.fieldnames
            
            if not fieldnames:
                logging.error("‚ùå –§–∞–π–ª –ø–æ—Ä–æ–∂–Ω—ñ–π!")
                return

            writer = csv.DictWriter(outfile, fieldnames=fieldnames)
            writer.writeheader()

            processed_rows = 0

            for row in reader:
                processed_rows += 1
                row_updated = False

                # –ü—Ä–æ—Ö–æ–¥–∏–º–æ –ø–æ –≤—Å—ñ—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö, –¥–ª—è —è–∫–∏—Ö —É –Ω–∞—Å —î –ø—Ä–∞–≤–∏–ª–∞ –≤ attribute.csv
                # col_name - —Ü–µ, –Ω–∞–ø—Ä–∏–∫–ª–∞–¥, "–û—Å–Ω–æ–≤–∞|ua" –∞–±–æ "–ö–æ–ª—ñ—Ä|ua"
                for col_name, rules in replacements_map.items():
                    
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —ñ—Å–Ω—É—î —Ç–∞–∫–∞ –∫–æ–ª–æ–Ω–∫–∞ —É —Ñ–∞–π–ª—ñ
                    if col_name in row:
                        current_value = row[col_name].strip()
                        
                        if not current_value:
                            continue

                        current_value_lower = current_value.lower()
                        
                        # –ß–∏ —î –ø—Ä–∞–≤–∏–ª–æ –¥–ª—è —Ü—å–æ–≥–æ –∑–Ω–∞—á–µ–Ω–Ω—è?
                        found_pair = rules.get(current_value_lower)

                        if found_pair:
                            # found_pair –º–∞—î –≤–∏–≥–ª—è–¥ ('–í–æ–¥–Ω–∞', '–í–æ–¥–Ω–∞—è')
                            # –ó–∞—Ö–∏—Å—Ç –≤—ñ–¥ —Å—Ç–∞—Ä–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç—É –¥–∞–Ω–∏—Ö
                            if isinstance(found_pair, tuple) and len(found_pair) >= 2:
                                ua_std, ru_std = found_pair
                            else:
                                ua_std = str(found_pair)
                                ru_std = ""

                            # 1. –û–Ω–æ–≤–ª—é—î–º–æ UA –∑–Ω–∞—á–µ–Ω–Ω—è (—è–∫—â–æ –≤—ñ–¥—Ä—ñ–∑–Ω—è—î—Ç—å—Å—è)
                            if row[col_name] != ua_std:
                                row[col_name] = ua_std
                                row_updated = True
                                # –õ–æ–≥—É—î–º–æ –∑–º—ñ–Ω—É
                                replacement_counter[col_name] = replacement_counter.get(col_name, 0) + 1

                            # 2. –û–Ω–æ–≤–ª—é—î–º–æ RU –∑–Ω–∞—á–µ–Ω–Ω—è (—Å—É—Å—ñ–¥–Ω—è –∫–æ–ª–æ–Ω–∫–∞)
                            if ru_std:
                                try:
                                    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ —ñ–Ω–¥–µ–∫—Å UA –∫–æ–ª–æ–Ω–∫–∏
                                    ua_idx = fieldnames.index(col_name)
                                    # –ë–µ—Ä–µ–º–æ –Ω–∞—Å—Ç—É–ø–Ω—É –∫–æ–ª–æ–Ω–∫—É (+1)
                                    if ua_idx + 1 < len(fieldnames):
                                        ru_col_name = fieldnames[ua_idx + 1]
                                        
                                        # –û–Ω–æ–≤–ª—é—î–º–æ RU, —è–∫—â–æ –≤–æ–Ω–æ –≤—ñ–¥—Ä—ñ–∑–Ω—è—î—Ç—å—Å—è –∞–±–æ –ø—É—Å—Ç–µ
                                        if row[ru_col_name] != ru_std:
                                            row[ru_col_name] = ru_std
                                            row_updated = True
                                except ValueError:
                                    pass # –Ø–∫—â–æ —â–æ—Å—å –ø—ñ—à–ª–æ –Ω–µ —Ç–∞–∫ –∑ —ñ–Ω–¥–µ–∫—Å–∞–º–∏

                writer.writerow(row)

        # --- 5. –ó–∞–º—ñ–Ω–∞ —Ñ–∞–π–ª—É ---
        os.replace(temp_file_path, csv_path)
        logging.info("–§—ñ–Ω–∞–ª—å–Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü—ñ—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –§–∞–π–ª –æ–Ω–æ–≤–ª–µ–Ω–æ.")

        # --- 6. –ü—ñ–¥—Å—É–º–∫–æ–≤–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è ---
        if replacement_counter:
            logging.info("===== –ó–í–Ü–¢ –ü–†–û –°–¢–ê–ù–î–ê–†–¢–ò–ó–ê–¶–Ü–Æ =====")
            for col, count in sorted(replacement_counter.items()):
                logging.info(f"‚Ä¢ {col}: —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–æ–≤–∞–Ω–æ {count} —Ä—è–¥–∫—ñ–≤")
            logging.info(f"–†–ê–ó–û–ú –∑–º—ñ–Ω: {sum(replacement_counter.values())}")
            logging.info("===================================")
        else:
            logging.info("–£—Å—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –≤–∂–µ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—é—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º.")

    except Exception as e:
        logging.error(f"–ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü—ñ—ó: {e}")
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)

def fill_product_category():
    """
    –ó–∞–ø–æ–≤–Ω—é—î —Å–ª—É–∂–±–æ–≤—ñ –∫–æ–ª–æ–Ω–∫–∏ —É csv:
    - Q (–∫–∞—Ç–µ–≥–æ—Ä—ñ—è) –Ω–∞ –æ—Å–Ω–æ–≤—ñ M, N, O
    - T (–ø–æ–∑–Ω–∞—á–∫–∏) –Ω–∞ –æ—Å–Ω–æ–≤—ñ –Ω–∞–∑–≤–∏ —Ç–æ–≤–∞—Ä—É G
    - U (Rank Math) –Ω–∞ –æ—Å–Ω–æ–≤—ñ –Ω–∞–∑–≤–∏ —Ç–æ–≤–∞—Ä—É G
    - AV (pa_used) –Ω–∞ –æ—Å–Ω–æ–≤—ñ category.csv
    - V, W, X, Y, AZ —Ñ—ñ–∫—Å–æ–≤–∞–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏
    - Z (–∫–æ—Ä–æ—Ç–∫–∏–π –æ–ø–∏—Å) –∑ H
    - AX (–¥–∞—Ç–∞)
    –ü—Ä–∞—Ü—é—î —Ç—ñ–ª—å–∫–∏ –¥–ª—è –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ –∑ ID=1
    """
    oc_log_message()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 5. –ü–æ—á–∏–Ω–∞—é –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó —Ç–∞ —Å–ª—É–∂–±–æ–≤–∏—Ö –∫–æ–ª–æ–Ω–æ–∫...")

    settings = load_oc_settings()
    try:
        csv_path = settings['paths']['csv_path_supliers_1_new']
        supplier_id = 1
        name_ukr = settings['suppliers']['1']['name_ukr']
    except (TypeError, KeyError) as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å: {e}")
        return

    # –Ü–Ω–¥–µ–∫—Å–∏ –∫–æ–ª–æ–Ω–æ–∫
    M, N, O = 12, 13, 14
    G, H = 6, 7
    Q, T, U = 16, 19, 20
    Z, V, W, X, Y = 25, 21, 22, 23, 24
    AV, AX, AZ = 47, 49, 51

    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø—Ä–∞–≤–∏–ª –∫–∞—Ç–µ–≥–æ—Ä—ñ–π —ñ –ø–æ–∑–Ω–∞—á–æ–∫
    category_map, raw_category = load_category_csv()
    rules_category = category_map.get(supplier_id, {})
    poznachky_list = load_poznachky_csv()
    changes_category = False
    max_row_len_category = len(raw_category[0]) if raw_category else 5

    # –°—Ç–≤–æ—Ä—é—î–º–æ –º–∞–ø—É –¥–ª—è pa_used
    pa_used_map = {}
    for row in raw_category:
        if len(row) > 5 and (row[0].strip() == str(supplier_id) or row[0].strip() == ''):
            key = tuple(v.strip().lower() for v in row[1:4])
            pa_used_map[key] = row[5].strip()

    logging.info(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(pa_used_map)} –ø—Ä–∞–≤–∏–ª pa_used")

    current_date = datetime.now().strftime('%Y-%m-%dT00:00:00')

    # –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –Ω–æ–≤–æ–≥–æ —Ä—è–¥–∫–∞ —É category.csv
    def get_insert_index(supplier_id, raw_data):
        insert_index = len(raw_data)
        found_block = False
        for i, r in enumerate(raw_data):
            if r and r[0].strip().isdigit():
                try:
                    cur_id = int(r[0].strip())
                    if cur_id == supplier_id:
                        found_block = True
                        insert_index = i + 1
                    elif cur_id > supplier_id and found_block:
                        return i
                except ValueError:
                    continue
            elif found_block:
                insert_index = i + 1
        return insert_index

    temp_path = csv_path + '.category_temp'
    try:
        with open(csv_path, 'r', encoding='utf-8') as infile, \
             open(temp_path, 'w', encoding='utf-8', newline='') as outfile:

            reader = csv.reader(infile)
            writer = csv.writer(outfile)
            headers = next(reader)
            writer.writerow(headers)

            for idx, row in enumerate(reader):
                # –†–æ–∑—à–∏—Ä—é—î–º–æ —Ä—è–¥–æ–∫ –∑–∞ –ø–æ—Ç—Ä–µ–±–∏
                max_col = max(M, N, O, Q, T, U, V, W, X, Y, Z, AV, AX, AZ, G, H)
                if len(row) <= max_col:
                    row.extend([''] * (max_col + 1 - len(row)))

                product_name = row[G].strip()
                product_desc = row[H]

                key = tuple(row[i].strip().lower() for i in (M, N, O))

                # --- –ö–∞—Ç–µ–≥–æ—Ä—ñ—è Q ---
                category_val = rules_category.get(key)
                if category_val is not None:
                    row[Q] = category_val or ""
                else:
                    # –î–æ–¥–∞—î–º–æ –Ω–æ–≤–∏–π —Ä—è–¥–æ–∫ —É category.csv
                    insert_idx = get_insert_index(supplier_id, raw_category)
                    new_row = [''] + list(row[M:O+1]) + [''] * (max_row_len_category - 4)
                    raw_category.insert(insert_idx, new_row)
                    rules_category[key] = ""
                    changes_category = True
                    logging.warning(f"–†—è–¥–æ–∫ {idx + 2}: –î–æ–¥–∞–Ω–∞ –Ω–æ–≤–∞ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—è –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó {key}")

                # --- –ü–æ–∑–Ω–∞—á–∫–∏ T ---
                if product_name and poznachky_list:
                    found_tags = []
                    covered = []
                    name_lower = product_name.lower()
                    for tag in poznachky_list:
                        if tag in name_lower:
                            start, end = name_lower.find(tag), name_lower.find(tag) + len(tag)
                            if not any(s <= start and end <= e for s, e in covered):
                                found_tags.append(tag.capitalize())
                                covered.append((start, end))
                                covered.sort(key=lambda x: x[1]-x[0], reverse=True)
                    if found_tags:
                        row[T] = ', '.join(found_tags)

                # --- Rank Math U ---
                if product_name:
                    cleaned = re.sub(r'[–∞-—è–ê-–Ø0-9]', '', product_name)
                    cleaned = re.sub(r'[^a-zA-Z\s]', '', cleaned)
                    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
                    row[U] = cleaned

                # --- pa_used AV ---
                pa_val = pa_used_map.get(key)
                if pa_val:
                    row[AV] = pa_val


                # --- –§—ñ–∫—Å–æ–≤–∞–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏ ---
                row[V] = name_ukr
                row[W] = "draft"
                row[X] = "yes"
                row[Y] = "none"
                row[AZ] = "simple"
                row[AX] = current_date

                # --- –ö–æ—Ä–æ—Ç–∫–∏–π –æ–ø–∏—Å Z ---
                if product_desc:
                    row[Z] = product_desc.split('\\n', 1)[0].strip()
                else:
                    row[Z] = ""

                writer.writerow(row)

        os.replace(temp_path, csv_path)
        logging.info("–ó–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–π —Ç–∞ —Å–ª—É–∂–±–æ–≤–∏—Ö –∫–æ–ª–æ–Ω–æ–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")

        if changes_category:
            save_category_csv(raw_category)
        else:
            logging.info("–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è category.csv –Ω–µ –ø–æ—Ç—Ä—ñ–±–Ω–µ. –ó–º—ñ–Ω: False.")

    except Exception as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—ñ –∫–æ–ª–æ–Ω–æ–∫: {e}")
        if os.path.exists(temp_path):
            os.remove(temp_path)

def refill_product_category():
    """
    –ü–æ–≤—Ç–æ—Ä–Ω–æ –∑–∞–ø–æ–≤–Ω—é—î –∫–æ–ª–æ–Ω–∫–∏ Q (–ö–∞—Ç–µ–≥–æ—Ä—ñ—è) —Ç–∞ AV (pa_used) —É 1.csv
    –Ω–∞ –æ—Å–Ω–æ–≤—ñ –æ–Ω–æ–≤–ª–µ–Ω–∏—Ö –ø—Ä–∞–≤–∏–ª —É category.csv.
    –ù–ï –¥–æ–¥–∞—î –Ω–æ–≤—ñ —Ä—è–¥–∫–∏ —É category.csv.
    –õ–æ–≥—É–≤–∞–Ω–Ω—è –ø–æ–∫–∞–∑—É—î, —è–∫—ñ —Ä—è–¥–∫–∏ –æ–Ω–æ–≤–ª–µ–Ω—ñ.
    """
    oc_log_message()
    logging.info("–§—É–Ω–∫—Ü—ñ—è 6. –ü–æ—á–∏–Ω–∞—é –ø–æ–≤—Ç–æ—Ä–Ω–µ –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–π —Ç–∞ pa_used —É 1.csv...")

    # --- 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å ---
    settings = load_oc_settings()
    try:
        csv_path = settings['paths']['csv_path_supliers_1_new']
        supplier_id = 1
    except (TypeError, KeyError) as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –¥–æ—Å—Ç—É–ø—É –¥–æ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å: {e}")
        return

    # --- 2. –Ü–Ω–¥–µ–∫—Å–∏ –∫–æ–ª–æ–Ω–æ–∫ CSV ---
    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –æ–¥—Ä–∞–∑—É —á–∏—Å–ª–∞, –±–µ–∑ –¥–æ–≤–≥–∏—Ö –∑–º—ñ–Ω–Ω–∏—Ö
    M, N, O = 12, 13, 14        # name_1, name_2, name_3
    Q, AV = 16, 47              # –ö–∞—Ç–µ–≥–æ—Ä—ñ—è —Ç–∞ pa_used
    max_index = max(M, N, O, Q, AV)
    missing_category_rows = []  # —Å–ø–∏—Å–æ–∫ —Ä—è–¥–∫—ñ–≤ –∑ –ø–æ—Ä–æ–∂–Ω—å–æ—é –∫–∞—Ç–µ–≥–æ—Ä—ñ—î—é

    # --- 3. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø—Ä–∞–≤–∏–ª –∫–∞—Ç–µ–≥–æ—Ä—ñ–π —Ç–∞ pa_used ---
    category_map, raw_category = load_category_csv()
    rules_category = {}
    pa_used_map = {}
    supplier_str = str(supplier_id)

    for row in raw_category:
        if len(row) > 5:
            supplier_value = row[0].strip()
            if supplier_value == supplier_str or supplier_value == '':
                key = tuple(v.strip().lower() for v in row[1:4])  # –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—è M,N,O
                rules_category[key] = row[4].strip() if len(row) > 4 else ""
                pa_used_map[key] = row[5].strip() if len(row) > 5 else ""

    logging.info(f"–ó—á–∏—Ç–∞–Ω–æ {len(rules_category)} –ø—Ä–∞–≤–∏–ª –¥–ª—è –ö–∞—Ç–µ–≥–æ—Ä—ñ—ó (Q) —Ç–∞ {len(pa_used_map)} –ø—Ä–∞–≤–∏–ª –¥–ª—è pa_used (AV)")

    # --- 4. –û–±—Ä–æ–±–∫–∞ CSV ---
    temp_path = csv_path + '.refill_temp'
    updated_rows = 0

    try:
        with open(csv_path, 'r', encoding='utf-8') as infile, \
             open(temp_path, 'w', encoding='utf-8', newline='') as outfile:

            reader = csv.reader(infile)
            writer = csv.writer(outfile)

            headers = next(reader)
            writer.writerow(headers)

            for idx, row in enumerate(reader):
                # –†–æ–∑—à–∏—Ä—é—î–º–æ —Ä—è–¥–æ–∫, —â–æ–± –Ω–µ –≤–∏—Ö–æ–¥–∏—Ç–∏ –∑–∞ –º–µ–∂—ñ
                if len(row) <= max_index:
                    row.extend([''] * (max_index + 1 - len(row)))

                # --- 4.1 –ö–ª—é—á –ø–æ—à—É–∫—É ---
                key = tuple(row[i].strip().lower() for i in (M, N, O))
                initial_category = row[Q].strip()
                initial_pa_used = row[AV].strip()
                row_changed = False

                # --- 4.2 –ü–æ–≤—Ç–æ—Ä–Ω–µ –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –ö–∞—Ç–µ–≥–æ—Ä—ñ—ó Q ---
                category_val = rules_category.get(key)
                if category_val and category_val != initial_category:
                    row[Q] = category_val
                    row_changed = True
                    logging.info(f"–†—è–¥–æ–∫ {idx + 2}: Q (–ö–∞—Ç–µ–≥–æ—Ä—ñ—è) –æ–Ω–æ–≤–ª–µ–Ω–æ. –ö–ª—é—á: {key}, –ó–Ω–∞—á–µ–Ω–Ω—è: '{category_val}'")

                # --- 4.3 –ü–æ–≤—Ç–æ—Ä–Ω–µ –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è pa_used AV ---
                pa_val = pa_used_map.get(key)
                if pa_val and pa_val != initial_pa_used:
                    row[AV] = pa_val
                    row_changed = True
                    logging.info(f"–†—è–¥–æ–∫ {idx + 2}: AV (pa_used) –æ–Ω–æ–≤–ª–µ–Ω–æ. –ö–ª—é—á: {key}, –ó–Ω–∞—á–µ–Ω–Ω—è: '{pa_val}'")

                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø–æ—Ä–æ–∂–Ω—å–æ—ó –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó –ø—ñ—Å–ª—è –æ–Ω–æ–≤–ª–µ–Ω–Ω—è
                if not row[Q].strip():
                    missing_category_rows.append(idx + 2)  # –∑–±–µ—Ä—ñ–≥–∞—î–º–æ –Ω–æ–º–µ—Ä —Ä—è–¥–∫–∞ —É —Ñ–∞–π–ª—ñ

                if row_changed:
                    updated_rows += 1

                writer.writerow(row)

        # --- 5. –ó–∞–º—ñ–Ω—é—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π CSV ---
        os.replace(temp_path, csv_path)
        logging.info(f"–ü–æ–≤—Ç–æ—Ä–Ω–µ –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –û–Ω–æ–≤–ª–µ–Ω–æ {updated_rows} —Ä—è–¥–∫—ñ–≤.")

        # --- –õ–æ–≥—É–≤–∞–Ω–Ω—è —Ä—è–¥–∫—ñ–≤ –∑ –ø–æ—Ä–æ–∂–Ω—å–æ—é –∫–∞—Ç–µ–≥–æ—Ä—ñ—î—é ---
        for row_num in missing_category_rows:
            logging.warning(f"–£–í–ê–ì–ê —Ä—è–¥–æ–∫ {row_num} –Ω–µ –∑–∞–ø–æ–≤–Ω–µ–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è!")

    except Exception as e:
        logging.error(f"–ù–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ–º—É –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—ñ: {e}")
        if os.path.exists(temp_path):
            os.remove(temp_path)

def separate_existing_products():
    """
    –ó–≤—ñ—Ä—è—î —à—Ç—Ä–∏—Ö–∫–æ–¥–∏ 1.csv –∑ –±–∞–∑–æ—é (zalishki.csv),
    –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—å –∑–Ω–∞–π–¥–µ–Ω—ñ —Ç–æ–≤–∞—Ä–∏ —É old_prod_new_SHK.csv,
    –≤–∏–¥–∞–ª—è—î —ó—Ö –∑ 1.csv —Ç–∞ —Ñ–æ—Ä–º—É—î –ø—ñ–¥—Å—É–º–∫–æ–≤—É —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É.
    –ö–æ–ª–æ–Ω–∫–∏ —Ç–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ—Å—Ç—ñ old -> new –≤–∏–Ω–µ—Å–µ–Ω—ñ —É settings.json.
    """
    oc_log_message()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 7. –ü–æ—á–∏–Ω–∞—é –∑–≤—ñ—Ä–∫—É 1.csv –∑—ñ —à—Ç—Ä–∏—Ö–∫–æ–¥–∞–º–∏ –±–∞–∑–∏ (zalishki.csv)...")

    settings = load_oc_settings()
    try:
        sl_new_path = settings['paths']['csv_path_supliers_1_new']
        zalishki_path = settings['paths']['output_file']
        sl_old_prod_shk_path = settings['paths']['csv_path_sl_old_prod_new_shk']
        column_mapping = settings['suppliers']['1']['column_mapping_sl_old_to_sl_new']
    except KeyError as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó. –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —à–ª—è—Ö –∞–±–æ –º–∞–ø—É –∫–æ–ª–æ–Ω–æ–∫: {e}")
        return

    # --- 0. –ó—á–∏—Ç—É–≤–∞–Ω–Ω—è —ñ—Å–Ω—É—é—á–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞ old_prod_new_SHK.csv ---
    sl_old_header = []
    try:
        if os.path.exists(sl_old_prod_shk_path):
            with open(sl_old_prod_shk_path, mode='r', encoding='utf-8') as f:
                reader = csv.reader(f)
                sl_old_header = next(reader, [])
        else:
            logging.warning("–§–∞–π–ª old_prod_new_SHK.csv –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ ‚Äî —Å—Ç–≤–æ—Ä—é—é –Ω–æ–≤–∏–π —ñ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º.")
            sl_old_header_base = [
                'id', 'sku', '–ú–µ—Ç–∞: url_lutsk', '–ú–µ—Ç–∞: shtrih_cod', '–ú–µ—Ç–∞: artykul_lutsk', '–ü–æ–∑–Ω–∞—á–∫–∏',
                'rank_math_focus_keyword', '–ú–µ—Ç–∞: postachalnyk', 'manage_stock', 'tax_status', 'excerpt'
            ]
            # –î–æ–¥–∞—î–º–æ –∞—Ç—Ä–∏–±—É—Ç–∏ —Ç–∞ –¥–æ–¥–∞—Ç–∫–æ–≤—ñ –∫–æ–ª–æ–Ω–∫–∏ (–±–µ–∑ attribute_none)
            sl_old_header = sl_old_header_base + [f'attribute_{i}' for i in range(1, 24)] + [
                'content', 'post_date', 'product_type'
            ]

        # –û—á–∏—â–∞—î–º–æ —Ñ–∞–π–ª, –∞–ª–µ –∑–∞–ª–∏—à–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫
        with open(sl_old_prod_shk_path, mode='w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(sl_old_header)

        logging.info("–§–∞–π–ª old_prod_new_SHK.csv –æ—á–∏—â–µ–Ω–æ, –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∑–∞–ª–∏—à–µ–Ω–æ –±–µ–∑ –∑–º—ñ–Ω.")
    except Exception as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó old_prod_new_SHK.csv: {e}")
        return

    # --- 1. –ó—á–∏—Ç—É–≤–∞–Ω–Ω—è –±–∞–∑–∏ —à—Ç—Ä–∏—Ö–∫–æ–¥—ñ–≤ ---
    zalishki_map = {}  # {shk: (id, sku)}
    try:
        with open(zalishki_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            next(reader)  # –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫
            for row in reader:
                if len(row) > 7:
                    shk = row[7].strip()
                    if shk:
                        zalishki_map[shk] = (row[0].strip(), row[1].strip())
        logging.info(f"–ó—á–∏—Ç–∞–Ω–æ {len(zalishki_map)} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —à—Ç—Ä–∏—Ö–∫–æ–¥—ñ–≤ –∑ –±–∞–∑–∏.")
    except Exception as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —á–∏—Ç–∞–Ω–Ω—ñ –±–∞–∑–∏: {e}")
        return

    # --- 2. –û–±—Ä–æ–±–∫–∞ 1.csv —Ç–∞ —Ñ–æ—Ä–º—É–≤–∞–Ω–Ω—è —Å–ø–∏—Å–∫—ñ–≤ ---
    items_to_keep = []
    items_to_move = []

    try:
        with open(sl_new_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            header = next(reader)
            items_to_keep.append(header)

            for row in reader:
                # –†–æ–∑—à–∏—Ä—é—î–º–æ —Ä—è–¥–æ–∫ –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —ñ–Ω–¥–µ–∫—Å—É —É –º–∞–ø—ñ
                max_index = max(column_mapping.values())
                if len(row) <= max_index:
                    row.extend([''] * (max_index + 1 - len(row)))

                shk_value = row[2].strip()  # C (–®—Ç—Ä–∏—Ö–∫–æ–¥)
                if shk_value in zalishki_map:
                    item_id, item_sku = zalishki_map[shk_value]

                    # –§–æ—Ä–º—É—î–º–æ –Ω–æ–≤–∏–π —Ä—è–¥–æ–∫ –¥–ª—è old_prod_new_SHK.csv
                    new_row = [''] * len(sl_old_header)
                    new_row[0] = item_id
                    new_row[1] = item_sku

                    for sl_old_idx_str, sl_new_idx in column_mapping.items():
                        sl_old_idx = int(sl_old_idx_str)  # –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ –∫–ª—é—á —É int
                        if sl_new_idx < len(row):
                            new_row[sl_old_idx] = row[sl_new_idx]

                    items_to_move.append(new_row)
                else:
                    items_to_keep.append(row)

        # --- 3. –ó–∞–ø–∏—Å –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ ---
        if items_to_move:
            with open(sl_old_prod_shk_path, 'a', encoding='utf-8', newline='') as f:
                writer = csv.writer(f)
                writer.writerows(items_to_move)
            logging.info(f"–ü–µ—Ä–µ–Ω–µ—Å–µ–Ω–æ {len(items_to_move)} —ñ—Å–Ω—É—é—á–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ —É old_prod_new_SHK.csv.")
        else:
            logging.info("–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∂–æ–¥–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä—É –∑ —ñ—Å–Ω—É—é—á–∏–º —à—Ç—Ä–∏—Ö–∫–æ–¥–æ–º —É –±–∞–∑—ñ.")

        # --- 4. –ó–∞–ø–∏—Å –æ–Ω–æ–≤–ª–µ–Ω–æ–≥–æ 1.csv ---
        temp_path = sl_new_path + '.temp'
        with open(temp_path, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerows(items_to_keep)
        os.replace(temp_path, sl_new_path)
        logging.info(f"1.csv –æ–Ω–æ–≤–ª–µ–Ω–æ. –ó–∞–ª–∏—à–∏–ª–æ—Å—å {len(items_to_keep)-1} –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ –¥–ª—è —ñ–º–ø–æ—Ä—Ç—É.")

    except Exception as e:
        logging.error(f"–ù–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –æ–±—Ä–æ–±–∫–∏ 1.csv: {e}")
        if 'temp_path' in locals() and os.path.exists(temp_path):
            os.remove(temp_path)

def assign_new_sku_to_products():
    """
    –ó–Ω–∞—Ö–æ–¥–∏—Ç—å –Ω–∞–π–±—ñ–ª—å—à–∏–π SKU —É zalishki.csv (—Å–æ—Ä—Ç—É—î –ø–æ –∫–æ–ª–æ–Ω—Ü—ñ B(1))
    —ñ –ø—Ä–∏—Å–≤–æ—é—î –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ SKU —Ç–æ–≤–∞—Ä–∞–º –±–µ–∑ SKU —É –∫–æ–ª–æ–Ω—Ü—ñ P(15) —Ñ–∞–π–ª—É 1.csv.
    """
    oc_log_message()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 8. –ü–æ—á–∏–Ω–∞—é –ø—Ä–∏—Å–≤–æ—î–Ω–Ω—è –Ω–æ–≤–∏—Ö SKU —Ç–æ–≤–∞—Ä–∞–º —É 1.csv...")

    # --- 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å ---
    settings = load_oc_settings()
    try:
        sl_new_path = settings['paths']['csv_path_supliers_1_new']
        zalishki_path = settings['paths']['output_file']
    except KeyError as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó. –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —à–ª—è—Ö: {e}")
        return

    # --- 2. –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è —ñ–Ω–¥–µ–∫—Å—É SKU —É 1.csv ---
    SKU_COL_INDEX = 15  # P
    ZALISHKI_SKU_INDEX = 1  # B

    # --- 3. –ó–Ω–∞—Ö–æ–¥–∏–º–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π SKU —É zalishki.csv ---
    try:
        with open(zalishki_path, mode='r', encoding='utf-8') as f:
            reader = csv.reader(f)
            next(reader, None)  # –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫
            sku_list = []
            for row in reader:
                if len(row) > ZALISHKI_SKU_INDEX:
                    val = row[ZALISHKI_SKU_INDEX].strip()
                    if val.isdigit():
                        sku_list.append(int(val))

            if not sku_list:
                logging.warning("–£ –±–∞–∑—ñ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∂–æ–¥–Ω–æ–≥–æ —á–∏—Å–ª–æ–≤–æ–≥–æ SKU. –ü—Ä–∏—Å–≤–æ—î–Ω–Ω—è –Ω–µ–º–æ–∂–ª–∏–≤–µ.")
                return

            sku_list.sort()
            last_sku = sku_list[-1]
            logging.info(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π SKU —É –±–∞–∑—ñ: {last_sku}")

    except FileNotFoundError:
        logging.error(f"–§–∞–π–ª –±–∞–∑–∏ zalishki.csv –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∑–∞ —à–ª—è—Ö–æ–º: {zalishki_path}")
        return
    except Exception as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —á–∏—Ç–∞–Ω–Ω—ñ zalishki.csv: {e}")
        return

    # --- 4. –ü—Ä–∏—Å–≤–æ—î–Ω–Ω—è –Ω–æ–≤–∏—Ö SKU —É 1.csv ---
    next_sku = last_sku + 1
    assigned_count = 0
    temp_path = sl_new_path + '.temp'

    try:
        with open(sl_new_path, mode='r', encoding='utf-8', newline='') as input_file:
            reader = csv.reader(input_file)
            header = next(reader, None)
            rows = [header] if header else []

            for row in reader:
                if len(row) <= SKU_COL_INDEX:
                    row.extend([''] * (SKU_COL_INDEX + 1 - len(row)))

                current_sku = row[SKU_COL_INDEX].strip()
                if not current_sku:
                    row[SKU_COL_INDEX] = str(next_sku)
                    assigned_count += 1
                    next_sku += 1

                rows.append(row)

        # --- 5. –ó–∞–ø–∏—Å –æ–Ω–æ–≤–ª–µ–Ω–æ–≥–æ CSV ---
        if assigned_count > 0:
            with open(temp_path, mode='w', encoding='utf-8', newline='') as f:
                writer = csv.writer(f)
                writer.writerows(rows)
            os.replace(temp_path, sl_new_path)
            logging.info(f"‚úÖ –£—Å–ø—ñ—à–Ω–æ –ø—Ä–∏—Å–≤–æ—î–Ω–æ {assigned_count} –Ω–æ–≤–∏—Ö SKU. –ù–∞—Å—Ç—É–ø–Ω–∏–π SKU –±—É–¥–µ {next_sku}.")
        else:
            logging.info("–£—Å—ñ —Ç–æ–≤–∞—Ä–∏ –≤–∂–µ –º–∞—é—Ç—å SKU. –ó–º—ñ–Ω –Ω–µ –≤–Ω–µ—Å–µ–Ω–æ.")

    except FileNotFoundError:
        logging.error(f"–§–∞–π–ª 1.csv –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∑–∞ —à–ª—è—Ö–æ–º")
    except Exception as e:
        logging.error(f"–ù–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –ø—Ä–∏—Å–≤–æ—î–Ω–Ω—è SKU: {e}")
        if os.path.exists(temp_path):
            os.remove(temp_path)