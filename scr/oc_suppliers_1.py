import csv
import logging
import os
import random
import time
import requests
import re
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from scr.oc_base_function import oc_log_message, load_oc_settings, load_attributes_csv, save_attributes_csv, \
                                load_category_csv, append_new_categories, load_poznachky_csv, clear_directory, \
                                download_product_images, move_gifs, convert_to_webp_square, sync_webp_column_named, \
                                copy_to_site, fill_opencart_paths_single_file, get_deepl_usage, translate_text_deepl, \
                                get_first_sentence

# –û–°–ù–û–í–ù–ê –§–£–ù–ö–¶–Ü–Ø 1: –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∑–º—ñ–Ω–∏ –∞—Ä—Ç–∏–∫—É–ª—É —ñ —à—Ç—Ä–∏—Ö–∫–æ–¥—É
def find_change_art_shtrihcod():
    """
    –ü–µ—Ä–µ–≤—ñ—Ä—è—î —Ä–æ–∑–±—ñ–∂–Ω–æ—Å—Ç—ñ:
    1) –ø–æ —à—Ç—Ä–∏—Ö–∫–æ–¥—É (–ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É)
    2) –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É (–ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –ø–æ —à—Ç—Ä–∏—Ö–∫–æ–¥—É)
    –£—Å—ñ —Ä–æ–∑–±—ñ–∂–Ω–æ—Å—Ç—ñ –∑–∞–ø–∏—Å—É—î —É change_art_shtrihcod
    """

    oc_log_message()
    logging.info("‚ñ∂ –°—Ç–∞—Ä—Ç –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –∞—Ä—Ç–∏–∫—É–ª—ñ–≤ —ñ —à—Ç—Ä–∏—Ö–∫–æ–¥—ñ–≤ (2 –Ω–∞–ø—Ä—è–º–∫–∏)")

    settings = load_oc_settings()
    if not settings:
        logging.info("‚ùå oc_settings.yaml –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ")
        return

    site_csv = settings["paths"]["output_file"]
    supplier_csv = settings["suppliers"][1]["csv_path"]
    result_csv = settings["paths"]["change_art_shtrihcod"]

    # --------------------------------------------------
    # 1. –ß–∏—Ç–∞—î–º–æ –ø—Ä–∞–π—Å –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞
    # --------------------------------------------------
    supplier_by_artykul = {}   # –ö–æ–¥_—Ç–æ–≤–∞—Ä–∞ -> –®—Ç—Ä–∏—Ö_–∫–æ–¥
    supplier_by_shtrih = {}    # –®—Ç—Ä–∏—Ö_–∫–æ–¥  -> –ö–æ–¥_—Ç–æ–≤–∞—Ä–∞

    with open(supplier_csv, newline="", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f, delimiter=";")

        for row in reader:
            artykul = row.get("–ö–æ–¥_—Ç–æ–≤–∞—Ä–∞", "").strip()
            shtrih = row.get("–®—Ç—Ä–∏—Ö_–∫–æ–¥", "").strip()

            if artykul and shtrih:
                supplier_by_artykul[artykul] = shtrih
                supplier_by_shtrih[shtrih] = artykul

    logging.info(
        f"‚Ñπ –¢–æ–≤–∞—Ä—ñ–≤ –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞: "
        f"{len(supplier_by_artykul)} (–ø–æ –∞—Ä—Ç–∏–∫—É–ª—É), "
        f"{len(supplier_by_shtrih)} (–ø–æ —à—Ç—Ä–∏—Ö–∫–æ–¥—É)"
    )

    # --------------------------------------------------
    # 2. –û—á–∏—â–µ–Ω–Ω—è —Ñ–∞–π–ª—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
    # --------------------------------------------------
    headers = [
        "sku",
        "shtrih_cod",
        "artykul_lutsk",
        "–ö–æ–¥_—Ç–æ–≤–∞—Ä–∞",
        "–®—Ç—Ä–∏—Ö_–∫–æ–¥"
    ]

    with open(result_csv, "w", newline="", encoding="utf-8") as f:
        csv.writer(f).writerow(headers)

    logging.info("üßπ change_art_shtrihcod –æ—á–∏—â–µ–Ω–æ")

    # --------------------------------------------------
    # 3. –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è (2 –Ω–∞–ø—Ä—è–º–∫–∏)
    # --------------------------------------------------
    checked = 0
    diff_count = 0
    written_keys = set()  # –∑–∞—Ö–∏—Å—Ç –≤—ñ–¥ –¥—É–±–ª—ñ–≤

    with open(site_csv, newline="", encoding="utf-8") as site_f, \
         open(result_csv, "a", newline="", encoding="utf-8") as out_f:

        site_reader = csv.DictReader(site_f)
        writer = csv.writer(out_f)

        for row in site_reader:
            sku = row.get("sku", "").strip()
            site_shtrih = row.get("shtrih_cod", "").strip()
            site_artykul = row.get("artykul_lutsk", "").strip()

            # ---------- –ü–†–ê–í–ò–õ–û 1 ----------
            # –Ñ –∞—Ä—Ç–∏–∫—É–ª ‚Üí –ø–æ—Ä—ñ–≤–Ω—é—î–º–æ —à—Ç—Ä–∏—Ö–∫–æ–¥
            if site_artykul and site_artykul in supplier_by_artykul:
                checked += 1
                supplier_shtrih = supplier_by_artykul[site_artykul]

                if site_shtrih != supplier_shtrih:
                    key = (sku, site_artykul, supplier_shtrih)
                    if key not in written_keys:
                        writer.writerow([
                            sku,
                            site_shtrih,
                            site_artykul,
                            site_artykul,
                            supplier_shtrih
                        ])
                        written_keys.add(key)
                        diff_count += 1

            # ---------- –ü–†–ê–í–ò–õ–û 2 ----------
            # –Ñ —à—Ç—Ä–∏—Ö–∫–æ–¥ ‚Üí –ø–æ—Ä—ñ–≤–Ω—é—î–º–æ –∞—Ä—Ç–∏–∫—É–ª
            if site_shtrih and site_shtrih in supplier_by_shtrih:
                checked += 1
                supplier_artykul = supplier_by_shtrih[site_shtrih]

                if site_artykul != supplier_artykul:
                    key = (sku, site_shtrih, supplier_artykul)
                    if key not in written_keys:
                        writer.writerow([
                            sku,
                            site_shtrih,
                            site_artykul,
                            supplier_artykul,
                            site_shtrih
                        ])
                        written_keys.add(key)
                        diff_count += 1

    # --------------------------------------------------
    # 4. –ü—ñ–¥—Å—É–º–æ–∫
    # --------------------------------------------------
    logging.info(f"‚úÖ –ü–µ—Ä–µ–≤—ñ—Ä–µ–Ω–æ –ø–æ–∑–∏—Ü—ñ–π: {checked}")
    logging.info(f"‚ö† –ó–Ω–∞–π–¥–µ–Ω–æ —Ä–æ–∑–±—ñ–∂–Ω–æ—Å—Ç–µ–π: {diff_count}")

# –û–°–ù–û–í–ù–ê –§–£–ù–ö–¶–Ü–Ø 2: –ó–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤
def find_new_products():
    """
    –ü–æ—Ä—ñ–≤–Ω—é—î –∞—Ä—Ç–∏–∫—É–ª–∏ —Ç–æ–≤–∞—Ä—ñ–≤ –∑ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç–∞ –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ –∑ –∞—Ä—Ç–∏–∫—É–ª–∞–º–∏,
    —â–æ —î –Ω–∞ —Å–∞–π—Ç—ñ, —ñ –∑–∞–ø–∏—Å—É—î –Ω–æ–≤—ñ —Ç–æ–≤–∞—Ä–∏ –≤ –æ–∫—Ä–µ–º–∏–π —Ñ–∞–π–ª.
    """
    # --- 1. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ª–æ–≥—É–≤–∞–Ω–Ω—è ---
    oc_log_message()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 1. –ü–æ—á–∏–Ω–∞—é –ø–æ—à—É–∫ –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤...")
    
    # --- 2. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å –∑ settings.json ---
    settings = load_oc_settings()
    if not settings:
        logging.info("‚ùå oc_settings.yaml –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ")
        return
    
    # --- 3. –û—Ç—Ä–∏–º–∞–Ω–Ω—è —à–ª—è—Ö—ñ–≤ –¥–æ –ø–æ—Ç—Ä—ñ–±–Ω–∏—Ö —Ñ–∞–π–ª—ñ–≤ ---
    zalishki_path = settings['paths']['output_file']                         # –ë–∞–∑–∞ —ñ—Å–Ω—É—é—á–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤
    supliers_new_path = settings['paths']['csv_path_new_product']         # –§–∞–π–ª, –∫—É–¥–∏ –±—É–¥–µ –∑–∞–ø–∏—Å–∞–Ω–æ –Ω–æ–≤—ñ —Ç–æ–≤–∞—Ä–∏
    supliers_csv_path = settings['suppliers'][1]['csv_path']                 # –ü—Ä–∞–π—Å-–ª–∏—Å—Ç –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ 1
    delimiter = settings['suppliers'][1]['delimiter']                        # –†–æ–∑–¥—ñ–ª—å–Ω–∏–∫ —É CSV
    
    # --- 4. –û—Ç—Ä–∏–º–∞–Ω–Ω—è –¥–æ–ø–æ–º—ñ–∂–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ ---
    sku_prefix = settings['suppliers'][1]['search']                          # –ü—Ä–µ—Ñ—ñ–∫—Å –¥–ª—è –ø–æ—à—É–∫—É
       
    logging.info("–ó—á–∏—Ç—É—é —ñ—Å–Ω—É—é—á—ñ –∞—Ä—Ç–∏–∫—É–ª–∏ –∑ —Ñ–∞–π–ª—É, –≤–∫–∞–∑–∞–Ω–æ–≥–æ –∑–∞ –∫–ª—é—á–µ–º 'csv_path_zalishki'.")

    try:

        # --- 3. –ó—á–∏—Ç—É–≤–∞–Ω–Ω—è –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤ –∑ —Ñ–∞–π–ª—É –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ ---
        fieldnames = []
        with open(supliers_new_path, mode='r', encoding='utf-8') as f:
            reader = csv.reader(f)
            try:
                fieldnames = next(reader) # –û—Ç—Ä–∏–º—É—î–º–æ —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤ –∫–æ–ª–æ–Ω–æ–∫: ['search', 'url_lutsk', ...]
            except StopIteration:
                logging.info("‚ùå –§–∞–π–ª –¥–ª—è –∑–∞–ø–∏—Å—É –ø–æ—Ä–æ–∂–Ω—ñ–π.")
                return

        logging.info(f"–°—Ç—Ä—É–∫—Ç—É—Ä—É —Ñ–∞–π–ª—É –∑—á–∏—Ç–∞–Ω–æ. –ö–æ–ª–æ–Ω–æ–∫: {len(fieldnames)}")

        # --- 4. –ó—á–∏—Ç—É–≤–∞–Ω–Ω—è —ñ—Å–Ω—É—é—á–∏—Ö –∞—Ä—Ç–∏–∫—É–ª—ñ–≤ —ñ–∑ –±–∞–∑–∏ (–û–ù–û–í–õ–ï–ù–û) ---
        logging.info("–ó—á–∏—Ç—É—é –±–∞–∑—É —ñ—Å–Ω—É—é—á–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤...")
        with open(zalishki_path, mode='r', encoding='utf-8') as zalishki_file:
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ DictReader, —â–æ–± –∑–≤–µ—Ä—Ç–∞—Ç–∏—Å—è –ø–æ –Ω–∞–∑–≤—ñ
            zalishki_reader = csv.DictReader(zalishki_file)

            existing_skus = {
                row.get("artykul_lutsk", "").strip().lower() 
                for row in zalishki_reader 
                if row.get("artykul_lutsk")
            }
            
            logging.info(f"–ó—á–∏—Ç–∞–Ω–æ {len(existing_skus)} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –∞—Ä—Ç–∏–∫—É–ª—ñ–≤ —ñ–∑ –±–∞–∑–∏.")

        # --- 5. –í—ñ–¥–∫—Ä–∏–≤–∞—î–º–æ —Ñ–∞–π–ª –¥–ª—è –∑–∞–ø–∏—Å—É (DictWriter) ---
        logging.info("–í—ñ–¥–∫—Ä–∏–≤–∞—é —Ñ–∞–π–ª –¥–ª—è –∑–∞–ø–∏—Å—É –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤...")
        with open(supliers_new_path, mode='w', encoding='utf-8', newline='') as new_file:
            # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ Writer, –ø–µ—Ä–µ–¥–∞—é—á–∏ –π–æ–º—É —Å–ø–∏—Å–æ–∫ –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤
            writer = csv.DictWriter(new_file, fieldnames=fieldnames)
            writer.writeheader() # –ó–∞–ø–∏—Å—É—î–º–æ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–∞–∑–∞–¥ —É —Ñ–∞–π–ª
            
            # --- 6. –ß–∏—Ç–∞—î–º–æ –ø—Ä–∞–π—Å –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ (DictReader) ---
            with open(supliers_csv_path, mode='r', encoding='utf-8') as supliers_file:
                # DictReader –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤—ñ–∑—å–º–µ –ø–µ—Ä—à–∏–π —Ä—è–¥–æ–∫ –ø—Ä–∞–π—Å—É —è–∫ –∫–ª—é—á—ñ —Å–ª–æ–≤–Ω–∏–∫–∞
                supliers_reader = csv.DictReader(supliers_file, delimiter=delimiter)
                
                new_products_count = 0

                for row in supliers_reader:
                    # –û—Ç—Ä–∏–º—É—î–º–æ –∞—Ä—Ç–∏–∫—É–ª –ø–æ –Ω–∞–∑–≤—ñ –∫–æ–ª–æ–Ω–∫–∏
                    sku = row.get("–ö–æ–¥_—Ç–æ–≤–∞—Ä–∞", "").strip().lower()
                    
                    if sku and sku not in existing_skus:
                        
                        # –°—Ç–≤–æ—Ä—é—î–º–æ –ø—É—Å—Ç–∏–π —Å–ª–æ–≤–Ω–∏–∫ –¥–ª—è –Ω–æ–≤–æ–≥–æ —Ä—è–¥–∫–∞, –∑–∞–ø–æ–≤–Ω—é—î–º–æ –ø–æ—Ä–æ–∂–Ω—ñ–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏
                        new_row = {key: '' for key in fieldnames}
                        
                        # --- 7. –ü–†–Ø–ú–ï –ú–ê–ü–£–í–ê–ù–ù–Ø (–ù–∞–∑–≤–∞ -> –ù–∞–∑–≤–∞) ---
                        
                        # –°–ø–µ—Ü—ñ–∞–ª—å–Ω–µ –ø–æ–ª–µ search (–ø—Ä–µ—Ñ—ñ–∫—Å + –∫–æ–¥)
                        new_row["search"] = sku_prefix + row.get("–ö–æ–¥_—Ç–æ–≤–∞—Ä–∞", "")
                        
                        # –û—Å–Ω–æ–≤–Ω—ñ –ø–æ–ª—è (–±–µ—Ä—É—Ç—å—Å—è –ø—Ä—è–º–æ –∑ row –ø–æ –∫–ª—é—á—É)
                        new_row["shtrih_cod"] = row.get("–®—Ç—Ä–∏—Ö_–∫–æ–¥", "")
                        new_row["–ö–æ–¥_—Ç–æ–≤–∞—Ä–∞"] = row.get("–ö–æ–¥_—Ç–æ–≤–∞—Ä–∞", "")
                        new_row["–ù–∞–∑–≤–∞–Ω–∏–µ_–ø–æ–∑–∏—Ü–∏–∏"] = row.get("–ù–∞–∑–≤–∞–Ω–∏–µ_–ø–æ–∑–∏—Ü–∏–∏", "")
                        new_row["–û–ø–∏—Å–∞–Ω–∏–µ"] = row.get("–û–ø–∏—Å–∞–Ω–∏–µ", "")
                        new_row["–¶–µ–Ω–∞"] = row.get("–¶–µ–Ω–∞", "")
                        new_row["–ù–∞–ª–∏—á–∏–µ"] = row.get("–ù–∞–ª–∏—á–∏–µ", "")
                        new_row["–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å"] = row.get("–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å", "")
                        new_row["–°—Ç—Ä–∞–Ω–∞_–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å"] = row.get("–°—Ç—Ä–∞–Ω–∞_–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å", "")
                        new_row["–ö–∞—Ç–µ–≥–æ—Ä–∏—è"] = row.get("–ö–∞—Ç–µ–≥–æ—Ä–∏—è", "")
                        new_row["–î–æ–ø. –ö–∞—Ç–µ–≥–æ—Ä–∏—è 1"] = row.get("–î–æ–ø. –ö–∞—Ç–µ–≥–æ—Ä–∏—è 1", "")
                        new_row["–î–æ–ø. –ö–∞—Ç–µ–≥–æ—Ä–∏—è 2"] = row.get("–î–æ–ø. –ö–∞—Ç–µ–≥–æ—Ä–∏—è 2", "")

                        # --- 8. –ó–∞–ø–∏—Å —Ä—è–¥–∫–∞ ---
                        writer.writerow(new_row)
                        new_products_count += 1

        # --- 17. –ü—ñ–¥—Å—É–º–∫–æ–≤–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è ---
        logging.info(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {new_products_count} –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤.")
        logging.info(f"–î–∞–Ω—ñ –∑–∞–ø–∏—Å–∞–Ω–æ —É —Ñ–∞–π–ª csv 'supliers_new_path'.")

    # --- 18. –û–±—Ä–æ–±–∫–∞ –ø–æ–º–∏–ª–æ–∫ ---
    except FileNotFoundError as e:
        logging.info(f"‚ùå –ü–æ–º–∏–ª–∫–∞: –§–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ - {e}")
    except Exception as e:
        logging.info(f"‚ùå –í–∏–Ω–∏–∫–ª–∞ –Ω–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")

# –û–°–ù–û–í–ù–ê –§–£–ù–ö–¶–Ü–Ø 3: –ó–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è —É—Ä–ª —Ç–æ–≤–∞—Ä—É
def find_product_url():
    """
    –ó—á–∏—Ç—É—î —Ñ–∞–π–ª –∑ –Ω–æ–≤–∏–º–∏ —Ç–æ–≤–∞—Ä–∞–º–∏, –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç—å –∑–∞ URL-–∞–¥—Ä–µ—Å–æ—é,
    –∑–Ω–∞—Ö–æ–¥–∏—Ç—å URL-–∞–¥—Ä–µ—Å—É –ø—Ä–æ—Å—Ç–æ–≥–æ –∞–±–æ –≤–∞—Ä—ñ–∞—Ç–∏–≤–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä—É,
    —ñ –∑–∞–ø–∏—Å—É—î –∑–Ω–∞–π–¥–µ–Ω—É URL-–∞–¥—Ä–µ—Å—É –≤ –∫–æ–ª–æ–Ω–∫—É B(1) –≤ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª.
    """

    # --- 1. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ª–æ–≥—É–≤–∞–Ω–Ω—è (–ø—ñ–¥–∫–ª—é—á–∞—î–º–æ —ñ—Å–Ω—É—é—á–∏–π –ª–æ–≥-—Ñ–∞–π–ª) ---
    oc_log_message()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 2. –ü–æ—á–∏–Ω–∞—é –ø–æ—à—É–∫ URL-–∞–¥—Ä–µ—Å —Ç–æ–≤–∞—Ä—ñ–≤...")
    
    # --- 2. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å —Ç–∞ —Ñ–æ—Ä–º—É–≤–∞–Ω–Ω—è —à–ª—è—Ö—ñ–≤/—Ç–∏–º—á–∞—Å–æ–≤–æ–≥–æ —Ñ–∞–π–ª—É ---
    settings = load_oc_settings()
    if not settings:
        logging.error("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è. –û–±—Ä–æ–±–∫–∞ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç–∞ –ø–µ—Ä–µ—Ä–≤–∞–Ω–∞.")
        return
    supliers_new_path = settings['paths']['csv_path_new_product']     # –≤—Ö—ñ–¥–Ω–∏–π CSV (1.csv)
    site_url = settings['suppliers'][1]['site']                       # –±–∞–∑–æ–≤–∏–π URL —Å–∞–π—Ç—É (—â–æ–± –¥–æ–¥–∞–≤–∞—Ç–∏ –≤—ñ–¥–Ω–æ—Å–Ω—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è)
    temp_file_path = supliers_new_path + '.temp'                      # —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª –ø—ñ–¥ —á–∞—Å –∑–∞–ø–∏—Å—É

    # --- –õ—ñ—á–∏–ª—å–Ω–∏–∫–∏ —ñ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ---
    total_rows = 0
    found_variant_count = 0
    found_simple_count = 0
    not_found_count = 0
    found_variant_rows = []
    not_found_rows = []

    try:
        # --- 3. –í—ñ–¥–∫—Ä–∏–≤–∞—î–º–æ –≤—Ö—ñ–¥–Ω–∏–π —Ñ–∞–π–ª –¥–ª—è —á–∏—Ç–∞–Ω–Ω—è ---
        with open(supliers_new_path, mode='r', encoding='utf-8') as input_file:
            reader = csv.DictReader(input_file)
            # –í–ê–ñ–õ–ò–í–û: –û—Ç—Ä–∏–º—É—î–º–æ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –æ–¥—Ä–∞–∑—É
            fieldnames = reader.fieldnames
            
            # --- –ü–ï–†–ï–í–Ü–†–ö–ê –ù–ê –ü–û–ú–ò–õ–ö–£: –Ø–∫—â–æ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –ø—É—Å—Ç—ñ ---
            if not fieldnames:
                logging.error("‚ùå –ü–æ–º–∏–ª–∫–∞: –ù–µ –≤–¥–∞–ª–æ—Å—è –ø—Ä–æ—á–∏—Ç–∞—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å—Ç–æ–≤–ø—Ü—ñ–≤. –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ, —á–∏ —Ñ–∞–π–ª –Ω–µ –ø—É—Å—Ç–∏–π —ñ —á–∏ –∫–æ—Ä–µ–∫—Ç–Ω–µ –∫–æ–¥—É–≤–∞–Ω–Ω—è.")
                return
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ –∫—Ä–∏—Ç–∏—á–Ω–∏—Ö –∫–æ–ª–æ–Ω–æ–∫
            required_columns = ['search', 'sku', 'url_lutsk']
            for col in required_columns:
                if col not in fieldnames:
                    logging.error(f"‚ùå –£ —Ñ–∞–π–ª—ñ –≤—ñ–¥—Å—É—Ç–Ω—è –æ–±–æ–≤'—è–∑–∫–æ–≤–∞ –∫–æ–ª–æ–Ω–∫–∞: {col}")
                    return

            # --- 4. –í—ñ–¥–∫—Ä–∏–≤–∞—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª –¥–ª—è –∑–∞–ø–∏—Å—É (DictWriter) ---
            with open(temp_file_path, mode='w', encoding='utf-8', newline='') as output_file:
                writer = csv.DictWriter(output_file, fieldnames=fieldnames)
                writer.writeheader() # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –∑–∞–ø–∏—Å—É—î —Ä—è–¥–æ–∫ –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤
                
                # --- 5. –Ü—Ç–µ—Ä–∞—Ü—ñ—è –ø–æ —Ä—è–¥–∫–∞—Ö ---
                for idx, row in enumerate(reader):
                    total_rows += 1
                    
                    # 5.1. –í–∏—Ç—è–≥—É—î–º–æ –¥–∞–Ω—ñ –ø–æ –Ω–∞–∑–≤–∞—Ö –∫–æ–ª–æ–Ω–æ–∫
                    search_url = row.get('search', '').strip()
                    
                    file_sku = row.get('–ö–æ–¥_—Ç–æ–≤–∞—Ä–∞', '').strip()

                    # --- 6. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –≤–∞–ª—ñ–¥–Ω–æ—Å—Ç—ñ URL ---
                    if not search_url or search_url.startswith('–ü–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Ç—É'):
                        writer.writerow(row)
                        continue

                    try:
                        # --- 7. –í–∏–∫–æ–Ω–∞–Ω–Ω—è HTTP-–∑–∞–ø–∏—Ç—É –¥–æ search_url —ñ –ø–∞—Ä—Å–∏–Ω–≥ HTML ---
                        response = requests.get(search_url)
                        response.raise_for_status()
                        soup = BeautifulSoup(response.text, 'html.parser')
                        found_type = None  # 'variant' –∞–±–æ 'simple'
                        found_url = None  # —Å—é–¥–∏ –∑–∞–ø–∏—à–µ–º–æ –∑–Ω–∞–π–¥–µ–Ω—É —Ä–µ–∞–ª—å–Ω—É URL-–∞–¥—Ä–µ—Å—É —Ç–æ–≤–∞—Ä—É
                        
                        # --- 8. –ü–æ—à—É–∫ –≤–∞—Ä—ñ–∞—Ç–∏–≤–Ω–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ (input.variant_control[data-code]) ---
                        # –®—É–∫–∞—î–º–æ input —Ç–µ–≥–∏ –∑ –∫–ª–∞—Å–æ–º variant_control —Ç–∞ –∞—Ç—Ä–∏–±—É—Ç–æ–º data-code,
                        # –ø–æ—Ä—ñ–≤–Ω—é—î–º–æ data-code –∑ file_sku ‚Äî —è–∫—â–æ —Å–ø—ñ–≤–ø–∞–¥—ñ–Ω–Ω—è, –±–µ—Ä–µ–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è —É –±–∞—Ç—å–∫—ñ–≤—Å—å–∫–æ–º—É –±–ª–æ—Ü—ñ.
                        variant_inputs = soup.find_all('input', class_='variant_control', attrs={'data-code': True})
                        for input_tag in variant_inputs:
                            site_sku = input_tag.get('data-code', '').strip()
                            if file_sku == site_sku:
                                parent_div = input_tag.find_parent('div', class_='card-block')
                                if parent_div:
                                    link_tag = parent_div.find('h4', class_='card-title').find('a')
                                    if link_tag and link_tag.has_attr('href'):
                                        # –§–æ—Ä–º—É—î–º–æ –ø–æ–≤–Ω–∏–π URL (–¥–æ–¥–∞—î–º–æ site_url –¥–æ –≤—ñ–¥–Ω–æ—Å–Ω–æ–≥–æ —à–ª—è—Ö—É)
                                        found_url = site_url + link_tag['href']
                                        found_type = 'variant'
                                        break

                        # --- 9. –Ø–∫—â–æ –Ω–µ –∑–Ω–∞–π—à–ª–∏ —Å–µ—Ä–µ–¥ –≤–∞—Ä—ñ–∞–Ω—Ç—ñ–≤ ‚Äî —à—É–∫–∞—î–º–æ –ø—Ä–æ—Å—Ç—ñ —Ç–æ–≤–∞—Ä–∏ ---
                        if not found_url:
                            # –î–ª—è –ø—Ä–æ—Å—Ç–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ —à—É–∫–∞—î–º–æ div –∑ –∫–ª–∞—Å–æ–º 'radio', –±–µ—Ä–µ–º–æ —Ç–µ–∫—Å—Ç —è–∫ SKU,
                            # —ñ –∑–∞ —Ç–∞–∫–∏–º –∂–µ –ø—ñ–¥—Ö–æ–¥–æ–º –∑–Ω–∞—Ö–æ–¥–∏–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è —É –±–ª–æ—Ü—ñ card-block.
                            simple_divs = soup.find_all('div', class_='radio')
                            for div_tag in simple_divs:
                                site_sku = div_tag.get_text(strip=True).strip()
                                if file_sku == site_sku:
                                    parent_div = div_tag.find_parent('div', class_='card-block')
                                    if parent_div:
                                        link_tag = parent_div.find('h4', class_='card-title').find('a')
                                        if link_tag and link_tag.has_attr('href'):
                                            found_url = site_url + link_tag['href']
                                            found_type = 'simple'
                                            break

                        # --- 10. –ó–∞–ø–∏—Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É –≤ –∫–æ–ª–æ–Ω–∫—É 'url_lutsk' –∞–±–æ –ª–æ–≥—É–≤–∞–Ω–Ω—è —è–∫—â–æ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ ---
                        if found_url:
                            row['url_lutsk'] = found_url
                            if found_type == 'variant':
                                found_variant_count += 1
                                found_variant_rows.append(idx + 2)  # +2, –±–æ —Ä—è–¥–∫–∏ CSV —Ä–∞—Ö—É—é—Ç—å—Å—è –∑ 1 + –∑–∞–≥–æ–ª–æ–≤–æ–∫
                            elif found_type == 'simple':
                                found_simple_count += 1
                        else:
                            not_found_count += 1
                            not_found_rows.append(idx + 2)

                        # –ó–∞–ø–∏—Å—É—î–º–æ (–∑–Ω–∞–π–¥–µ–Ω–∏–π –∞–±–æ –Ω–µ–∑–º—ñ–Ω–µ–Ω–∏–π) —Ä—è–¥–æ–∫ —É —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª
                        writer.writerow(row)

                    except requests.RequestException as e:
                        # --- 11. –û–±—Ä–æ–±–∫–∞ –ø–æ–º–∏–ª–æ–∫ –∑–∞–ø–∏—Ç—É ---
                        logging.error(f"–†—è–¥–æ–∫ {idx + 2}: –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Ç—ñ: {e}")
                        # –ó–∞–ø–∏—Å—É—î–º–æ –ø–æ–º–∏–ª–∫—É –≤ –∫–æ–ª–æ–Ω–∫—É 'search', —è–∫ –±—É–ª–æ —Ä–∞–Ω—ñ—à–µ
                        row['search'] = f'–ü–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Ç—É: {e}'
                        writer.writerow(row)
                    
                    # --- 12. –î–æ–¥–∞—Ç–∫–æ–≤–∞ –ø–∞—É–∑–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏ (—Ä–∞–Ω–¥–æ–º—ñ–∑–æ–≤–∞–Ω–∞) –¥–ª—è —É–Ω–∏–∫–Ω–µ–Ω–Ω—è –±–∞–Ω–∞/DDOS ---
                    time.sleep(random.uniform(1, 3))
  
        # --- 13. –ü—ñ—Å–ª—è —É—Å–ø—ñ—à–Ω–æ—ó –æ–±—Ä–æ–±–∫–∏: –∑–∞–º—ñ–Ω–∞ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª—É —Ç–∏–º—á–∞—Å–æ–≤–∏–º ---
        os.replace(temp_file_path, supliers_new_path)

        # --- 14. –ó–≤–µ–¥–µ–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ---
        logging.info("=== –ü–Ü–î–°–£–ú–ö–û–í–ê –Ü–ù–§–û–†–ú–ê–¶–Ü–Ø ===")
        logging.info(f"–í—Å—å–æ–≥–æ —Ä—è–¥–∫—ñ–≤ –∑ —Ç–æ–≤–∞—Ä–∞–º–∏: {total_rows}")
        logging.info(
            f"–ó–Ω–∞–π–¥–µ–Ω–æ URL –≤–∞—Ä—ñ–∞—Ç–∏–≤–Ω–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤: {found_variant_count}"
            + (f" (–†—è–¥–∫–∏ {', '.join(map(str, found_variant_rows))})" if found_variant_rows else "")
        )
        logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ URL –ø—Ä–æ—Å—Ç–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤: {found_simple_count}")
        logging.info(
            f"–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ URL: {not_found_count}"
            + (f" (–†—è–¥–∫–∏ {', '.join(map(str, not_found_rows))})" if not_found_rows else "")
        )

    except FileNotFoundError as e:
        # --- 15. –û–±—Ä–æ–±–∫–∞ –ø–æ–º–∏–ª–∫–∏: –≤—Ö—ñ–¥–Ω–∏–π —Ñ–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ ---
        logging.error(f"–ü–æ–º–∏–ª–∫–∞: –§–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ - {e}")
    except Exception as e:
        # --- 16. –ì–∞—Ä–∞–Ω—Ç—ñ–π–Ω–µ –ø—Ä–∏–±–∏—Ä–∞–Ω–Ω—è: –≤–∏–¥–∞–ª—è—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª –ø—Ä–∏ –ø–æ–º–∏–ª—Ü—ñ, —â–æ–± –Ω–µ –∑–∞–ª–∏—à–∏—Ç–∏ —Å–º—ñ—Ç—Ç—è ---
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)
        logging.error(f"–í–∏–Ω–∏–∫–ª–∞ –Ω–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")

# –û–°–ù–û–í–ù–ê –§–£–ù–ö–¶–Ü–Ø 4: –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ç—Ä–∏–±—É—Ç—ñ–≤
def parse_product_attributes():
    oc_log_message()
    logging.info("‚ñ∂ –§–£–ù–ö–¶–Ü–Ø: –ü–∞—Ä—Å–∏–Ω–≥ (Mapping + Auto-suffix |ua + Next Col RU)")

    settings = load_oc_settings()
    if not settings: return
    
    supliers_new_path = settings["paths"]["csv_path_new_product"]

    # =========================================================================
    # üîß –°–õ–û–í–ù–ò–ö –ü–ï–†–ï–ô–ú–ï–ù–£–í–ê–ù–ù–Ø –ê–¢–†–ò–ë–£–¢–Ü–í
    # –¢—É—Ç –º–∏ –≤–∫–∞–∑—É—î–º–æ: "–Ø–∫ –Ω–∞–∑–≤–∞–Ω–æ –Ω–∞ —Å–∞–π—Ç—ñ" : "–Ø–∫ –Ω–∞–∑–∏–≤–∞—î—Ç—å—Å—è –∫–æ–ª–æ–Ω–∫–∞ —É —Ñ–∞–π–ª—ñ"
    # =========================================================================
    ATTRIBUTE_NAME_MAPPING = {
        "–ö—Ä–∞—ó–Ω–∞": "–ó—Ä–æ–±–ª–µ–Ω–æ –≤|ua",
        "–ú–∞—Ä–∫–∞/–õ—ñ–Ω—ñ—è": "–í–∏—Ä–æ–±–Ω–∏–∫|ua",
        # –î–æ–¥–∞–≤–∞–π—Ç–µ —Å—é–¥–∏ –Ω–æ–≤—ñ, —è–∫—â–æ –±—É–¥—É—Ç—å —Ä–æ–∑–±—ñ–∂–Ω–æ—Å—Ç—ñ
        # "–ú–∞—Ç–µ—Ä—ñ–∞–ª": "–°–∫–ª–∞–¥ —Ç–∫–∞–Ω–∏–Ω–∏|ua", 
    }

    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–∞–ø–∏ –∑–Ω–∞—á–µ–Ω—å (attribute.csv)
    replacements_map, raw_data = load_attributes_csv()
    changes_made = False
    max_raw_row_len = len(raw_data[0]) if raw_data and raw_data[0] else 12

    # –¢–æ—á–∫–∏ –≤—Å—Ç–∞–≤–∫–∏
    insertion_points = {} 
    current_block_name = None
    for i, row_raw in enumerate(raw_data[1:], start=1):
        first_col = row_raw[0].strip()
        if first_col: 
            current_block_name = first_col
            insertion_points[current_block_name] = i + 1
        elif current_block_name:
            insertion_points[current_block_name] = i + 1

    new_attributes_counter = {} 
    temp_file_path = supliers_new_path + ".temp"

    try:
        with open(supliers_new_path, mode="r", encoding="utf-8") as input_file, \
             open(temp_file_path, mode="w", encoding="utf-8", newline="") as output_file:

            reader = csv.DictReader(input_file)
            fieldnames = reader.fieldnames 
            
            if not fieldnames:
                logging.error("‚ùå –§–∞–π–ª –ø–æ—Ä–æ–∂–Ω—ñ–π!")
                return
            
            logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ –∫–æ–ª–æ–Ω–æ–∫ —É —Ñ–∞–π–ª—ñ: {len(fieldnames)}")

            writer = csv.DictWriter(output_file, fieldnames=fieldnames)
            writer.writeheader()

            processed_count = 0

            for row in reader:
                processed_count += 1
                row_values = list(row.values())
                product_url = row_values[1].strip() if len(row_values) > 1 else ""

                if not product_url or product_url.startswith("–ü–æ–º–∏–ª–∫–∞"):
                    writer.writerow(row)
                    continue

                try:
                    response = requests.get(product_url, timeout=10)
                    response.raise_for_status()
                    soup = BeautifulSoup(response.text, "html.parser")

                    parsed_attributes = {}
                    characteristics_div = soup.find("div", id="w0-tab0")
                    if characteristics_div and characteristics_div.find("table"):
                        for tr in characteristics_div.find("table").find_all("tr"):
                            cells = tr.find_all("td")
                            if len(cells) == 2:
                                key = cells[0].get_text(strip=True).replace(":", "")
                                value = cells[1].get_text(strip=True)
                                parsed_attributes[key] = value

                    other_attributes_list = []

                    # --- –û–±—Ä–æ–±–∫–∞ –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ ---
                    for attr_name_site, attr_value in parsed_attributes.items():
                        if attr_name_site == "–®—Ç—Ä–∏—Ö-–∫–æ–¥": continue

                        # === –õ–û–ì–Ü–ö–ê –í–ò–ó–ù–ê–ß–ï–ù–ù–Ø –ö–û–õ–û–ù–ö–ò ===
                        final_col_name = None
                        
                        # 1. –ü—Ä—ñ–æ—Ä–∏—Ç–µ—Ç: –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ä—É—á–Ω–∏–π –º–∞–ø–∏–Ω–≥ (–°–ª–æ–≤–Ω–∏–∫ –∑–≤–µ—Ä—Ö—É)
                        if attr_name_site in ATTRIBUTE_NAME_MAPPING:
                            mapped_name = ATTRIBUTE_NAME_MAPPING[attr_name_site]
                            if mapped_name in row:
                                final_col_name = mapped_name
                        
                        # 2. –ü—Ä—ñ–æ—Ä–∏—Ç–µ—Ç: –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π —Å—É—Ñ—ñ–∫—Å |ua
                        if not final_col_name:
                            target_col_ua = f"{attr_name_site}|ua"
                            if target_col_ua in row:
                                final_col_name = target_col_ua
                            elif attr_name_site in row: # –ü—Ä—è–º–∏–π –∑–±—ñ–≥ (–±–µ–∑ —Å—É—Ñ—ñ–∫—Å—ñ–≤)
                                final_col_name = attr_name_site
                        
                        # === –î–Ü–á –ó –ó–ù–ê–ô–î–ï–ù–û–Æ –ö–û–õ–û–ù–ö–û–Æ ===
                        if final_col_name:
                            original_value_lower = attr_value.strip().lower()
                            rules_for_this_attr = replacements_map.get(final_col_name, {})
                            
                            found_data = rules_for_this_attr.get(original_value_lower)

                            if found_data:
                                # –†–æ–∑–ø–∞–∫–æ–≤–∫–∞ –∑–Ω–∞—á–µ–Ω—å (Tuple check)
                                if isinstance(found_data, tuple) and len(found_data) >= 2:
                                    ua_new, ru_new = found_data
                                else:
                                    ua_new = str(found_data)
                                    ru_new = "" 

                                # –ü–∏—à–µ–º–æ UA
                                row[final_col_name] = ua_new
                                
                                # –ü–∏—à–µ–º–æ RU (–Ω–∞—Å—Ç—É–ø–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞)
                                if ru_new:
                                    try:
                                        current_idx = fieldnames.index(final_col_name)
                                        if current_idx + 1 < len(fieldnames):
                                            next_col_name = fieldnames[current_idx + 1]
                                            row[next_col_name] = ru_new
                                    except ValueError: pass 
                            else:
                                # –ù–æ–≤–µ –∑–Ω–∞—á–µ–Ω–Ω—è -> –ø–∏—à–µ–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª
                                row[final_col_name] = attr_value
                                
                                # –î–æ–¥–∞—î–º–æ –≤ attribute.csv
                                if original_value_lower not in rules_for_this_attr:
                                    insert_index = insertion_points.get(final_col_name)
                                    if insert_index is not None:
                                        new_raw_row = [""] * max_raw_row_len
                                        new_raw_row[2] = original_value_lower
                                        raw_data.insert(insert_index, new_raw_row)
                                        
                                        # –ö–µ—à—É—î–º–æ —è–∫ –∫–æ—Ä—Ç–µ–∂
                                        replacements_map.setdefault(final_col_name, {})[original_value_lower] = (attr_value, "") 
                                        
                                        changes_made = True
                                        new_attributes_counter[final_col_name] = new_attributes_counter.get(final_col_name, 0) + 1
                                        
                                        for k, v in insertion_points.items():
                                            if v >= insert_index: insertion_points[k] += 1
                        else:
                            # –ö–æ–ª–æ–Ω–∫–∏ –Ω–µ–º–∞—î –Ω—ñ –≤ –º–∞–ø–∏–Ω–≥—É, –Ω—ñ –Ω–∞–ø—Ä—è–º—É -> "–ù–æ–≤—ñ –∞—Ç—Ä–∏–±—É—Ç–∏"
                            other_attributes_list.append(f"{attr_name_site}:{attr_value}")

                    # --- –ó–∞–ø–∏—Å "–ù–æ–≤—ñ –∞—Ç—Ä–∏–±—É—Ç–∏" ---
                    if other_attributes_list:
                        new_content = ", ".join(other_attributes_list)
                        if "–ù–æ–≤—ñ –∞—Ç—Ä–∏–±—É—Ç–∏" in row:
                            current = row.get("–ù–æ–≤—ñ –∞—Ç—Ä–∏–±—É—Ç–∏", "")
                            row["–ù–æ–≤—ñ –∞—Ç—Ä–∏–±—É—Ç–∏"] = (current + ", " + new_content) if current else new_content

                    writer.writerow(row)

                except Exception as e:
                    logging.error(f"–ü–æ–º–∏–ª–∫–∞ URL {product_url}: {e}")
                    writer.writerow(row)

                time.sleep(random.uniform(1, 3))

        os.replace(temp_file_path, supliers_new_path)
        logging.info(f"–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –¢–æ–≤–∞—Ä—ñ–≤: {processed_count}")

        if changes_made:
            save_attributes_csv(raw_data)
            logging.info("===== –ó–í–Ü–¢ –ü–†–û –ù–û–í–Ü –ó–ù–ê–ß–ï–ù–ù–Ø =====")
            total_new = 0
            for attr_block, count in sorted(new_attributes_counter.items()):
                logging.info(f"‚Ä¢ {attr_block}: +{count}")
                total_new += count
            logging.info(f"–†–ê–ó–û–ú –¥–æ–¥–∞–Ω–æ: {total_new}")
            logging.info("==================================")
        else:
            logging.info("–ù–æ–≤–∏—Ö –∑–Ω–∞—á–µ–Ω—å –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ –Ω–µ –≤–∏—è–≤–ª–µ–Ω–æ.")
        
    except Exception as e:
        logging.error(f"–ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")
        if os.path.exists(temp_file_path): os.remove(temp_file_path)

# –û–°–ù–û–í–ù–ê –§–£–ù–ö–¶–Ü–Ø 5: –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü—ñ—è –∞—Ç—Ä–∏–±—É—Ç—ñ–≤
def apply_final_standardization():
    """
    –ü—Ä–æ—Ö–æ–¥–∏—Ç—å –ø–æ –≤–∂–µ —Å—Ñ–æ—Ä–º–æ–≤–∞–Ω–æ–º—É —Ñ–∞–π–ª—É —ñ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑—É—î –∑–Ω–∞—á–µ–Ω–Ω—è –∑–≥—ñ–¥–Ω–æ –∑ attribute.csv.
    1. –í–∏–ø—Ä–∞–≤–ª—è—î —Ä–µ–≥—ñ—Å—Ç—Ä (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥ "–≤–æ–¥–Ω–∞" -> "–í–æ–¥–Ω–∞").
    2. –û–Ω–æ–≤–ª—é—î —Ä–æ—Å—ñ–π—Å—å–∫–∏–π –ø–µ—Ä–µ–∫–ª–∞–¥ –≤ —Å—É—Å—ñ–¥–Ω—ñ–π –∫–æ–ª–æ–Ω—Ü—ñ.
    3. –ü—Ä–∞—Ü—é—î –Ω–∞ –æ—Å–Ω–æ–≤—ñ –Ω–∞–∑–≤ –∫–æ–ª–æ–Ω–æ–∫.
    """
    oc_log_message()
    logging.info("‚ñ∂ –§–£–ù–ö–¶–Ü–Ø 5. –§—ñ–Ω–∞–ª—å–Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü—ñ—è (UA + RU update)...")

    # --- 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å ---
    settings = load_oc_settings()
    if not settings: return

    csv_path = settings['paths']['csv_path_new_product']
    
    # --- 2. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø—Ä–∞–≤–∏–ª –∑–∞–º—ñ–Ω–∏ ---
    # –û—Ç—Ä–∏–º—É—î–º–æ —Å–ª–æ–≤–Ω–∏–∫: {'–û—Å–Ω–æ–≤–∞|ua': {'–≤–æ–¥–Ω–∞': ('–í–æ–¥–Ω–∞', '–í–æ–¥–Ω–∞—è')}}
    replacements_map, _ = load_attributes_csv()
    
    if not replacements_map:
        logging.warning("‚ö†Ô∏è attribute.csv –ø–æ—Ä–æ–∂–Ω—ñ–π –∞–±–æ –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏–≤—Å—è. –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü—ñ—è –ø—Ä–æ–ø—É—â–µ–Ω–∞.")
        return

    # --- 3. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ ---
    replacement_counter = {}  # {col_name: count}

    # --- 4. –û–±—Ä–æ–±–∫–∞ CSV ---
    temp_file_path = csv_path + '.final_temp'
    
    try:
        with open(csv_path, 'r', encoding='utf-8') as infile, \
             open(temp_file_path, 'w', encoding='utf-8', newline='') as outfile:

            reader = csv.DictReader(infile)
            fieldnames = reader.fieldnames
            
            if not fieldnames:
                logging.error("‚ùå –§–∞–π–ª –ø–æ—Ä–æ–∂–Ω—ñ–π!")
                return

            writer = csv.DictWriter(outfile, fieldnames=fieldnames)
            writer.writeheader()

            processed_rows = 0

            for row in reader:
                processed_rows += 1
                row_updated = False

                # –ü—Ä–æ—Ö–æ–¥–∏–º–æ –ø–æ –≤—Å—ñ—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö, –¥–ª—è —è–∫–∏—Ö —É –Ω–∞—Å —î –ø—Ä–∞–≤–∏–ª–∞ –≤ attribute.csv
                # col_name - —Ü–µ, –Ω–∞–ø—Ä–∏–∫–ª–∞–¥, "–û—Å–Ω–æ–≤–∞|ua" –∞–±–æ "–ö–æ–ª—ñ—Ä|ua"
                for col_name, rules in replacements_map.items():
                    
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —ñ—Å–Ω—É—î —Ç–∞–∫–∞ –∫–æ–ª–æ–Ω–∫–∞ —É —Ñ–∞–π–ª—ñ
                    if col_name in row:
                        current_value = row[col_name].strip()
                        
                        if not current_value:
                            continue

                        current_value_lower = current_value.lower()
                        
                        # –ß–∏ —î –ø—Ä–∞–≤–∏–ª–æ –¥–ª—è —Ü—å–æ–≥–æ –∑–Ω–∞—á–µ–Ω–Ω—è?
                        found_pair = rules.get(current_value_lower)

                        if found_pair:
                            # found_pair –º–∞—î –≤–∏–≥–ª—è–¥ ('–í–æ–¥–Ω–∞', '–í–æ–¥–Ω–∞—è')
                            # –ó–∞—Ö–∏—Å—Ç –≤—ñ–¥ —Å—Ç–∞—Ä–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç—É –¥–∞–Ω–∏—Ö
                            if isinstance(found_pair, tuple) and len(found_pair) >= 2:
                                ua_std, ru_std = found_pair
                            else:
                                ua_std = str(found_pair)
                                ru_std = ""

                            # 1. –û–Ω–æ–≤–ª—é—î–º–æ UA –∑–Ω–∞—á–µ–Ω–Ω—è (—è–∫—â–æ –≤—ñ–¥—Ä—ñ–∑–Ω—è—î—Ç—å—Å—è)
                            if row[col_name] != ua_std:
                                row[col_name] = ua_std
                                row_updated = True
                                # –õ–æ–≥—É—î–º–æ –∑–º—ñ–Ω—É
                                replacement_counter[col_name] = replacement_counter.get(col_name, 0) + 1

                            # 2. –û–Ω–æ–≤–ª—é—î–º–æ RU –∑–Ω–∞—á–µ–Ω–Ω—è (—Å—É—Å—ñ–¥–Ω—è –∫–æ–ª–æ–Ω–∫–∞)
                            if ru_std:
                                try:
                                    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ —ñ–Ω–¥–µ–∫—Å UA –∫–æ–ª–æ–Ω–∫–∏
                                    ua_idx = fieldnames.index(col_name)
                                    # –ë–µ—Ä–µ–º–æ –Ω–∞—Å—Ç—É–ø–Ω—É –∫–æ–ª–æ–Ω–∫—É (+1)
                                    if ua_idx + 1 < len(fieldnames):
                                        ru_col_name = fieldnames[ua_idx + 1]
                                        
                                        # –û–Ω–æ–≤–ª—é—î–º–æ RU, —è–∫—â–æ –≤–æ–Ω–æ –≤—ñ–¥—Ä—ñ–∑–Ω—è—î—Ç—å—Å—è –∞–±–æ –ø—É—Å—Ç–µ
                                        if row[ru_col_name] != ru_std:
                                            row[ru_col_name] = ru_std
                                            row_updated = True
                                except ValueError:
                                    pass # –Ø–∫—â–æ —â–æ—Å—å –ø—ñ—à–ª–æ –Ω–µ —Ç–∞–∫ –∑ —ñ–Ω–¥–µ–∫—Å–∞–º–∏

                writer.writerow(row)

        # --- 5. –ó–∞–º—ñ–Ω–∞ —Ñ–∞–π–ª—É ---
        os.replace(temp_file_path, csv_path)
        logging.info("–§—ñ–Ω–∞–ª—å–Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü—ñ—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –§–∞–π–ª –æ–Ω–æ–≤–ª–µ–Ω–æ.")

        # --- 6. –ü—ñ–¥—Å—É–º–∫–æ–≤–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è ---
        if replacement_counter:
            logging.info("===== –ó–í–Ü–¢ –ü–†–û –°–¢–ê–ù–î–ê–†–¢–ò–ó–ê–¶–Ü–Æ =====")
            for col, count in sorted(replacement_counter.items()):
                logging.info(f"‚Ä¢ {col}: —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–æ–≤–∞–Ω–æ {count} —Ä—è–¥–∫—ñ–≤")
            logging.info(f"–†–ê–ó–û–ú –∑–º—ñ–Ω: {sum(replacement_counter.values())}")
            logging.info("===================================")
        else:
            logging.info("–£—Å—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –≤–∂–µ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—é—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º.")

    except Exception as e:
        logging.error(f"–ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü—ñ—ó: {e}")
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)

# –û–°–ù–û–í–ù–ê –§–£–ù–ö–¶–Ü–Ø 6: –ó–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –¥–æ–ø–æ–º—ñ–∂–Ω–∏—Ö –∫–æ–ª–æ–Ω–æ–∫
def fill_auxiliary_columns():
    """
    –ê–¥–∞–ø—Ç–æ–≤–∞–Ω–æ –ø—ñ–¥ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ë–î OpenCart (oc_product, oc_product_description).
    - –¶—ñ–Ω–∞ –±–µ–∑ .00
    - –î–∞—Ç–∞ –∑—Ä–æ—Å—Ç–∞—î –Ω–∞ 1 —Ö–≤ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä—É
    """
    oc_log_message()
    logging.info("–§–£–ù–ö–¶–Ü–Ø: –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è OpenCart...")

    settings = load_oc_settings()
    try:
        csv_path = settings['paths']['csv_path_new_product']
        name_ukr = settings['suppliers'][1]['name_ukr'] 
    except (TypeError, KeyError) as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å: {e}")
        return

    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–∞–ø –∫–∞—Ç–µ–≥–æ—Ä—ñ–π
    category_map, cat_fieldnames = load_category_csv()
    poznachky_list = load_poznachky_csv()
    
    new_category_entries = []
    seen_new_keys = set()
    
    # --- –õ–û–ì–Ü–ö–ê –î–ê–¢–ò (–°—Ç–∞—Ä—Ç –æ 00:00:00 —Å—å–æ–≥–æ–¥–Ω—ñ) ---
    # –ë–µ—Ä–µ–º–æ –ø–æ—Ç–æ—á–Ω—É –¥–∞—Ç—É, –∞–ª–µ —á–∞—Å —Å–∫–∏–¥–∞—î–º–æ –Ω–∞ 00:00:00
    base_date = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
    
    temp_path = csv_path + '.oc_temp'

    try:
        with open(csv_path, 'r', encoding='utf-8') as infile, \
             open(temp_path, 'w', encoding='utf-8', newline='') as outfile:

            reader = csv.DictReader(infile)
            fieldnames = list(reader.fieldnames)

            # –î–æ–¥–∞—î–º–æ –Ω–æ–≤—ñ OC –∫–æ–ª–æ–Ω–∫–∏
            oc_columns = [
                'category', '–ö–∞—Ç–µ–≥–æ—Ä—ñ—è|ua', '–ö–∞—Ç–µ–≥–æ—Ä–∏—è|ru', 
                '–ü–æ–∑–Ω–∞—á–∫–∏', 
                'stock_status_id','price', 'status', 'subtract', 
                'minimum', 'shipping', 'date_added', 
                'store_id', 'layout_id'
            ]
            
            for col in oc_columns:
                if col not in fieldnames:
                    fieldnames.append(col)

            writer = csv.DictWriter(outfile, fieldnames=fieldnames)
            writer.writeheader()

            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ enumerate, —â–æ–± –º–∞—Ç–∏ —ñ–Ω–¥–µ–∫—Å —Ä—è–¥–∫–∞ (i) –¥–ª—è –∑–±—ñ–ª—å—à–µ–Ω–Ω—è —á–∞—Å—É
            for i, row in enumerate(reader):
                
                # --- –õ–û–ì–Ü–ö–ê –ö–ê–¢–ï–ì–û–†–Ü–ô ---
                c1 = row.get("–ö–∞—Ç–µ–≥–æ—Ä–∏—è", "").strip()
                c2 = row.get("–î–æ–ø. –ö–∞—Ç–µ–≥–æ—Ä–∏—è 1", "").strip()
                c3 = row.get("–î–æ–ø. –ö–∞—Ç–µ–≥–æ—Ä–∏—è 2", "").strip()
                lookup_key = tuple(sorted([c1.lower(), c2.lower(), c3.lower()]))

                if lookup_key in category_map:
                    data = category_map[lookup_key]
                    row['category']     = data['category']
                    row['–ö–∞—Ç–µ–≥–æ—Ä—ñ—è|ua'] = data['cat_ua']
                    row['–ö–∞—Ç–µ–≥–æ—Ä–∏—è|ru'] = data['cat_ru']
                else:
                    if any(lookup_key) and lookup_key not in seen_new_keys:
                        new_category_entries.append({
                            'name_1': c1, 'name_2': c2, 'name_3': c3,
                            'category_name': f"{c1} {c2} {c3}".strip(),
                            'category': '', '–ö–∞—Ç–µ–≥–æ—Ä—ñ—è|ua': '', '–ö–∞—Ç–µ–≥–æ—Ä–∏—è|ru': ''
                        })
                        seen_new_keys.add(lookup_key)
                    row['category'] = ''
                    row['–ö–∞—Ç–µ–≥–æ—Ä—ñ—è|ua'] = ''
                    row['–ö–∞—Ç–µ–≥–æ—Ä–∏—è|ru'] = ''

                # --- –õ–û–ì–Ü–ö–ê –ü–û–ó–ù–ê–ß–û–ö ---
                prod_name = row.get("–ù–∞–∑–≤–∞–Ω–∏–µ_–ø–æ–∑–∏—Ü–∏–∏", "")
                if prod_name and poznachky_list:
                    found = [tag.capitalize() for tag in poznachky_list if tag in prod_name.lower()]
                    row["–ü–æ–∑–Ω–∞—á–∫–∏"] = ', '.join(sorted(list(set(found)))) if found else ""

                # --- 3. –ó–ê–ü–û–í–ù–ï–ù–ù–Ø OPENCART –î–ê–ù–ò–• ---
                
                row["status"] = "0" 
                row["stock_status_id"] = "7" 
                row["subtract"] = "1"
                row["minimum"] = "1"
                row["shipping"] = "1"

                # === –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø –¶–Ü–ù–ò ===
                try:
                    # 1. –ó–∞–º—ñ–Ω–∞ –∫–æ–º–∏ –Ω–∞ –∫—Ä–∞–ø–∫—É, –≤–∏–¥–∞–ª–µ–Ω–Ω—è –ø—Ä–æ–±—ñ–ª—ñ–≤
                    raw_price_str = row.get("–¶–µ–Ω–∞", "0").replace(',', '.').replace(' ', '')
                    # 2. –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è —É float (—â–æ–± –∑—Ä–æ–∑—É–º—ñ—Ç–∏ "802.00")
                    price_float = float(raw_price_str)
                    # 3. –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è —É int (–≤—ñ–¥–∫–∏–¥–∞—î –¥—Ä–æ–±–æ–≤—É —á–∞—Å—Ç–∏–Ω—É: 802.99 -> 802, 802.00 -> 802)
                    # –Ø–∫—â–æ –≤–∞–º –≤–∞–∂–ª–∏–≤–æ –æ–∫—Ä—É–≥–ª—è—Ç–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–Ω–æ, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ round(price_float)
                    price_int = int(price_float)
                    row["price"] = str(price_int)
                except ValueError:
                    row["price"] = "0"

                # === –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø –î–ê–¢–ò ===
                # –î–æ–¥–∞—î–º–æ i —Ö–≤–∏–ª–∏–Ω –¥–æ –±–∞–∑–æ–≤–æ–≥–æ —á–∞—Å—É
                # i=0 -> 00:00, i=1 -> 00:01, i=2 -> 00:02...
                row_time = base_date + timedelta(minutes=i)
                row["date_added"] = row_time.strftime('%Y-%m-%d %H:%M:%S')

                row["store_id"] = "0"
                row["layout_id"] = "0"
                row["postachalnyk"] = name_ukr

                writer.writerow(row)

        os.replace(temp_path, csv_path)
        logging.info("–§–∞–π–ª –æ–Ω–æ–≤–ª–µ–Ω–æ –ø—ñ–¥ —Å—Ç–∞–Ω–¥–∞—Ä—Ç OpenCart.")

        if new_category_entries:
            append_new_categories(new_category_entries, cat_fieldnames)

    except Exception as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞: {e}")
        if os.path.exists(temp_path):
            os.remove(temp_path)

# –û–°–ù–û–í–ù–ê –§–£–ù–ö–¶–Ü–Ø 7: –ü–æ–≤—Ç–æ—Ä–Ω–µ –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–π
def refill_product_category():
    """
    –ü–æ–≤—Ç–æ—Ä–Ω–æ –ø—Ä–æ—Ö–æ–¥–∏—Ç—å –ø–æ —Ñ–∞–π–ª—É —Ç–æ–≤–∞—Ä—ñ–≤ —ñ –∑–∞–ø–æ–≤–Ω—é—î –∫–æ–ª–æ–Ω–∫–∏ –∫–∞—Ç–µ–≥–æ—Ä—ñ–π
    (category, –ö–∞—Ç–µ–≥–æ—Ä—ñ—è|ua, –ö–∞—Ç–µ–≥–æ—Ä–∏—è|ru) –Ω–∞ –æ—Å–Ω–æ–≤—ñ rules –∑ category.csv.
    
    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è, –∫–æ–ª–∏ –≤ category.csv –¥–æ–¥–∞–ª–∏ –Ω–æ–≤—ñ –ø—Ä–∞–≤–∏–ª–∞ —Ä—É–∫–∞–º–∏,
    —ñ —Ç—Ä–µ–±–∞ –æ–Ω–æ–≤–∏—Ç–∏ —Ç–æ–≤–∞—Ä–∏ –±–µ–∑ –ø–æ–≤–Ω–æ–≥–æ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫—É –≤—Å—å–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—É.
    """
    oc_log_message()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 7. –ü–æ—á–∏–Ω–∞—é –ø–æ–≤—Ç–æ—Ä–Ω–µ –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–π (–ø–æ –Ω–∞–∑–≤–∞—Ö)...")

    # --- 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å ---
    settings = load_oc_settings()
    try:
        csv_path = settings['paths']['csv_path_new_product']
    except (TypeError, KeyError) as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –¥–æ—Å—Ç—É–ø—É –¥–æ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å: {e}")
        return

    # --- 2. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø—Ä–∞–≤–∏–ª (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å–ø—ñ–ª—å–Ω—É —Ñ—É–Ω–∫—Ü—ñ—é) ---
    # category_map –º–∞—î —Å—Ç—Ä—É–∫—Ç—É—Ä—É: {(key): {'category': '...', 'cat_ua': '...', 'cat_ru': '...'}}
    category_map, _ = load_category_csv()
    
    logging.info(f"–ó—á–∏—Ç–∞–Ω–æ {len(category_map)} –ø—Ä–∞–≤–∏–ª –∫–∞—Ç–µ–≥–æ—Ä—ñ–π.")

    # --- 3. –û–±—Ä–æ–±–∫–∞ CSV ---
    temp_path = csv_path + '.refill_temp'
    updated_rows_count = 0
    missing_category_rows = []

    try:
        with open(csv_path, 'r', encoding='utf-8') as infile, \
             open(temp_path, 'w', encoding='utf-8', newline='') as outfile:

            reader = csv.DictReader(infile)
            fieldnames = list(reader.fieldnames)
            
            # –ü–µ—Ä–µ–∫–æ–Ω—É—î–º–æ—Å—å, —â–æ —Ü—ñ–ª—å–æ–≤—ñ –∫–æ–ª–æ–Ω–∫–∏ —ñ—Å–Ω—É—é—Ç—å (—Ö–æ—á–∞ –ø—Ä–∏ refill –≤–æ–Ω–∏ –≤–∂–µ –º–∞—é—Ç—å –±—É—Ç–∏)
            target_cols = ['category', '–ö–∞—Ç–µ–≥–æ—Ä—ñ—è|ua', '–ö–∞—Ç–µ–≥–æ—Ä–∏—è|ru']
            for col in target_cols:
                if col not in fieldnames:
                    fieldnames.append(col)

            writer = csv.DictWriter(outfile, fieldnames=fieldnames)
            writer.writeheader()

            for idx, row in enumerate(reader):
                # --- –§–æ—Ä–º—É—î–º–æ –∫–ª—é—á –ø–æ—à—É–∫—É (—Ç–∞–∫ —Å–∞–º–æ, —è–∫ –≤ fill_auxiliary_columns) ---
                c1 = row.get("–ö–∞—Ç–µ–≥–æ—Ä–∏—è", "").strip()
                c2 = row.get("–î–æ–ø. –ö–∞—Ç–µ–≥–æ—Ä–∏—è 1", "").strip()
                c3 = row.get("–î–æ–ø. –ö–∞—Ç–µ–≥–æ—Ä–∏—è 2", "").strip()
                
                # –°–æ—Ä—Ç—É—î–º–æ, —â–æ–± –Ω–µ –∑–∞–ª–µ–∂–∞—Ç–∏ –≤—ñ–¥ –ø–æ—Ä—è–¥–∫—É —Å–ª—ñ–≤
                key = tuple(sorted([c1.lower(), c2.lower(), c3.lower()]))

                row_changed = False
                
                # --- –ü–æ—à—É–∫ —É –º–∞–ø—ñ ---
                if key in category_map:
                    data = category_map[key]
                    
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ç–∞ –æ–Ω–æ–≤–ª—é—î–º–æ ID –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó
                    if row.get('category', '').strip() != data['category']:
                        row['category'] = data['category']
                        row_changed = True
                    
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ç–∞ –æ–Ω–æ–≤–ª—é—î–º–æ UA –Ω–∞–∑–≤—É
                    if row.get('–ö–∞—Ç–µ–≥–æ—Ä—ñ—è|ua', '').strip() != data['cat_ua']:
                        row['–ö–∞—Ç–µ–≥–æ—Ä—ñ—è|ua'] = data['cat_ua']
                        row_changed = True

                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ç–∞ –æ–Ω–æ–≤–ª—é—î–º–æ RU –Ω–∞–∑–≤—É
                    if row.get('–ö–∞—Ç–µ–≥–æ—Ä–∏—è|ru', '').strip() != data['cat_ru']:
                        row['–ö–∞—Ç–µ–≥–æ—Ä–∏—è|ru'] = data['cat_ru']
                        row_changed = True
                
                # --- –õ–æ–≥—É–≤–∞–Ω–Ω—è –∑–º—ñ–Ω ---
                if row_changed:
                    updated_rows_count += 1
                    # logging.info(f"–†—è–¥–æ–∫ {idx + 2}: –û–Ω–æ–≤–ª–µ–Ω–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—é –¥–ª—è '{c1} {c2}' -> ID: {row['category']}")

                # --- –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –ø—Ä–æ–ø—É—Å–∫–∏ ---
                # –Ø–∫—â–æ –ø—ñ—Å–ª—è –≤—Å—ñ—Ö –º–∞–Ω—ñ–ø—É–ª—è—Ü—ñ–π ID –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó –≤—Å–µ —â–µ –ø–æ—Ä–æ–∂–Ω—ñ–π - –∑–∞–ø–∏—Å—É—î–º–æ –≤ –ø–æ–º–∏–ª–∫–∏
                # (–Ü–≥–Ω–æ—Ä—É—î–º–æ —Ä—è–¥–∫–∏, –¥–µ –≤–∑–∞–≥–∞–ª—ñ –Ω–µ–º–∞—î –≤—Ö—ñ–¥–Ω–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä—ñ–π)
                if any(key) and not row.get('category', '').strip():
                    missing_category_rows.append(idx + 2)

                writer.writerow(row)

        # --- 4. –ó–∞–º—ñ–Ω–∞ —Ñ–∞–π–ª—É ---
        os.replace(temp_path, csv_path)
        logging.info(f"–ü–æ–≤—Ç–æ—Ä–Ω–µ –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –û–Ω–æ–≤–ª–µ–Ω–æ –∑–∞–ø–∏—Å—ñ–≤: {updated_rows_count}.")

        # --- 5. –í–∏–≤—ñ–¥ –ø–æ–ø–µ—Ä–µ–¥–∂–µ–Ω—å ---
        if missing_category_rows:
            logging.warning(f"–£–í–ê–ì–ê: {len(missing_category_rows)} —Ç–æ–≤–∞—Ä—ñ–≤ –≤—Å–µ —â–µ –±–µ–∑ –ø—Ä–∏–≤'—è–∑–∫–∏ –¥–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó (ID empty).")
            # –í–∏–≤–æ–¥–∏–º–æ –ø–µ—Ä—à—ñ 5 –¥–ª—è –ø—Ä–∏–∫–ª–∞–¥—É, —â–æ–± –Ω–µ —Å–ø–∞–º–∏—Ç–∏
            logging.warning(f"–ù–æ–º–µ—Ä–∏ —Ä—è–¥–∫—ñ–≤ (–ø–µ—Ä—à—ñ 5): {missing_category_rows[:5]} ...")

    except Exception as e:
        logging.error(f"–ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏ refill_product_category: {e}")
        if os.path.exists(temp_path):
            os.remove(temp_path)

# –û–°–ù–û–í–ù–ê –§–£–ù–ö–¶–Ü–Ø 8: –ó–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è —Ç–æ–≤–∞—Ä—ñ–≤, —è–∫—ñ –≤–∂–µ —î –≤ –±–∞–∑—ñ (–ù–ï–û–ù–û–í–õ–ï–ù–û)
def separate_existing_products():
    """
    –ó–≤—ñ—Ä—è—î —à—Ç—Ä–∏—Ö–∫–æ–¥–∏ –ø—Ä–∞–π—Å—É 1.csv –∑ –±–∞–∑–æ—é (oc_zalishki.csv),
    –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—å –∑–Ω–∞–π–¥–µ–Ω—ñ —Ç–æ–≤–∞—Ä–∏ —É old_prod_new_SHK.csv,
    –≤–∏–¥–∞–ª—è—î —ó—Ö –∑ 1.csv —Ç–∞ —Ñ–æ—Ä–º—É—î –ø—ñ–¥—Å—É–º–∫–æ–≤—É —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É.
    """
    oc_log_message()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 8. –ü–æ—á–∏–Ω–∞—é –∑–≤—ñ—Ä–∫—É 1.csv –∑—ñ —à—Ç—Ä–∏—Ö–∫–æ–¥–∞–º–∏ –±–∞–∑–∏ (zalishki.csv)...")

    settings = load_oc_settings()
    try:
        sl_new_path = settings['paths']['csv_path_supliers_1_new']
        zalishki_path = settings['paths']['output_file']
        sl_old_prod_shk_path = settings['paths']['csv_path_sl_old_prod_new_shk']
        column_mapping = settings['suppliers'][1]['column_mapping_sl_old_to_sl_new']
    except KeyError as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó. –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —à–ª—è—Ö –∞–±–æ –º–∞–ø—É –∫–æ–ª–æ–Ω–æ–∫: {e}")
        return

    # --- 0. –ó—á–∏—Ç—É–≤–∞–Ω–Ω—è —ñ—Å–Ω—É—é—á–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞ old_prod_new_SHK.csv ---
    sl_old_header = []
    try:
        if os.path.exists(sl_old_prod_shk_path):
            with open(sl_old_prod_shk_path, mode='r', encoding='utf-8') as f:
                reader = csv.reader(f)
                sl_old_header = next(reader, [])
        else:
            logging.warning("–§–∞–π–ª old_prod_new_SHK.csv –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ ‚Äî —Å—Ç–≤–æ—Ä—é—é –Ω–æ–≤–∏–π —ñ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º.")
            sl_old_header_base = [
                'id', 'sku', '–ú–µ—Ç–∞: url_lutsk', '–ú–µ—Ç–∞: shtrih_cod', '–ú–µ—Ç–∞: artykul_lutsk', '–ü–æ–∑–Ω–∞—á–∫–∏',
                'rank_math_focus_keyword', '–ú–µ—Ç–∞: postachalnyk', 'manage_stock', 'tax_status', 'excerpt'
            ]
            # –î–æ–¥–∞—î–º–æ –∞—Ç—Ä–∏–±—É—Ç–∏ —Ç–∞ –¥–æ–¥–∞—Ç–∫–æ–≤—ñ –∫–æ–ª–æ–Ω–∫–∏ (–±–µ–∑ attribute_none)
            sl_old_header = sl_old_header_base + [f'attribute_{i}' for i in range(1, 24)] + [
                'content', 'post_date', 'product_type'
            ]

        # –û—á–∏—â–∞—î–º–æ —Ñ–∞–π–ª, –∞–ª–µ –∑–∞–ª–∏—à–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫
        with open(sl_old_prod_shk_path, mode='w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(sl_old_header)

        logging.info("–§–∞–π–ª old_prod_new_SHK.csv –æ—á–∏—â–µ–Ω–æ, –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∑–∞–ª–∏—à–µ–Ω–æ –±–µ–∑ –∑–º—ñ–Ω.")
    except Exception as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó old_prod_new_SHK.csv: {e}")
        return

    # --- 1. –ó—á–∏—Ç—É–≤–∞–Ω–Ω—è –±–∞–∑–∏ —à—Ç—Ä–∏—Ö–∫–æ–¥—ñ–≤ ---
    zalishki_map = {}  # {shk: (id, sku)}
    try:
        with open(zalishki_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            next(reader)  # –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫
            for row in reader:
                if len(row) > 7:
                    shk = row[7].strip()
                    if shk:
                        zalishki_map[shk] = (row[0].strip(), row[1].strip())
        logging.info(f"–ó—á–∏—Ç–∞–Ω–æ {len(zalishki_map)} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —à—Ç—Ä–∏—Ö–∫–æ–¥—ñ–≤ –∑ –±–∞–∑–∏.")
    except Exception as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —á–∏—Ç–∞–Ω–Ω—ñ –±–∞–∑–∏: {e}")
        return

    # --- 2. –û–±—Ä–æ–±–∫–∞ 1.csv —Ç–∞ —Ñ–æ—Ä–º—É–≤–∞–Ω–Ω—è —Å–ø–∏—Å–∫—ñ–≤ ---
    items_to_keep = []
    items_to_move = []

    try:
        with open(sl_new_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            header = next(reader)
            items_to_keep.append(header)

            for row in reader:
                # –†–æ–∑—à–∏—Ä—é—î–º–æ —Ä—è–¥–æ–∫ –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —ñ–Ω–¥–µ–∫—Å—É —É –º–∞–ø—ñ
                max_index = max(column_mapping.values())
                if len(row) <= max_index:
                    row.extend([''] * (max_index + 1 - len(row)))

                shk_value = row[2].strip()  # C (–®—Ç—Ä–∏—Ö–∫–æ–¥)
                if shk_value in zalishki_map:
                    item_id, item_sku = zalishki_map[shk_value]

                    # –§–æ—Ä–º—É—î–º–æ –Ω–æ–≤–∏–π —Ä—è–¥–æ–∫ –¥–ª—è old_prod_new_SHK.csv
                    new_row = [''] * len(sl_old_header)
                    new_row[0] = item_id
                    new_row[1] = item_sku

                    for sl_old_idx_str, sl_new_idx in column_mapping.items():
                        sl_old_idx = int(sl_old_idx_str)  # –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ –∫–ª—é—á —É int
                        if sl_new_idx < len(row):
                            new_row[sl_old_idx] = row[sl_new_idx]

                    items_to_move.append(new_row)
                else:
                    items_to_keep.append(row)

        # --- 3. –ó–∞–ø–∏—Å –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ ---
        if items_to_move:
            with open(sl_old_prod_shk_path, 'a', encoding='utf-8', newline='') as f:
                writer = csv.writer(f)
                writer.writerows(items_to_move)
            logging.info(f"–ü–µ—Ä–µ–Ω–µ—Å–µ–Ω–æ {len(items_to_move)} —ñ—Å–Ω—É—é—á–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ —É old_prod_new_SHK.csv.")
        else:
            logging.info("–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∂–æ–¥–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä—É –∑ —ñ—Å–Ω—É—é—á–∏–º —à—Ç—Ä–∏—Ö–∫–æ–¥–æ–º —É –±–∞–∑—ñ.")

        # --- 4. –ó–∞–ø–∏—Å –æ–Ω–æ–≤–ª–µ–Ω–æ–≥–æ 1.csv ---
        temp_path = sl_new_path + '.temp'
        with open(temp_path, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerows(items_to_keep)
        os.replace(temp_path, sl_new_path)
        logging.info(f"1.csv –æ–Ω–æ–≤–ª–µ–Ω–æ. –ó–∞–ª–∏—à–∏–ª–æ—Å—å {len(items_to_keep)-1} –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ –¥–ª—è —ñ–º–ø–æ—Ä—Ç—É.")

    except Exception as e:
        logging.error(f"–ù–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –æ–±—Ä–æ–±–∫–∏ 1.csv: {e}")
        if 'temp_path' in locals() and os.path.exists(temp_path):
            os.remove(temp_path)

# –û–°–ù–û–í–ù–ê –§–£–ù–ö–¶–Ü–Ø 9: –ù–æ–≤—ñ SKU
def assign_new_sku_to_products():
    """
    –ó–Ω–∞—Ö–æ–¥–∏—Ç—å –Ω–∞–π–±—ñ–ª—å—à–∏–π SKU —É zalishki.csv (—à—É–∫–∞—î –≤ –∫–æ–ª–æ–Ω—Ü—ñ 'sku')
    —ñ –ø—Ä–∏—Å–≤–æ—é—î –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ SKU —Ç–æ–≤–∞—Ä–∞–º –±–µ–∑ SKU —É –∫–æ–ª–æ–Ω—Ü—ñ 'sku' —Ñ–∞–π–ª—É 1.csv.
    """
    oc_log_message()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 9. –ü–æ—á–∏–Ω–∞—é –ø—Ä–∏—Å–≤–æ—î–Ω–Ω—è –Ω–æ–≤–∏—Ö SKU —Ç–æ–≤–∞—Ä–∞–º (–ø–æ –Ω–∞–∑–≤–∞—Ö –∫–æ–ª–æ–Ω–æ–∫)...")

    # --- 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å ---
    settings = load_oc_settings()
    try:
        new_product = settings['paths']['csv_path_new_product']
        zalishki = settings['paths']['output_file']
    except KeyError as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó. –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —à–ª—è—Ö: {e}")
        return

    # --- 2. –ó–Ω–∞—Ö–æ–¥–∏–º–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π SKU —É zalishki.csv ---
    try:
        with open(zalishki, mode='r', encoding='utf-8') as f:
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ DictReader –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –Ω–∞–∑–≤–∞–º–∏ –∫–æ–ª–æ–Ω–æ–∫
            reader = csv.DictReader(f)
            
            sku_list = []
            for row in reader:
                # –û—Ç—Ä–∏–º—É—î–º–æ –∑–Ω–∞—á–µ–Ω–Ω—è –ø–æ –Ω–∞–∑–≤—ñ –∫–æ–ª–æ–Ω–∫–∏ 'sku'
                val = row.get('sku', '').strip()
                
                if val.isdigit():
                    sku_list.append(int(val))

            if not sku_list:
                logging.warning("–£ –±–∞–∑—ñ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∂–æ–¥–Ω–æ–≥–æ —á–∏—Å–ª–æ–≤–æ–≥–æ SKU. –ü—Ä–∏—Å–≤–æ—î–Ω–Ω—è –Ω–µ–º–æ–∂–ª–∏–≤–µ.")
                return

            sku_list.sort()
            last_sku = sku_list[-1]
            logging.info(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π SKU —É –±–∞–∑—ñ: {last_sku}")

    except FileNotFoundError:
        logging.error(f"–§–∞–π–ª –±–∞–∑–∏ zalishki.csv –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∑–∞ —à–ª—è—Ö–æ–º: {zalishki}")
        return
    except Exception as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —á–∏—Ç–∞–Ω–Ω—ñ zalishki.csv: {e}")
        return

    # --- 3. –ü—Ä–∏—Å–≤–æ—î–Ω–Ω—è –Ω–æ–≤–∏—Ö SKU —É 1.csv ---
    next_sku = last_sku + 1
    assigned_count = 0
    temp_path = new_product + '.temp'

    try:
        with open(new_product, mode='r', encoding='utf-8', newline='') as input_file:
            reader = csv.DictReader(input_file)
            fieldnames = reader.fieldnames # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –∑–∞–ø–∏—Å—É
            
            # –Ø–∫—â–æ –∫–æ–ª–æ–Ω–∫–∏ 'sku' –Ω–µ–º–∞—î —É —Ñ–∞–π–ª—ñ, –∫–æ–¥ –≤–ø–∞–¥–µ, —Ç–æ–º—É —Ä–æ–±–∏–º–æ –ø–µ—Ä–µ–≤—ñ—Ä–∫—É
            if 'sku' not in fieldnames:
                 logging.error("–£ —Ñ–∞–π–ª—ñ –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ –≤—ñ–¥—Å—É—Ç–Ω—è –∫–æ–ª–æ–Ω–∫–∞ 'sku'.")
                 return

            rows = []
            for row in reader:
                current_sku = row.get('sku', '').strip()
                
                # –Ø–∫—â–æ SKU –ø–æ—Ä–æ–∂–Ω—ñ–π, –ø—Ä–∏—Å–≤–æ—é—î–º–æ –Ω–æ–≤–∏–π
                if not current_sku:
                    row['sku'] = str(next_sku)
                    assigned_count += 1
                    next_sku += 1
                
                rows.append(row)

        # --- 4. –ó–∞–ø–∏—Å –æ–Ω–æ–≤–ª–µ–Ω–æ–≥–æ CSV ---
        if assigned_count > 0:
            with open(temp_path, mode='w', encoding='utf-8', newline='') as f:
                # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ DictWriter –¥–ª—è –∑–∞–ø–∏—Å—É —Å–ª–æ–≤–Ω–∏–∫—ñ–≤
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader() # –ó–∞–ø–∏—Å—É—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫
                writer.writerows(rows) # –ó–∞–ø–∏—Å—É—î–º–æ –¥–∞–Ω—ñ
            
            os.replace(temp_path, new_product)
            logging.info(f"‚úÖ –£—Å–ø—ñ—à–Ω–æ –ø—Ä–∏—Å–≤–æ—î–Ω–æ {assigned_count} –Ω–æ–≤–∏—Ö SKU. –ù–∞—Å—Ç—É–ø–Ω–∏–π SKU –±—É–¥–µ {next_sku}.")
        else:
            logging.info("–£—Å—ñ —Ç–æ–≤–∞—Ä–∏ –≤–∂–µ –º–∞—é—Ç—å SKU. –ó–º—ñ–Ω –Ω–µ –≤–Ω–µ—Å–µ–Ω–æ.")

    except FileNotFoundError:
        logging.error(f"–§–∞–π–ª –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
    except Exception as e:
        logging.error(f"–ù–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –ø—Ä–∏—Å–≤–æ—î–Ω–Ω—è SKU: {e}")
        if os.path.exists(temp_path):
            os.remove(temp_path)

# –û–°–ù–û–í–ù–ê –§–£–ù–ö–¶–Ü–Ø 10: –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω—å
def process_phase_1_download():
    """
    –ï–¢–ê–ü 1: 
    1. –û—á–∏—Å—Ç–∫–∞ –ø–∞–ø–æ–∫.
    2. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ñ–æ—Ç–æ (–∑ 'url').
    3. –ó–∞–ø–∏—Å —Å–ø–∏—Å–∫—É —Ñ–∞–π–ª—ñ–≤ —É 'img_name_jpg'.
    """
    oc_log_message()
    logging.info("üöÄ –§–ê–ó–ê 1. –ü–æ—á–∞—Ç–æ–∫ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω—å...")

    settings = load_oc_settings()
    try:
        csv_path = settings['paths']['csv_path_new_product']
        jpg_path = settings['paths']['img_path_jpg']
        webp_path = settings['paths']['img_path_webp']
        cat_map = settings['categories']
    except KeyError as e:
        logging.error(f"‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —à–ª—è—Ö —É settings.json: {e}")
        return

    # 1Ô∏è‚É£ –û—á–∏—Å—Ç–∫–∞
    clear_directory(jpg_path)
    clear_directory(webp_path)
    logging.info("1. ‚úÖ –û—á–∏—Å—Ç–∫–∞ –ø–∞–ø–æ–∫ JPG —Ç–∞ WEBP –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

    # 2Ô∏è‚É£ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
    rows = []
    fieldnames = []

    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        fieldnames = reader.fieldnames # –û—Ç—Ä–∏–º—É—î–º–æ —Å–ø–∏—Å–æ–∫ –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞, —á–∏ —ñ—Å–Ω—É—î –∫–æ–ª–æ–Ω–∫–∞ –¥–ª—è –∑–∞–ø–∏—Å—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É, —è–∫—â–æ –Ω—ñ ‚Äî –¥–æ–¥–∞—î–º–æ
        if 'img_name_jpg' not in fieldnames:
            fieldnames.append('img_name_jpg')

        for row in reader:
            # üëâ –ü–†–Ø–ú–ï –í–ò–ö–û–†–ò–°–¢–ê–ù–ù–Ø –ù–ê–ó–í –ö–û–õ–û–ù–û–ö –¢–£–¢:
            url = row.get('url_lutsk', '').strip()
            sku = row.get('sku', '').strip()
            cat = row.get('category', '').strip()

            if url and sku and cat:
                # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
                imgs = download_product_images(url, sku, cat, jpg_path, cat_map)
                
                # –ó–∞–ø–∏—Å—É—î–º–æ —ñ–º–µ–Ω–∞ —Ñ–∞–π–ª—ñ–≤ —É –∫–æ–ª–æ–Ω–∫—É 'img_name_jpg'
                row['img_name_jpg'] = ', '.join(imgs) if imgs else ''
            
            rows.append(row)
            time.sleep(random.uniform(0.1, 0.5))

    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –æ–Ω–æ–≤–ª–µ–Ω–æ–≥–æ CSV
    with open(csv_path, 'w', encoding='utf-8', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(rows)

    logging.info(f"2. üì• –§–ê–ó–ê 1 –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û–±—Ä–æ–±–ª–µ–Ω–æ {len(rows)} —Ä—è–¥–∫—ñ–≤.")

def process_phase_2_finish():
    """
    –ï–¢–ê–ü 2:
    1. –ü–µ—Ä–µ–º—ñ—â–µ–Ω–Ω—è GIF.
    2. –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è —É WEBP.
    3. –û–Ω–æ–≤–ª–µ–Ω–Ω—è –∫–æ–ª–æ–Ω–∫–∏ 'image_name_webp'.
    4. –ö–æ–ø—ñ—é–≤–∞–Ω–Ω—è –Ω–∞ —Å–∞–π—Ç.
    """
    oc_log_message()
    logging.info("‚öôÔ∏è –§–ê–ó–ê 2. –ü–æ—á–∞—Ç–æ–∫ –æ–±—Ä–æ–±–∫–∏ —Ñ–∞–π–ª—ñ–≤...")
    
    settings = load_oc_settings()
    try:
        csv_path = settings['paths']['csv_path_new_product']
        jpg_path = settings['paths']['img_path_jpg']
        webp_path = settings['paths']['img_path_webp']
        site_path = settings['paths']['site_path_images']
    except KeyError as e:
        logging.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å: {e}")
        return

    # 3Ô∏è‚É£ GIF
    move_gifs(jpg_path, webp_path)
    logging.info("3. ‚úÖ –ü–µ—Ä–µ–º—ñ—â–µ–Ω–Ω—è GIF –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")

    # 4Ô∏è‚É£ WEBP
    convert_to_webp_square(jpg_path, webp_path)
    logging.info("4. ‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è JPG —É WEBP –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

    # 5Ô∏è‚É£ –û–Ω–æ–≤–ª–µ–Ω–Ω—è CSV (–≤–∏–∫–ª–∏–∫–∞—î–º–æ –Ω–æ–≤—É —Ñ—É–Ω–∫—Ü—ñ—é —Å–∏–Ω—Ö—Ä–æ–Ω—ñ–∑–∞—Ü—ñ—ó)
    sync_webp_column_named(csv_path, webp_path)
    logging.info("5. ‚úÖ –û–Ω–æ–≤–ª–µ–Ω–Ω—è –∫–æ–ª–æ–Ω–∫–∏ WEBP —É CSV –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")

    # 6Ô∏è‚É£ –ù–∞ –æ—Å–Ω–æ–≤—ñ –∫–æ–ª–æ–Ω–∫–∏ 'image_name_webp' –∑–∞–ø–æ–≤–Ω—é—î–º–æ —à–ª—è—Ö–∏ –¥–ª—è OpenCart
    # –¢—É—Ç —Å–∫—Ä–∏–ø—Ç —Ä–æ–±–∏—Ç—å: "catalog/product/.../sku.webp"
    fill_opencart_paths_single_file()  # <--- –ù–û–í–ê –§–£–ù–ö–¶–Ü–Ø

    # 7Ô∏è‚É£ –ö–æ–ø—ñ—é–≤–∞–Ω–Ω—è
    copy_to_site(webp_path, site_path)
    logging.info("6. ‚úÖ –ö–æ–ø—ñ—é–≤–∞–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω—å –Ω–∞ —Å–∞–π—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")


    
    logging.info("üèÅ –§–ê–ó–ê 2 –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø—ñ—à–Ω–æ.")

    import csv

# –û–°–ù–û–í–ù–ê –§–£–ù–ö–¶–Ü–Ø 11: –ü–µ—Ä–µ–∫–ª–∞–¥ –Ω–∞ —Ä–æ—Å
def translate_and_prepare_csv():
    """
    –ü–µ—Ä–µ–∫–ª–∞–¥ –Ω–∞–∑–≤–∏ —ñ –æ–ø–∏—Å—É –∑ –¥–æ–ø–æ–º–æ–≥–æ—é Deepl
    –¢–∞–∫–æ–∂ –∑–∞–ø–æ–≤–Ω—é—é—Ç—å—Å—è –¥–æ–ø–æ–º—ñ–∂–Ω—ñ –°–ï–û–ø–æ–ª—è
    """
    
    oc_log_message() 
    logging.info("üöÄ –ü–æ—á–∞—Ç–æ–∫ –æ–±—Ä–æ–±–∫–∏ CSV –¥–ª—è OpenCart (–ü–æ–≤–Ω–∏–π —Ü–∏–∫–ª)...")

    settings = load_oc_settings()
    
    # --- –í–ò–ö–û–†–ò–°–¢–û–í–£–Ñ–ú–û –û–î–ò–ù –§–ê–ô–õ ---
    csv_path = settings['paths']['csv_path_new_product']
    api_key = settings['deepl_api_key'] # –Ø–∫—â–æ –∑–∞–∫—ñ–Ω—á–∞—Ç—å—Å—è –ª—è–º—ñ—Ç–∏ –Ω–∞ —Ü—å–æ–º—É, —Ç–æ —Ç—Ä–µ–±–∞ –≤–∑—è—Ç–∏ deepl_api_key2
    api_url = settings['DEEPL_API_URL']

    if not csv_path or not api_key:
        logging.error("‚ùå –ù–µ –≤–∫–∞–∑–∞–Ω–æ —à–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É (csv_path_new_product) –∞–±–æ API –∫–ª—é—á")
        return

    # 1. –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —Å–ª–æ–≤–Ω–∏–∫ –ø–æ–∑–Ω–∞—á–æ–∫ (–≤–∏–∫–ª–∏–∫ –±–µ–∑ –∞—Ä–≥—É–º–µ–Ω—Ç—ñ–≤, –±–æ —à–ª—è—Ö –±–µ—Ä–µ—Ç—å—Å—è –∑ settings –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó)
    tags_map = load_poznachky_csv()

    # 2. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ª—ñ–º—ñ—Ç—É DeepL
    get_deepl_usage(api_key)

    try:

        # --- –ö–†–û–ö 1: –ß–ò–¢–ê–ù–ù–Ø –§–ê–ô–õ–£ ---
        rows = []
        fieldnames = []
        
        with open(csv_path, 'r', encoding='utf-8') as f_in:
            reader = csv.DictReader(f_in)
            fieldnames = list(reader.fieldnames) # –ö–æ–ø—ñ—é—î–º–æ —Å–ø–∏—Å–æ–∫ –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤
            rows = list(reader)                  # –ß–∏—Ç–∞—î–º–æ –≤—Å—ñ –¥–∞–Ω—ñ –≤ –ø–∞–º'—è—Ç—å

        total_rows = len(rows)
        logging.info(f"üì¶ –ó—á–∏—Ç–∞–Ω–æ {total_rows} —Ç–æ–≤–∞—Ä—ñ–≤ –∑ {csv_path}")

        # --- –ö–†–û–ö 2: –î–û–î–ê–í–ê–ù–ù–Ø –ù–û–í–ò–• –ö–û–õ–û–ù–û–ö (–Ø–ö–©–û –á–• –ù–ï–ú–ê–Ñ) ---
        required_cols = [
            "name|ru", "description|ru", 
            "meta_title|ua", "meta_title|ru", 
            "meta_keywords|ru", 
            "meta_description|ua", "meta_description|ru"
        ]
        for col in required_cols:
            if col not in fieldnames:
                fieldnames.append(col)

        # --- –ö–†–û–ö 3: –û–ë–†–û–ë–ö–ê –î–ê–ù–ò–• ---
        processed_rows = []
        
        for idx, row in enumerate(rows, start=1):
            # --- –ó–ß–ò–¢–£–í–ê–ù–ù–Ø –í–ò–•–Ü–î–ù–ò–• –î–ê–ù–ò–• ---
            name_ua = row.get("name|ua", "").strip()
            # –û–±–µ—Ä–µ–∂–Ω–æ –∑ –Ω–∞–∑–≤–æ—é –∫–æ–ª–æ–Ω–∫–∏ –æ–ø–∏—Å—É (—É –≤–∞—Å –±—É–ª–æ "–û–ø–∏—Å–∞–Ω–∏–µ")
            desc_ua = row.get("–û–ø–∏—Å–∞–Ω–∏–µ", "").strip() 
            tags_ua_raw = row.get("–ü–æ–∑–Ω–∞—á–∫–∏", "").strip()
            

            # 1. –ü–ï–†–ï–ö–õ–ê–î –ù–ê–ó–í–ò (—è–∫—â–æ –ø—É—Å—Ç–æ)
            if name_ua and not row.get("name|ru"):
                row["name|ru"] = translate_text_deepl(name_ua, "RU", api_key, api_url)
            
            name_ru = row.get("name|ru", "") # –ê–∫—Ç—É–∞–ª—å–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è

            # 2. –ü–ï–†–ï–ö–õ–ê–î –û–ü–ò–°–£ (—è–∫—â–æ –ø—É—Å—Ç–æ)
            if desc_ua and not row.get("description|ru"):
                # –ü—Ä–∏–º—ñ—Ç–∫–∞: is_html –Ω–µ –ø–æ—Ç—Ä—ñ–±–µ–Ω –¥–ª—è –Ω–æ–≤–æ—ó "—Ä–æ–∑—É–º–Ω–æ—ó" —Ñ—É–Ω–∫—Ü—ñ—ó
                row["description|ru"] = translate_text_deepl(desc_ua, "RU", api_key, api_url)
            
            desc_ru = row.get("description|ru", "")

            # 3. META TITLE
            # UA: –ù–∞–∑–≤–∞ + —Å—É—Ñ—ñ–∫—Å
            if name_ua:
                row["meta_title|ua"] = f"{name_ua} üíï –Ü–Ω—Ç–∏–º-–ë—É—Ç—ñ–∫ –ï–†–û–° ‚ù±‚ù± –ö—É–ø–∏—Ç–∏ —Å–µ–∫—Å —ñ–≥—Ä–∞—à–∫–∏ –≤ –£–∫—Ä–∞—ó–Ω—ñ"
            # RU: –ù–∞–∑–≤–∞ RU + —Å—É—Ñ—ñ–∫—Å
            if name_ru:
                row["meta_title|ru"] = f"{name_ru} üíï –ò–Ω—Ç–∏–º-–ë—É—Ç–∏–∫ –ï–†–û–° ‚ù±‚ù± –ö—É–ø–∏—Ç—å —Å–µ–∫—Å –∏–≥—Ä—É—à–∫–∏ –≤ –£–∫—Ä–∞–∏–Ω–µ"

            # 4. META KEYWORDS (RU) - –°–ª–æ–≤–Ω–∏–∫
            if tags_ua_raw:
                # –†–æ–∑–±–∏–≤–∞—î–º–æ –ø–æ –∫–æ–º—ñ
                source_tags = [t.strip() for t in tags_ua_raw.split(',') if t.strip()]
                translated_tags = []
                
                for tag in source_tags:
                    # –®—É–∫–∞—î–º–æ –≤ —Å–ª–æ–≤–Ω–∏–∫—É (lower() –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç—ñ)
                    translated_tag = tags_map.get(tag.lower())
                    if translated_tag:
                        translated_tags.append(translated_tag)
                    else:
                        # –Ø–∫—â–æ –ø–µ—Ä–µ–∫–ª–∞–¥—É –Ω–µ–º–∞—î - –∑–∞–ª–∏—à–∞—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª
                        translated_tags.append(tag) 
                
                row["meta_keywords|ru"] = ", ".join(translated_tags)

            # 5. META DESCRIPTION
            # UA
            if desc_ua:
                first_sent_ua = get_first_sentence(desc_ua)
                row["meta_description|ua"] = f"{first_sent_ua} | –ù–∏–∑—å–∫–∞ —Ü—ñ–Ω–∞ | –®–≤–∏–¥–∫–∞, –±–µ–∑–∫–æ—à—Ç–æ–≤–Ω–∞, –∞–Ω–æ–Ω—ñ–º–Ω–∞ –¥–æ—Å—Ç–∞–≤–∫–∞"

            # RU
            if desc_ru:
                first_sent_ru = get_first_sentence(desc_ru)
                row["meta_description|ru"] = f"{first_sent_ru} | –ù–∏–∑–∫–∞—è —Ü–µ–Ω–∞ | –ë—ã—Å—Ç—Ä–∞—è, –±–µ—Å–ø–ª–∞—Ç–Ω–∞—è, –∞–Ω–æ–Ω–∏–º–Ω–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞"

            processed_rows.append(row)

            if idx % 10 == 0:
                logging.info(f"‚úÖ –û–±—Ä–æ–±–ª–µ–Ω–æ {idx}/{total_rows} —Ç–æ–≤–∞—Ä—ñ–≤...")

        # --- –ö–†–û–ö 4: –ó–ê–ü–ò–° (–ü–ï–†–ï–ó–ê–ü–ò–°) –§–ê–ô–õ–£ ---
        with open(csv_path, 'w', encoding='utf-8', newline='') as f_out:
            writer = csv.DictWriter(f_out, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(processed_rows)
            
        logging.info(f"üéâ –ì–æ—Ç–æ–≤–æ! –§–∞–π–ª –æ–Ω–æ–≤–ª–µ–Ω–æ: {csv_path}")
        
        # –§—ñ–Ω–∞–ª—å–Ω–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ª—ñ–º—ñ—Ç—É
        get_deepl_usage(api_key)

    except Exception as e:
        logging.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")