from woocommerce import API
import json
import os, csv, shutil, logging, requests, mimetypes, glob
import logging
import html
import re
import mysql.connector
from datetime import datetime
from typing import Dict, Tuple, List, Optional, Any
from PIL import Image
from bs4 import BeautifulSoup
import time
import pymysql


# --- –ó–ê–ì–ê–õ–¨–ù–Ü –§–£–ù–ö–¶–Ü–á ---
def load_settings():
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó –∑ settings.json
    """
    config_path = os.path.join(os.path.dirname(__file__), "..", "config", "settings.json")
    try:
        with open(config_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞: —Ñ–∞–π–ª –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∑–∞ —à–ª—è—Ö–æ–º: {config_path}")
        return None
    except json.JSONDecodeError:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞: —Ñ–∞–π–ª –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó –ø–æ—à–∫–æ–¥–∂–µ–Ω–∏–π: {config_path}")
        return None

# --- –ü–Ü–î–ö–õ–Æ–ß–ï–ù–ù–Ø –î–û WOOCOMMERCE API ---
def get_wc_api(settings):
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó –∑ settings.json
    """
    config_path = os.path.join(os.path.dirname(__file__), "..", "config", "settings.json")
    with open(config_path, "r", encoding="utf-8") as f:
        settings = json.load(f)

    wcapi = API(
        url=settings["url"],
        consumer_key=settings["consumer_key"],
        consumer_secret=settings["consumer_secret"],
        version="wc/v3",
        timeout=120,
        query_string_auth=False  # –ó–º—ñ–Ω–∏–≤ –∑ True üëà –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î Basic Auth (—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ) 
    )
    return wcapi

# --- –ü–ï–†–ï–í–Ü–†–ö–ê –í–ï–†–°–Ü–á WOOCOMMERCE ---
def check_version():
    wcapi = get_wc_api()
    response = wcapi.get("system_status")
    if response.status_code == 200:
        data = response.json()
        print("WooCommerce version:", data.get("environment", {}).get("version"))
    else:
        print("Error:", response.status_code, response.text)

# --- –õ–û–ì–£–í–ê–ù–ù–Ø (–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–æ–≤–æ–≥–æ —Ñ–∞–π–ª—É)---
def setup_new_log_file():
    """
    –ü–µ—Ä–µ–π–º–µ–Ω–æ–≤—É—î —ñ—Å–Ω—É—é—á–∏–π –ª–æ–≥-—Ñ–∞–π–ª —Ç–∞ –Ω–∞–ª–∞—à—Ç–æ–≤—É—î –Ω–æ–≤–∏–π,
    –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ —à–ª—è—Ö –∑ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å.
    """
    settings = load_settings()
    if not settings or "paths" not in settings or "main_log_file" not in settings["paths"]:
        print("‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —à–ª—è—Ö –¥–æ –ª–æ–≥-—Ñ–∞–π–ª—É –≤ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è—Ö.")
        return

    current_log_path = os.path.join(os.path.dirname(__file__), "..", settings["paths"]["main_log_file"])
    log_dir = os.path.dirname(current_log_path)
    
    os.makedirs(log_dir, exist_ok=True)
    
    if os.path.exists(current_log_path):
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        file_name, file_extension = os.path.splitext(os.path.basename(current_log_path))
        new_log_path = os.path.join(log_dir, f"{file_name}_{timestamp}{file_extension}")
        try:
            os.rename(current_log_path, new_log_path)
            print(f"‚úÖ –°—Ç–∞—Ä–∏–π –ª–æ–≥-—Ñ–∞–π–ª –ø–µ—Ä–µ–π–º–µ–Ω–æ–≤–∞–Ω–æ –Ω–∞ {os.path.basename(new_log_path)}")
        except OSError as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–π–º–µ–Ω—É–≤–∞–Ω–Ω—ñ –ª–æ–≥-—Ñ–∞–π–ª—É: {e}")

    logging.basicConfig(
        filename=current_log_path,
        level=logging.INFO,
        format='%(asctime)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S',
        filemode='a'
    )
    logging.info("--- –ù–æ–≤–∏–π —Å–µ–∞–Ω—Å –ª–æ–≥—É–≤–∞–Ω–Ω—è —Ä–æ–∑–ø–æ—á–∞—Ç–æ ---")

# --- –õ–û–ì–£–í–ê–ù–ù–Ø (–ó–∞–ø–∏—Å –≤ —ñ—Å–Ω—É—é—á–∏–π —Ñ–∞–π–ª)---
def log_message_to_existing_file():
    """
    –ù–∞–ª–∞—à—Ç–æ–≤—É—î –ª–æ–≥—É–≤–∞–Ω–Ω—è –¥–ª—è –¥–æ–ø–∏—Å—É–≤–∞–Ω–Ω—è –≤ —ñ—Å–Ω—É—é—á–∏–π —Ñ–∞–π–ª,
    –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ —à–ª—è—Ö –∑ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å.
    """
    settings = load_settings()
    if not settings or "paths" not in settings or "main_log_file" not in settings["paths"]:
        print("‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —à–ª—è—Ö –¥–æ –ª–æ–≥-—Ñ–∞–π–ª—É –≤ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è—Ö.")
        return

    current_log_path = os.path.join(os.path.dirname(__file__), "..", settings["paths"]["main_log_file"])
    log_dir = os.path.dirname(current_log_path)
    
    os.makedirs(log_dir, exist_ok=True)

    if not logging.getLogger().hasHandlers():
        logging.basicConfig(
            filename=current_log_path,
            level=logging.INFO,
            #level=logging.DEBUG,
            format='%(asctime)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S',
            filemode='a'
        )
    logging.info("--- –ü–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –¥–æ–¥–∞–Ω–æ –¥–æ —ñ—Å–Ω—É—é—á–æ–≥–æ –ª–æ–≥—É ---")

# --- –ü–ï–†–ï–í–Ü–†–ö–ê CSV ---
def check_csv_data(profile_id):
    """
    –ü–µ—Ä–µ–≤—ñ—Ä—è—î CSV-—Ñ–∞–π–ª –Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω—ñ—Å—Ç—å –ø—Ä–∞–≤–∏–ª–∞–º, –≤–∏–∑–Ω–∞—á–µ–Ω–∏–º —É settings.json.
    
    Args:
        profile_id (str): ID –ø—Ä–æ—Ñ—ñ–ª—é –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –∑ 'validation_profiles' –≤ settings.json.
    
    Returns:
        bool: True, —è–∫—â–æ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø—Ä–æ–π—à–ª–∞ —É—Å–ø—ñ—à–Ω–æ, —ñ–Ω–∞–∫—à–µ False.
    """
    # 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å
    # –¶–µ–π –±–ª–æ–∫ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –∑–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó –∑ —Ñ–∞–π–ª—É settings.json
    # —Ç–∞ –ø–µ—Ä–µ–≤—ñ—Ä—è—î, —á–∏ —ñ—Å–Ω—É—î –≤–∫–∞–∑–∞–Ω–∏–π –ø—Ä–æ—Ñ—ñ–ª—å –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó.
    # –Ø–∫—â–æ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –∞–±–æ –ø—Ä–æ—Ñ—ñ–ª—å –≤—ñ–¥—Å—É—Ç–Ω—ñ–π, —Ñ—É–Ω–∫—Ü—ñ—è –∑–∞–≤–µ—Ä—à—É—î —Ä–æ–±–æ—Ç—É.
    
    log_message_to_existing_file()
    
    try:
        with open(os.path.join(os.path.dirname(__file__), "..", "config", "settings.json"), "r", encoding="utf-8") as f:
            settings = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError) as e:
        logging.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó: {e}")
        return False
        
    profiles = settings.get("validation_profiles", {})
    if profile_id not in profiles:
        logging.error(f"‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –ø—Ä–æ—Ñ—ñ–ª—å –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó –∑ ID '{profile_id}' –≤ settings.json.")
        return False
    
    # 2. –û—Ç—Ä–∏–º–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö –ø—Ä–æ—Ñ—ñ–ª—é
    # –û—Ç—Ä–∏–º—É—î–º–æ —à–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É —Ç–∞ –ø—Ä–∞–≤–∏–ª–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó –¥–ª—è –æ–±—Ä–∞–Ω–æ–≥–æ –ø—Ä–æ—Ñ—ñ–ª—é.
    profile = profiles[profile_id]
    csv_path_relative = profile.get("path")
    validation_rules = profile.get("rules")
    
    if not csv_path_relative or validation_rules is None:
        logging.error("‚ùå –ù–µ–ø–æ–≤–Ω—ñ –¥–∞–Ω—ñ –≤ –ø—Ä–æ—Ñ—ñ–ª—ñ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó.")
        return False
        
    # 3. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ —Ñ–∞–π–ª—É
    # –§–æ—Ä–º—É—î–º–æ –ø–æ–≤–Ω–∏–π —à–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É —Ç–∞ –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –π–æ–≥–æ —ñ—Å–Ω—É–≤–∞–Ω–Ω—è –Ω–∞ –¥–∏—Å–∫—É.
    base_dir = os.path.join(os.path.dirname(__file__), "..")
    full_csv_path = os.path.join(base_dir, csv_path_relative)
    
    if not os.path.exists(full_csv_path):
        logging.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞: —Ñ–∞–π–ª –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
        return False
        
    logging.info(f"üîé –ü–æ—á–∞—Ç–æ–∫ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —Ñ–∞–π–ª—É")
    
    # 4. –ß–∏—Ç–∞–Ω–Ω—è —Ç–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö
    # –í—ñ–¥–∫—Ä–∏–≤–∞—î–º–æ —Ñ–∞–π–ª —Ç–∞ –ø–æ—á–∏–Ω–∞—î–º–æ —ñ—Ç–µ—Ä–∞—Ü—ñ—é –ø–æ –π–æ–≥–æ –≤–º—ñ—Å—Ç—É.
    try:
        with open(full_csv_path, "r", encoding="utf-8") as f:
            reader = csv.reader(f)
            try:
                headers = next(reader)
            except StopIteration:
                logging.error("‚ùå –§–∞–π–ª –ø–æ—Ä–æ–∂–Ω—ñ–π. –í—ñ–¥—Å—É—Ç–Ω—ñ –∑–∞–≥–æ–ª–æ–≤–∫–∏.")
                return False
            
            # 5. –û–Ω–æ–≤–ª–µ–Ω–∞ –ª–æ–≥—ñ–∫–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –≤—Å—ñ –æ—á—ñ–∫—É–≤–∞–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏ –ø—Ä–∏—Å—É—Ç–Ω—ñ —É –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö —Ñ–∞–π–ª—É
            rule_columns = list(validation_rules.keys())
            headers_set = set(headers)
            for col_name in rule_columns:
                if col_name not in headers_set:
                    logging.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞: –æ—á—ñ–∫—É–≤–∞–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ '{col_name}' –≤—ñ–¥—Å—É—Ç–Ω—è —É —Ñ–∞–π–ª—ñ.")
                    return False
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ —Å–ª–æ–≤–Ω–∏–∫ –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –¥–æ—Å—Ç—É–ø—É –¥–æ —ñ–Ω–¥–µ–∫—Å—ñ–≤ –∫–æ–ª–æ–Ω–æ–∫
            header_map = {name: index for index, name in enumerate(headers)}
            
            # 6. –í–∞–ª—ñ–¥–∞—Ü—ñ—è –∫–æ–∂–Ω–æ–≥–æ —Ä—è–¥–∫–∞
            for i, row in enumerate(reader):
                row_number = i + 2
                if not row or all(not col.strip() for col in row):
                    logging.info(f"‚úÖ –†—è–¥–æ–∫ {row_number} –ø–æ—Ä–æ–∂–Ω—ñ–π –∞–±–æ –º—ñ—Å—Ç–∏—Ç—å –ª–∏—à–µ –ø—Ä–æ–±—ñ–ª–∏. –ü—Ä–æ–ø—É—Å–∫–∞—é.")
                    continue
                
                # 7. –í–∞–ª—ñ–¥–∞—Ü—ñ—è –∫–æ–∂–Ω–æ–≥–æ –ø–æ–ª—è, —è–∫–µ —î –≤ –ø—Ä–∞–≤–∏–ª–∞—Ö
                for col_name, rule_type in validation_rules.items():
                    try:
                        col_index = header_map.get(col_name)
                        if col_index is None:
                            # –¶–µ –º–∞–ª–æ –±—É—Ç–∏ —Å–ø—ñ–π–º–∞–Ω–æ –Ω–∞ –µ—Ç–∞–ø—ñ 5, –∞–ª–µ —Ü–µ –¥–æ–¥–∞—Ç–∫–æ–≤–∞ –ø–µ—Ä–µ—Å—Ç—Ä–∞—Ö–æ–≤–∫–∞
                            continue
                        
                        if col_index >= len(row):
                            logging.error(f"‚ùå –†—è–¥–æ–∫ {row_number}: –†—è–¥–æ–∫ –∫–æ—Ä–æ—Ç—à–∏–π –∑–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –æ—á—ñ–∫—É–≤–∞–Ω–∏—Ö –∫–æ–ª–æ–Ω–æ–∫.")
                            return False
                        
                        value = row[col_index].strip()

                        # 7.0. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –æ–±–æ–≤‚Äô—è–∑–∫–æ–≤—ñ—Å—Ç—å –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è
                        if rule_type == "not_empty":
                            if not value:
                                logging.error(f"‚ùå –†—è–¥–æ–∫ {row_number}, –∫–æ–ª–æ–Ω–∫–∞ '{col_name}': –ø–æ–ª–µ –Ω–µ –ø–æ–≤–∏–Ω–Ω–æ –±—É—Ç–∏ –ø–æ—Ä–æ–∂–Ω—ñ–º.")
                                return False
                            continue  # –Ω–µ –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –¥–∞–ª—ñ
                        
                        # 7.1. –í–∞–ª—ñ–¥–∞—Ü—ñ—è —Ü—ñ–ª–∏—Ö —á–∏—Å–µ–ª
                        if rule_type == "integer":
                            if not value:
                                logging.error(f"‚ùå –†—è–¥–æ–∫ {row_number}, –∫–æ–ª–æ–Ω–∫–∞ '{col_name}': –æ—á—ñ–∫—É—î—Ç—å—Å—è —Ü—ñ–ª–µ —á–∏—Å–ª–æ, –∞–ª–µ –ø–æ–ª–µ –ø–æ—Ä–æ–∂–Ω—î.")
                                return False
                            if not value.lstrip('-').isdigit():
                                logging.error(f"‚ùå –†—è–¥–æ–∫ {row_number}, –∫–æ–ª–æ–Ω–∫–∞ '{col_name}': –æ—á—ñ–∫—É—î—Ç—å—Å—è —Ü—ñ–ª–µ —á–∏—Å–ª–æ, –∞–ª–µ –æ—Ç—Ä–∏–º–∞–Ω–æ '{value}'.")
                                return False

                        # 7.2. –í–∞–ª—ñ–¥–∞—Ü—ñ—è –∑–Ω–∞—á–µ–Ω—å –∑—ñ —Å–ø–∏—Å–∫—É
                        elif isinstance(rule_type, list):
                            if not value:
                                logging.error(f"‚ùå –†—è–¥–æ–∫ {row_number}, –∫–æ–ª–æ–Ω–∫–∞ '{col_name}': –æ—á—ñ–∫—É—î—Ç—å—Å—è –æ–¥–Ω–µ –∑—ñ –∑–Ω–∞—á–µ–Ω—å {rule_type}, –∞–ª–µ –ø–æ–ª–µ –ø–æ—Ä–æ–∂–Ω—î.")
                                return False
                            if value not in rule_type:
                                logging.error(f"‚ùå –†—è–¥–æ–∫ {row_number}, –∫–æ–ª–æ–Ω–∫–∞ '{col_name}': –æ—á—ñ–∫—É—î—Ç—å—Å—è –æ–¥–Ω–µ –∑—ñ –∑–Ω–∞—á–µ–Ω—å {rule_type}, –∞–ª–µ –æ—Ç—Ä–∏–º–∞–Ω–æ '{value}'.")
                                return False
                        
                        # 7.3. –í–∞–ª—ñ–¥–∞—Ü—ñ—è —Ñ–æ—Ä–º–∞—Ç—É –¥–∞—Ç–∏-—á–∞—Å—É
                        elif rule_type == "datetime":
                            if value:
                                try:
                                    datetime.strptime(value, "%Y-%m-%dT%H:%M:%S")
                                except ValueError:
                                    logging.error(f"‚ùå –†—è–¥–æ–∫ {row_number}, –∫–æ–ª–æ–Ω–∫–∞ '{col_name}': –Ω–µ–≤—ñ—Ä–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç–∏-—á–∞—Å—É. –û—á—ñ–∫—É—î—Ç—å—Å—è 'YYYY-MM-DDTHH:MM:SS', –∞–ª–µ –æ—Ç—Ä–∏–º–∞–Ω–æ '{value}'.")
                                    return False

                        # 7.4. –í–∞–ª—ñ–¥–∞—Ü—ñ—è —Ü—ñ–ª–∏—Ö —á–∏—Å–µ–ª (–¥–æ–ø—É—Å–∫–∞—î –ø–æ—Ä–æ–∂–Ω—î –ø–æ–ª–µ)
                        if rule_type == "integer_or_empty":
                            if value == "":
                                continue  # –ü–æ—Ä–æ–∂–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è ‚Äî –¥–æ–∑–≤–æ–ª–µ–Ω–µ
                            if not value.lstrip('-').isdigit():
                                logging.error(f"‚ùå –†—è–¥–æ–∫ {row_number}, –∫–æ–ª–æ–Ω–∫–∞ '{col_name}': –æ—á—ñ–∫—É—î—Ç—å—Å—è —Ü—ñ–ª–µ —á–∏—Å–ª–æ –∞–±–æ –ø–æ—Ä–æ–∂–Ω—î –ø–æ–ª–µ, –∞–ª–µ –æ—Ç—Ä–∏–º–∞–Ω–æ '{value}'.")
                                return False

                        # 7.5. –í–∞–ª—ñ–¥–∞—Ü—ñ—è —á–∏—Å–µ–ª –∑ –ø–ª–∞–≤–∞—é—á–æ—é –∫–æ–º–æ—é (float) (–¥–æ–ø—É—Å–∫–∞—î –ø–æ—Ä–æ–∂–Ω—î –ø–æ–ª–µ)
                        elif rule_type == "float_or_empty":
                            if value == "":
                                continue  # –¥–æ–∑–≤–æ–ª—è—î–º–æ –ø—É—Å—Ç–µ –ø–æ–ª–µ

                            # –î–æ–∑–≤–æ–ª—è—î–º–æ —î–≤—Ä–æ–ø–µ–π—Å—å–∫–∏–π —Ñ–æ—Ä–º–∞—Ç –∑ –∫–æ–º–æ—é ‚Äî –∑–∞–º—ñ–Ω—é—î–º–æ –Ω–∞ –∫—Ä–∞–ø–∫—É
                            normalized_value = value.replace(",", ".")
                            try:
                                float(normalized_value)
                            except ValueError:
                                logging.error(
                                    f"‚ùå –†—è–¥–æ–∫ {row_number}, –∫–æ–ª–æ–Ω–∫–∞ '{col_name}': –æ—á—ñ–∫—É—î—Ç—å—Å—è —á–∏—Å–ª–æ (float) –∞–±–æ –ø–æ—Ä–æ–∂–Ω—î –ø–æ–ª–µ, "
                                    f"–∞–ª–µ –æ—Ç—Ä–∏–º–∞–Ω–æ '{value}'."
                                )
                                return False
                                    
                    except (ValueError, IndexError):
                        logging.error(f"‚ùå –ù–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –≤ —Ä—è–¥–∫—É {row_number}. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∑—É–ø–∏–Ω–µ–Ω–∞.")
                        return False

    except Exception as e:
        logging.error(f"‚ùå –í–∏–Ω–∏–∫–ª–∞ –Ω–µ–≤—ñ–¥–æ–º–∞ –ø–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å —á–∏—Ç–∞–Ω–Ω—è CSV: {e}", exc_info=True)
        return False
        
    logging.info(f"‚úÖ –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ñ–∞–π–ª—É –ø—Ä–æ–π—à–ª–∞ —É—Å–ø—ñ—à–Ω–æ.")
    return True

# --- –†–û–ë–û–¢–ê –ó –§–ê–ô–õ–ê–ú–ò –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–á (attribute.csv, category.csv, poznachky.csv) ---
def get_config_path(filename):
    """–ü–æ–≤–µ—Ä—Ç–∞—î –ø–æ–≤–Ω–∏–π —à–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó."""
    # –ü—Ä–∏–ø—É—Å–∫–∞—î–º–æ, —â–æ config –∑–Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è –Ω–∞ –æ–¥–∏–Ω —Ä—ñ–≤–µ–Ω—å –≤–∏—â–µ –≤—ñ–¥ scr
    current_dir = os.path.dirname(os.path.abspath(__file__))
    config_dir = os.path.abspath(os.path.join(current_dir, '..', 'config'))
    return os.path.join(config_dir, filename)

def load_attributes_csv():
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –ø—Ä–∞–≤–∏–ª–∞ –∑–∞–º—ñ–Ω–∏ –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ –∑ attribute.csv (–≥—ñ–±—Ä–∏–¥–Ω–∞ –±–ª–æ—á–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞).
    –ü–æ–≤–µ—Ä—Ç–∞—î:
    1. replacements_map: –°–ª–æ–≤–Ω–∏–∫ {col_index: {original_value: new_value}} –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –ø–æ—à—É–∫—É.
    2. raw_data: –°–ø–∏—Å–æ–∫ —Å–∏—Ä–∏—Ö —Ä—è–¥–∫—ñ–≤ –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ —Ñ–∞–π–ª—É.
    """
    attribute_path = get_config_path('attribute.csv')
    replacements_map = {}
    raw_data = []          
    
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
    default_header = ["column_number", "attr_site_name", "atr_a", "atr_b", "atr_c", "atr_d", "atr_e", "atr_f", "atr_g", "atr_h", "atr_i"]
    current_col_index = None # –í—ñ–¥—Å—Ç–µ–∂—É—î–º–æ –ø–æ—Ç–æ—á–Ω–∏–π –±–ª–æ–∫

    try:
        with open(attribute_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            
            try:
                header = next(reader)
                raw_data.append(header)
                max_row_len = len(header)
            except StopIteration:
                return {}, [default_header]

            for row in reader:
                # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –¥–æ–≤–∂–∏–Ω—É —Ä—è–¥–∫–∞
                row = row[:max_row_len] + [''] * (max_row_len - len(row))
                raw_data.append(row)
                
                # 1. –Ø–∫—â–æ —Ü–µ —Ä—è–¥–æ–∫-–∑–∞–≥–æ–ª–æ–≤–æ–∫ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, "27",,,,,)
                if row and row[0].strip().isdigit():
                    try:
                        current_col_index = int(row[0].strip())
                        if current_col_index not in replacements_map:
                            replacements_map[current_col_index] = {}
                    except ValueError:
                        current_col_index = None
                        continue
                
                # 2. –Ø–∫—â–æ —Ü–µ —Ä—è–¥–æ–∫-–ø—Ä–∞–≤–∏–ª–æ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, ,,,—á–æ—Ä–Ω–∏–π,,)
                elif current_col_index is not None and len(row) >= 3:
                    
                    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–æ–≤–∞–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è –∑–Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è –≤ –∫–æ–ª–æ–Ω—Ü—ñ 1 (attr_site_name)
                    new_value = row[1].strip() 
                    
                    # –ü–µ—Ä–µ–≥–ª—è–¥–∞—î–º–æ –≤—Å—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫—ñ–≤ (–ø–æ—á–∏–Ω–∞—é—á–∏ –∑ —ñ–Ω–¥–µ–∫—Å—É 2)
                    for original in row[2:]:
                        original = original.strip().lower()
                        if original:
                            # –ö–ª—é—á - –æ—Ä–∏–≥—ñ–Ω–∞–ª (lower), –ó–Ω–∞—á–µ–Ω–Ω—è - –∑–∞–º—ñ–Ω–∞ (–∑ attr_site_name)
                            replacements_map[current_col_index][original] = new_value

        return replacements_map, raw_data
    
    except FileNotFoundError:
        logging.warning(f"–§–∞–π–ª –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ 'attribute.csv' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. –ë—É–¥–µ —Å—Ç–≤–æ—Ä–µ–Ω–æ –Ω–æ–≤–∏–π.")
        return {}, [default_header]
    except Exception as e:
        logging.error(f"–í–∏–Ω–∏–∫–ª–∞ –ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ attribute.csv: {e}")
        return {}, [default_header]

def save_attributes_csv(raw_data):
    """
    –ó–±–µ—Ä—ñ–≥–∞—î –æ–Ω–æ–≤–ª–µ–Ω—ñ —Å–∏—Ä—ñ –¥–∞–Ω—ñ —É attribute.csv.
    """
    attribute_path = get_config_path('attribute.csv')
    try:
        # 'newline=''' –≤–∞–∂–ª–∏–≤–∏–π –¥–ª—è –∫–æ—Ä–µ–∫—Ç–Ω–æ–≥–æ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è CSV –Ω–∞ —Ä—ñ–∑–Ω–∏—Ö –û–°
        with open(attribute_path, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerows(raw_data)
        logging.info("–§–∞–π–ª –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ attribute.csv –æ–Ω–æ–≤–ª–µ–Ω–æ.")
    except Exception as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—ñ —Ñ–∞–π–ª—É –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ attribute.csv: {e}")

def load_category_csv():
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –ø—Ä–∞–≤–∏–ª–∞ –∑–∞–º—ñ–Ω–∏ –∫–∞—Ç–µ–≥–æ—Ä—ñ–π –∑ category.csv.
    –ü–æ–≤–µ—Ä—Ç–∞—î:
    1. category_map: –°–ª–æ–≤–Ω–∏–∫ {supplier_id: {(name1, name2, name3): category_value}}
    2. raw_data: –°–ø–∏—Å–æ–∫ —Å–∏—Ä–∏—Ö —Ä—è–¥–∫—ñ–≤ –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ —Ñ–∞–π–ª—É.
    """
    category_path = get_config_path('category.csv')
    category_map = {}
    raw_data = []          
    
    default_header = ["postachalnyk", "name_1", "name_2", "name_3", "category"]
    current_supplier_id = None

    try:
        with open(category_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            
            try:
                header = next(reader)
                raw_data.append(header)
                max_row_len = len(header)
            except StopIteration:
                return {}, [default_header]

            for row in reader:
                row = row[:max_row_len] + [''] * (max_row_len - len(row))
                raw_data.append(row)
                
                # 1. –Ø–∫—â–æ —Ü–µ —Ä—è–¥–æ–∫-–∑–∞–≥–æ–ª–æ–≤–æ–∫ –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, "1",,,,)
                if row and row[0].strip().isdigit():
                    try:
                        current_supplier_id = int(row[0].strip())
                        if current_supplier_id not in category_map:
                            category_map[current_supplier_id] = {}
                    except ValueError:
                        current_supplier_id = None
                        continue
                
                # 2. –Ø–∫—â–æ —Ü–µ —Ä—è–¥–æ–∫-–ø—Ä–∞–≤–∏–ª–æ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, ,"–õ—è–ª—å–∫–∏","–ö—É–∫–ª–∏","–ù–∞–¥—É–≤–Ω—ñ","–ù–∞–¥—É–≤–Ω—ñ –ª—è–ª—å–∫–∏")
                elif current_supplier_id is not None and len(row) >= 5:
                    
                    # –ö–ª—é—á—ñ –¥–ª—è –º–∞–ø–∏: (name_1, name_2, name_3) - –≤—Å—ñ –≤ –Ω–∏–∂–Ω—å–æ–º—É —Ä–µ–≥—ñ—Å—Ç—Ä—ñ
                    key_tuple = (
                        row[1].strip().lower(), 
                        row[2].strip().lower(), 
                        row[3].strip().lower()
                    )
                    
                    # –ó–Ω–∞—á–µ–Ω–Ω—è: category (–±–µ–∑ –∑–º—ñ–Ω–∏ —Ä–µ–≥—ñ—Å—Ç—Ä—É)
                    category_value = row[4].strip()
                    
                    category_map[current_supplier_id][key_tuple] = category_value

        return category_map, raw_data
    
    except FileNotFoundError:
        logging.warning(f"–§–∞–π–ª –∫–∞—Ç–µ–≥–æ—Ä—ñ–π 'category.csv' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. –ë—É–¥–µ —Å—Ç–≤–æ—Ä–µ–Ω–æ –Ω–æ–≤–∏–π.")
        return {}, [default_header]
    except Exception as e:
        logging.error(f"–í–∏–Ω–∏–∫–ª–∞ –ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ category.csv: {e}")
        return {}, [default_header]

def save_category_csv(raw_data):
    """
    –ó–±–µ—Ä—ñ–≥–∞—î –æ–Ω–æ–≤–ª–µ–Ω—ñ —Å–∏—Ä—ñ –¥–∞–Ω—ñ —É category.csv.
    """
    category_path = get_config_path('category.csv')
    try:
        with open(category_path, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerows(raw_data)
        logging.info("–§–∞–π–ª –∫–∞—Ç–µ–≥–æ—Ä—ñ–π category.csv –æ–Ω–æ–≤–ª–µ–Ω–æ.")
    except Exception as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—ñ —Ñ–∞–π–ª—É –∫–∞—Ç–µ–≥–æ—Ä—ñ–π category.csv: {e}")

def load_poznachky_csv():
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Å—Ç–∞—Ç–∏—á–Ω–∏–π —Å–ø–∏—Å–æ–∫ –ø–æ–∑–Ω–∞—á–æ–∫ –∑ poznachky.csv.
    –ü–æ–≤–µ—Ä—Ç–∞—î:
    1. poznachky_list: –°–ø–∏—Å–æ–∫ —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –ø–æ–∑–Ω–∞—á–æ–∫ (—É –Ω–∏–∂–Ω—å–æ–º—É —Ä–µ–≥—ñ—Å—Ç—Ä—ñ).
    """
    poznachky_path = get_config_path('poznachky.csv')
    poznachky_list = []
    
    try:
        with open(poznachky_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            
            try:
                # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ "Poznachky"
                next(reader) 
            except StopIteration:
                return []

            for row in reader:
                if row and row[0].strip():
                    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –ø–æ–∑–Ω–∞—á–∫–∏ —É –Ω–∏–∂–Ω—å–æ–º—É —Ä–µ–≥—ñ—Å—Ç—Ä—ñ –¥–ª—è —É–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
                    poznachky_list.append(row[0].strip().lower())

        # –°–æ—Ä—Ç—É—î–º–æ –≤—ñ–¥ –Ω–∞–π–¥–æ–≤—à–∏—Ö –¥–æ –Ω–∞–π–∫–æ—Ä–æ—Ç—à–∏—Ö, —â–æ–± –∑–Ω–∞–π—Ç–∏ –Ω–∞–π–∫—Ä–∞—â–µ —Å–ø—ñ–≤–ø–∞–¥—ñ–Ω–Ω—è
        poznachky_list.sort(key=len, reverse=True)
        
        return poznachky_list
    
    except FileNotFoundError:
        logging.warning(f"–§–∞–π–ª –ø–æ–∑–Ω–∞—á–æ–∫ 'poznachky.csv' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
        return []
    except Exception as e:
        logging.error(f"–í–∏–Ω–∏–∫–ª–∞ –ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ poznachky.csv: {e}")
        return []

# --- –û–ë–†–û–ë–ö–ê –ó–û–ë–†–ê–ñ–ï–ù–¨ ---   
def clear_directory(folder_path: str):
    """–û—á–∏—â–∞—î –∞–±–æ —Å—Ç–≤–æ—Ä—é—î –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é."""
    if not os.path.exists(folder_path):
        os.makedirs(folder_path, exist_ok=True)
        return
    for item in os.listdir(folder_path):
        path = os.path.join(folder_path, item)
        try:
            if os.path.isfile(path) or os.path.islink(path):
                os.unlink(path)
            elif os.path.isdir(path):
                shutil.rmtree(path)
        except Exception as e:
            logging.error(f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –≤–∏–¥–∞–ª–∏—Ç–∏ {path}: {e}")

def move_gifs(src: str, dest: str) -> int:
    """–ü–µ—Ä–µ–º—ñ—â—É—î –≤—Å—ñ GIF —ñ–∑ src —É dest."""
    moved = 0
    os.makedirs(dest, exist_ok=True)
    for root, _, files in os.walk(src):
        for f in files:
            if f.lower().endswith('.gif'):
                src_path = os.path.join(root, f)
                rel = os.path.relpath(src_path, src)
                dest_path = os.path.join(dest, rel)
                os.makedirs(os.path.dirname(dest_path), exist_ok=True)
                shutil.move(src_path, dest_path)
                moved += 1
    logging.info(f"üü£ –ü–µ—Ä–µ–º—ñ—â–µ–Ω–æ {moved} GIF-—Ñ–∞–π–ª—ñ–≤.")
    return moved

def convert_to_webp_square(src: str, dest: str) -> int:
    """
    –ö–æ–Ω–≤–µ—Ä—Ç—É—î JPG/PNG ‚Üí WEBP, –≤–∏—Ä—ñ–≤–Ω—é—î –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –¥–æ –∫–≤–∞–¥—Ä–∞—Ç—É
    —Ç–∞ –∫–æ—Ä–µ–∫—Ç–Ω–æ –æ–±—Ä–æ–±–ª—è—î –ø—Ä–æ–∑–æ—Ä—ñ—Å—Ç—å (RGBA / –ø–∞–ª—ñ—Ç—Ä–æ–≤—ñ P-–∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è).
    """
    import os
    import logging
    from PIL import Image

    converted = 0

    for root, _, files in os.walk(src):
        rel = os.path.relpath(root, src)
        out_dir = os.path.join(dest, rel)
        os.makedirs(out_dir, exist_ok=True)

        for f in files:
            if not f.lower().endswith(('.jpg', '.jpeg', '.png', '.webp')):
                continue

            try:
                img_path = os.path.join(root, f)
                img = Image.open(img_path)

                # üîπ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è –∫–æ–ª—å–æ—Ä–æ–≤–æ–≥–æ —Ä–µ–∂–∏–º—É (–¥–ª—è —É–Ω–∏–∫–Ω–µ–Ω–Ω—è warning)
                if img.mode == "P":
                    img = img.convert("RGBA")
                elif img.mode not in ("RGB", "RGBA"):
                    img = img.convert("RGB")

                w, h = img.size
                max_side = max(w, h)

                # üîπ –Ø–∫—â–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –º–∞—î –∞–ª—å—Ñ–∞-–∫–∞–Ω–∞–ª ‚Äî —Å—Ç–≤–æ—Ä—é—î–º–æ –ø—Ä–æ–∑–æ—Ä–µ –ø–æ–ª–æ—Ç–Ω–æ
                if img.mode == "RGBA":
                    canvas = Image.new("RGBA", (max_side, max_side), (255, 255, 255, 0))
                else:
                    canvas = Image.new("RGB", (max_side, max_side), (255, 255, 255))

                # –¶–µ–Ω—Ç—Ä—É–≤–∞–Ω–Ω—è
                canvas.paste(img, ((max_side - w) // 2, (max_side - h) // 2))

                # üîπ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —É —Ñ–æ—Ä–º–∞—Ç—ñ WEBP
                new_name = os.path.splitext(f)[0] + '.webp'
                out_path = os.path.join(out_dir, new_name)
                canvas.save(out_path, 'webp', quality=90)

                converted += 1

            except Exception as e:
                logging.error(f"‚ùå WEBP-–∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è '{f}' –Ω–µ –≤–¥–∞–ª–∞—Å—è: {e}")

    logging.info(f"üü¢ WEBP-–∫–æ–Ω–≤–µ—Ä—Ç–æ–≤–∞–Ω–æ {converted} –∑–æ–±—Ä–∞–∂–µ–Ω—å.")
    return converted

# --- –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø –ó–û–ë–†–ê–ñ–ï–ù–¨ –¢–û–í–ê–†–£ ---
def download_product_images(url: str, sku: str, category: str, base_path: str, cat_map: Dict[str, str]) -> List[str]:
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –≤—Å—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Ç–æ–≤–∞—Ä—É –∑ URL."""
    cat_slug = cat_map.get(category.strip()) or category.strip().lower().replace(' ', '_').replace(',', '')
    dest = os.path.join(base_path, cat_slug)
    os.makedirs(dest, exist_ok=True)

    try:
        page = requests.get(url, timeout=10)
        page.raise_for_status()
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Å—Ç–æ—Ä—ñ–Ω–∫—É {url}: {e}")
        return []

    soup = BeautifulSoup(page.content, 'html.parser')
    links = {a.get('href') for a in soup.find_all('a', class_='thumb_image_container') if a.get('href')}
    files = []

    for i, img_url in enumerate(links, 1):
        try:
            r = requests.get(img_url, timeout=10)
            r.raise_for_status()
            mime = r.headers.get('Content-Type')
            ext = mimetypes.guess_extension(mime) or '.jpg'
            fname = f"{sku}-{i}{ext}"
            with open(os.path.join(dest, fname), 'wb') as f:
                f.write(r.content)
            files.append(fname)
        except Exception:
            continue

    # üü¢ –õ–æ–≥—É–≤–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
    logging.info(f"üì∏ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(files)} –∑–æ–±—Ä–∞–∂–µ–Ω—å –¥–ª—è SKU {sku}: {', '.join(files)}")

    return files

# --- –°–ò–ù–•–†–û–ù–Ü–ó–ê–¶–Ü–Ø –ö–û–õ–û–ù–ö–ò WEBP –£ CSV ---
def sync_webp_column(sl_path: str, webp_path: str, col_index: int, sku_index: int) -> int:
    """–û–Ω–æ–≤–ª—é—î –∫–æ–ª–æ–Ω–∫—É WEBP/GIF-—Å–ø–∏—Å–∫—ñ–≤ —É CSV."""
    with open(sl_path, 'r', encoding='utf-8') as f:
        reader = list(csv.reader(f))
    if not reader:
        return 0

    header, *rows = reader
    sku_map = {}
    for root, _, files in os.walk(webp_path):
        for f in files:
            if '-' in f and f.lower().endswith(('.webp', '.gif')):
                sku = f.split('-')[0]
                sku_map.setdefault(sku, []).append(f)

    updated = 0
    for row in rows:
        if len(row) <= max(col_index, sku_index):
            row.extend([''] * (max(col_index, sku_index) + 1 - len(row)))
        sku = row[sku_index].strip()
        if sku in sku_map:
            row[col_index] = ', '.join(sorted(sku_map[sku]))
            updated += 1
    with open(sl_path, 'w', encoding='utf-8', newline='') as f:
        csv.writer(f).writerows([header] + rows)
    logging.info(f"üîÅ –û–Ω–æ–≤–ª–µ–Ω–æ {updated} SKU —É –∫–æ–ª–æ–Ω—Ü—ñ WEBP.")
    return updated

# --- –ö–û–ü–Ü–Æ–í–ê–ù–ù–Ø –§–ê–ô–õ–Ü–í –£ –§–Ü–ù–ê–õ–¨–ù–£ –î–ò–†–ï–ö–¢–û–†–Ü–Æ ---
def copy_to_site(src: str, dest: str):
    """–ö–æ–ø—ñ—é—î WEBP/GIF –¥–æ —Ñ—ñ–Ω–∞–ª—å–Ω–æ—ó –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó –∑ –ø—Ä–∞–≤–∞–º–∏."""
    uid, gid = 33, 33
    fperm, dperm = 0o644, 0o755
    copied = 0

    for root, _, files in os.walk(src):
        rel = os.path.relpath(root, src)
        out_dir = os.path.join(dest, rel)
        os.makedirs(out_dir, mode=dperm, exist_ok=True)
        for f in files:
            if not f.lower().endswith(('.webp', '.gif')):
                continue
            src_f = os.path.join(root, f)
            dst_f = os.path.join(out_dir, f)
            shutil.copy2(src_f, dst_f)
            try:
                os.chown(dst_f, uid, gid)
                os.chmod(dst_f, fperm)
                copied += 1
            except PermissionError:
                logging.warning(f"‚ö†Ô∏è –ù–µ–º–∞—î –ø—Ä–∞–≤ –¥–ª—è –∑–º—ñ–Ω–∏ –≤–ª–∞—Å–Ω–∏–∫–∞ {dst_f}")
    logging.info(f"üì¶ –°–∫–æ–ø—ñ–π–æ–≤–∞–Ω–æ {copied} —Ñ–∞–π–ª—ñ–≤ —É {dest}.")
    return copied

# --- –î–û–ü–û–ú–Ü–ñ–ù–ê –§–£–ù–ö–¶–Ü–Ø –î–õ–Ø –ü–ê–ö–ï–¢–ù–û–ì–û –ó–ê–ü–ò–°–£ (–û–Ω–æ–≤–ª–µ–Ω–Ω—è) ---
def _process_batch_update(wcapi: Any, batch_data: List[Dict[str, Any]], errors_list: List[str]) -> int:
    """–í–∏–∫–æ–Ω—É—î –ø–∞–∫–µ—Ç–Ω–∏–π –∑–∞–ø–∏—Ç 'update' –¥–æ WooCommerce API."""
    
    payload = {"update": batch_data}
    
    try:
        logging.info(f"–ù–∞–¥—Å–∏–ª–∞—é –ø–∞–∫–µ—Ç –Ω–∞ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è ({len(batch_data)} —Ç–æ–≤–∞—Ä—ñ–≤)...")
        
        response = wcapi.post("products/batch", data=payload) 
        
        if response.status_code == 200:
            result = response.json()
            updated_count = len(result.get('update', []))
            
            # –î–µ—Ç–∞–ª—å–Ω–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è –ø–æ–º–∏–ª–æ–∫, —è–∫—ñ –ø–æ–≤–µ—Ä–Ω—É–≤ API
            api_errors = result.get('errors', [])
            if api_errors:
                for err in api_errors:
                    err_msg = f"API-–ü–æ–º–∏–ª–∫–∞ (ID: {err.get('id', 'N/A')}): {err.get('message', '–ù–µ–≤—ñ–¥–æ–º–∞ –ø–æ–º–∏–ª–∫–∞')}"
                    errors_list.append(err_msg)
                    logging.error(err_msg)
                
            logging.info(f"‚úÖ –ü–∞–∫–µ—Ç –æ–Ω–æ–≤–ª–µ–Ω–æ. –£—Å–ø—ñ—à–Ω–æ –æ–±—Ä–æ–±–ª–µ–Ω–æ API: {updated_count} —Ç–æ–≤–∞—Ä—ñ–≤. –ü–æ–º–∏–ª–æ–∫ —É –ø–∞–∫–µ—Ç—ñ: {len(api_errors)}")
            return updated_count
        else:
            err_msg = f"‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ API ({response.status_code}) –ø—Ä–∏ –ø–∞–∫–µ—Ç–Ω–æ–º—É –æ–Ω–æ–≤–ª–µ–Ω–Ω—ñ. –ü–æ–º–∏–ª–∫–∞: {response.text[:200]}..."
            errors_list.append(err_msg)
            logging.critical(err_msg)
            return 0
            
    except Exception as e:
        err_msg = f"‚ùå –ù–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –≤—ñ–¥–ø—Ä–∞–≤–∫–∏ –ø–∞–∫–µ—Ç—É: {e}"
        errors_list.append(err_msg)
        logging.critical(err_msg, exc_info=True)
        return 0

# --- –ì–ª–æ–±–∞–ª—å–Ω–∞ HTTPS-—Å–µ—Å—ñ—è —Ç–∞ –∫–µ—à ---
_session = requests.Session()
_media_cache: Dict[str, int] = {}  # slug -> id

def _get_media_id_by_filename_sql(db_conf, filename):
    """
    –ú–∏—Ç—Ç—î–≤–∏–π –ø–æ—à—É–∫ ID –º–µ–¥—ñ–∞—Ñ–∞–π–ª—É –Ω–∞–ø—Ä—è–º—É —É –±–∞–∑—ñ WordPress.
    –ü—Ä–∞—Ü—é—î —É 50-100 —Ä–∞–∑—ñ–≤ —à–≤–∏–¥—à–µ –∑–∞ REST.
    """
    try:
        clean_name = re.sub(r'-\d+x\d+(?=\.)', '', filename)
        file_slug = os.path.splitext(clean_name)[0]

        conn = pymysql.connect(
            host=db_conf["host"],
            user=db_conf["user"],
            password=db_conf["password"],
            database=db_conf["database"],
            charset="utf8mb4",
            cursorclass=pymysql.cursors.DictCursor
        )
        with conn.cursor() as cursor:
            sql = """
            SELECT ID 
            FROM wp_posts 
            WHERE post_name=%s AND post_type='attachment' 
            LIMIT 1;
            """
            cursor.execute(sql, (file_slug,))
            result = cursor.fetchone()
            conn.close()

            if result:
                media_id = result["ID"]
                logging.debug(f"‚úÖ SQL-–∑–Ω–∞–π–¥–µ–Ω–æ –º–µ–¥—ñ–∞ '{file_slug}' ‚Üí ID: {media_id}")
                return media_id
            else:
                logging.warning(f"‚ö†Ô∏è SQL: –º–µ–¥—ñ–∞ '{file_slug}' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
                return None
    except Exception as e:
        logging.error(f"‚ùå SQL-–ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –ø–æ—à—É–∫—É –º–µ–¥—ñ–∞ '{filename}': {e}", exc_info=True)
        return None

# --- –î–û–ü–û–ú–Ü–ñ–ù–ê –§–£–ù–ö–¶–Ü–Ø –î–õ–Ø –ü–æ—à—É–∫—É Media IDs ---
def find_media_ids_for_sku(wcapi, sku: str, uploads_path: str) -> List[Dict[str, Any]]:
    """
    –ó–Ω–∞—Ö–æ–¥–∏—Ç—å —É—Å—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –¥–ª—è SKU —É uploads_path —Ç–∞ –ø–æ–≤–µ—Ä—Ç–∞—î —Å–ø–∏—Å–æ–∫ ID –¥–ª—è WooCommerce.
    - –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î persistent HTTPS-—Å–µ—Å—ñ—é (_session)
    - –ú–∞—î –∫–µ—à –¥–ª—è –≤–∂–µ –∑–Ω–∞–π–¥–µ–Ω–∏—Ö slug
    - –Ü–≥–Ω–æ—Ä—É—î —Ç–µ—Ö–Ω—ñ—á–Ω—ñ –∫–æ–ø—ñ—ó (-150x150, -300x300, ...)
    - GIF —Å—Ç–∞–≤–∏—Ç—å –æ—Å—Ç–∞–Ω–Ω—ñ–º
    """
    # --- –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –∫–æ–Ω—Ñ—ñ–≥ ---
    settings = load_settings()
    if not settings:
        logging.error("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ settings.json ‚Äî –ø–æ—à—É–∫ –∑–æ–±—Ä–∞–∂–µ–Ω—å –ø—Ä–æ–ø—É—â–µ–Ω–æ.")
        return []

    db_conf = settings.get("db", {})
    if not db_conf:
        logging.error("‚ùå –£ settings.json –≤—ñ–¥—Å—É—Ç–Ω—ñ–π –±–ª–æ–∫ 'db'.")
        return []


    def _get_media_id_by_filename(wcapi, filename: str) -> int | None:
        """
        –ü–æ—à—É–∫ ID –º–µ–¥—ñ–∞—Ñ–∞–π–ª—É —É WordPress —á–µ—Ä–µ–∑ REST API /wp/v2/media.
        –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –ø—É–±–ª—ñ—á–Ω–∏–π –∑–∞–ø–∏—Ç, –ø—Ä–∏ 401/403 ‚Äî –ø–æ–≤—Ç–æ—Ä –∑ –∫–ª—é—á–∞–º–∏ WooCommerce.
        –ú–∞—î –∫–µ—à—É–≤–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ —ñ rate-limit 0.1 —Å.
        """
        try:
            # 1Ô∏è‚É£ –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ —ñ–º‚Äô—è
            clean_name = re.sub(r'-\d+x\d+(?=\.)', '', filename)
            file_slug = os.path.splitext(clean_name)[0]

            # 2Ô∏è‚É£ –Ø–∫—â–æ –≤–∂–µ –≤ –∫–µ—à—ñ ‚Äî –Ω–µ –∑–∞–ø–∏—Ç—É—î–º–æ
            if file_slug in _media_cache:
                logging.debug(f"‚ôªÔ∏è –ö–µ—à–æ–≤–∞–Ω–æ –º–µ–¥—ñ–∞ '{file_slug}' ‚Üí ID: {_media_cache[file_slug]}")
                return _media_cache[file_slug]

            wp_media_url = f"{wcapi.url.rstrip('/')}/wp-json/wp/v2/media"
            params = {"search": file_slug, "per_page": 5}

            # 3Ô∏è‚É£ –ü—Ä–æ–±—É—î–º–æ –±–µ–∑ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü—ñ—ó
            response = _session.get(wp_media_url, params=params, timeout=15)

            # 4Ô∏è‚É£ –Ø–∫—â–æ –∑–∞–∫—Ä–∏—Ç–æ ‚Äî –ø–æ–≤—Ç–æ—Ä –∑ WooCommerce –∫–ª—é—á–∞–º–∏
            if response.status_code in (401, 403):
                logging.debug(f"üîí REST API –ø–æ—Ç—Ä–µ–±—É—î auth –¥–ª—è '{file_slug}', –ø—Ä–æ–±—É—é –∑ –∫–ª—é—á–∞–º–∏ WooCommerce...")
                response = _session.get(
                    wp_media_url,
                    params=params,
                    auth=(wcapi.consumer_key, wcapi.consumer_secret),
                    timeout=15
                )

            # 5Ô∏è‚É£ –û–±—Ä–æ–±–ª—è—î–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å
            if response.status_code == 200:
                data = response.json()
                if isinstance(data, list) and data:
                    # –ü–æ—à—É–∫ —Ç–æ—á–Ω–æ–≥–æ –∑–±—ñ–≥—É slug
                    exact_match = next((item for item in data if item.get("slug", "").lower() == file_slug.lower()), None)
                    item = exact_match or data[0]

                    media_id = item.get("id")
                    media_slug = item.get("slug", "")
                    media_url = item.get("source_url", "")
                    media_title = item.get("title", {}).get("rendered", "")

                    if media_slug.lower() != file_slug.lower():
                        logging.warning(f"‚ö†Ô∏è '{file_slug}' –∑–Ω–∞–π–¥–µ–Ω–æ –Ω–µ—Ç–æ—á–Ω–æ: –ø–æ–≤–µ—Ä–Ω–µ–Ω–æ '{media_slug}' (ID {media_id})")
                    else:
                        logging.debug(f"‚úÖ –¢–æ—á–Ω–∏–π –∑–±—ñ–≥ '{file_slug}' ‚Üí ID: {media_id} | –ù–∞–∑–≤–∞: {media_title} | URL: {media_url}")

                    _media_cache[file_slug] = media_id
                    time.sleep(0.1)  # –Ω–µ–≤–µ–ª–∏–∫–∞ –ø–∞—É–∑–∞, —â–æ–± –Ω–µ –ø–µ—Ä–µ–≤–∞–Ω—Ç–∞–∂—É–≤–∞—Ç–∏ WP
                    return media_id
                else:
                    logging.warning(f"‚ö†Ô∏è –ú–µ–¥—ñ–∞ '{file_slug}' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ —É WP –º–µ–¥—ñ–∞—Ç–µ—Ü—ñ.")
            else:
                logging.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ {response.status_code} –ø—Ä–∏ –ø–æ—à—É–∫—É –º–µ–¥—ñ–∞ '{file_slug}'.")
        except Exception as e:
            logging.error(f"‚ùå –í–∏–Ω—è—Ç–æ–∫ –ø—Ä–∏ –ø–æ—à—É–∫—É –º–µ–¥—ñ–∞ '{filename}': {e}", exc_info=True)
        return None

    # --- –û—Å–Ω–æ–≤–Ω–∞ –ª–æ–≥—ñ–∫–∞ ---
    pattern = os.path.join(uploads_path, '**', f'{sku}*.*')
    files = glob.glob(pattern, recursive=True)

    # –£–Ω—ñ–∫–∞–ª—å–Ω—ñ base-—ñ–º–µ–Ω–∞ –±–µ–∑ —Å—É—Ñ—ñ–∫—Å—ñ–≤
    unique_files = {re.sub(r'-\d+x\d+(?=\.)', '', os.path.basename(p)) for p in files}

    # GIF ‚Äî –æ—Å—Ç–∞–Ω–Ω—ñ–º
    sorted_files = sorted(unique_files, key=lambda f: (f.lower().endswith('.gif'), f))

    media_ids = []
    for filename in sorted_files:
        media_id = _get_media_id_by_filename_sql(settings["db"], filename)
        if media_id:
            media_ids.append({"id": media_id})

    if not media_ids:
        logging.warning(f"‚ö†Ô∏è SKU {sku}: –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∑–æ–±—Ä–∞–∂–µ–Ω—å —É '{uploads_path}'")
    else:
        logging.info(f"üñºÔ∏è SKU {sku}: –î–æ–¥–∞–Ω–æ {len(media_ids)} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –∑–æ–±—Ä–∞–∂–µ–Ω—å.")

    return media_ids

# --- –î–û–ü–û–ú–Ü–ñ–ù–ê –§–£–ù–ö–¶–Ü–Ø –î–õ–Ø –ü–ê–ö–ï–¢–ù–û–ì–û –ó–ê–ü–ò–°–£ (–°—Ç–≤–æ—Ä–µ–Ω–Ω—è) ---
def _process_batch_create(wcapi: Any, batch_data: List[Dict[str, Any]], errors_list: List[str]) -> int:
    """–í–∏–∫–æ–Ω—É—î –ø–∞–∫–µ—Ç–Ω–∏–π –∑–∞–ø–∏—Ç 'create' –¥–æ WooCommerce API."""
    
    payload = {"create": batch_data}
    
    try:
        logging.info(f"–ù–∞–¥—Å–∏–ª–∞—é –ø–∞–∫–µ—Ç –Ω–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è ({len(batch_data)} —Ç–æ–≤–∞—Ä—ñ–≤)...")
        response = wcapi.post("products/batch", data=payload) 
        
        if response.status_code == 200:
            result = response.json()
            created_count = len(result.get('create', []))
            
            api_errors = result.get('errors', [])
            if api_errors:
                for err in api_errors:
                    err_msg = f"API-–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—ñ: SKU '{err.get('data', {}).get('resource_id', 'N/A')}', Code: {err.get('code', 'N/A')}: {err.get('message', '–ù–µ–≤—ñ–¥–æ–º–∞ –ø–æ–º–∏–ª–∫–∞')}"
                    errors_list.append(err_msg)
                    logging.error(err_msg)
                
            logging.info(f"‚úÖ –ü–∞–∫–µ—Ç —Å—Ç–≤–æ—Ä–µ–Ω–æ. –£—Å–ø—ñ—à–Ω–æ –æ–±—Ä–æ–±–ª–µ–Ω–æ API: {created_count} —Ç–æ–≤–∞—Ä—ñ–≤. –ü–æ–º–∏–ª–æ–∫ —É –ø–∞–∫–µ—Ç—ñ: {len(api_errors)}")
            return created_count
        else:
            err_msg = f"‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ API ({response.status_code}) –ø—Ä–∏ –ø–∞–∫–µ—Ç–Ω–æ–º—É —Å—Ç–≤–æ—Ä–µ–Ω–Ω—ñ. –ü–æ–º–∏–ª–∫–∞: {response.text[:200]}..."
            errors_list.append(err_msg)
            logging.critical(err_msg)
            return 0
            
    except Exception as e:
        err_msg = f"‚ùå –ù–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –≤—ñ–¥–ø—Ä–∞–≤–∫–∏ –ø–∞–∫–µ—Ç—É: {e}"
        errors_list.append(err_msg)
        logging.critical(err_msg, exc_info=True)
        return 0

def _clean_text(value: str) -> str:
    """–û—á–∏—â—É—î HTML-—Ç–µ–≥–∏, –∫–æ–¥—É–≤–∞–Ω–Ω—è —ñ –∑–∞–π–≤—ñ –ø—Ä–æ–±—ñ–ª–∏."""
    if not value:
        return ""
    value = html.unescape(str(value))  # —Ä–æ–∑–∫–æ–¥–æ–≤—É—î &#8211; ‚Üí ‚Äì
    value = re.sub(r"<.*?>", "", value)  # –≤–∏–¥–∞–ª—è—î HTML-—Ç–µ–≥–∏
    return value.strip()

# --- –ï–ö–°–ü–û–†–¢ –¢–û–í–ê–†–£ –ó–ê ID ---
def export_product_by_id():
    """
    –ï–∫—Å–ø–æ—Ä—Ç—É—î –≤—Å—ñ –¥–∞–Ω—ñ —Ç–æ–≤–∞—Ä—É –∑–∞ –≤–≤–µ–¥–µ–Ω–∏–º ID —É /csv/input/ID_tovar.csv.
    –í–∏–ø—Ä–∞–≤–ª–µ–Ω–æ HTML-–∫–æ–¥—É–≤–∞–Ω–Ω—è, –µ–∫—Ä–∞–Ω—É–≤–∞–Ω–Ω—è CSV —ñ –¥–æ–¥–∞–Ω–æ –ø–µ—Ä–µ–∫–ª–∞–¥–∏ WPML.
    """
    log_message_to_existing_file()
    settings = load_settings()
    if not settings:
        logging.error("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è.")
        return

    wcapi = get_wc_api(settings)
    if not wcapi:
        logging.error("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è —Å—Ç–≤–æ—Ä–∏—Ç–∏ –æ–±'—î–∫—Ç WooCommerce API.")
        return

    product_id = input("–í–≤–µ–¥—ñ—Ç—å ID —Ç–æ–≤–∞—Ä—É –¥–ª—è –µ–∫—Å–ø–æ—Ä—Ç—É: ").strip()
    if not product_id.isdigit():
        logging.error("‚ùå –ù–µ–∫–æ—Ä–µ–∫—Ç–Ω–∏–π ID —Ç–æ–≤–∞—Ä—É.")
        return
    product_id = int(product_id)

    output_dir = "/var/www/scripts/update/csv/input"
    os.makedirs(output_dir, exist_ok=True)
    csv_path = os.path.join(output_dir, "ID_tovar.csv")

    start_time = time.time()
    try:
        # === –û—Å–Ω–æ–≤–Ω—ñ –¥–∞–Ω—ñ —Ç–æ–≤–∞—Ä—É ===
        response = wcapi.get(f"products/{product_id}", params={"context": "edit"})
        if response.status_code != 200:
            logging.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ {response.status_code}: {response.text}")
            return
        product = response.json()
        if not isinstance(product, dict):
            logging.error(f"‚ùå –ù–µ–∫–æ—Ä–µ–∫—Ç–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ API –¥–ª—è —Ç–æ–≤–∞—Ä—É ID {product_id}")
            return

        row = {"id": product_id}

        # === –û—Å–Ω–æ–≤–Ω—ñ –ø–æ–ª—è ===
        for key, value in product.items():
            if isinstance(value, dict):
                for subkey, subval in value.items():
                    row[f"{key}.{subkey}"] = _clean_text(subval)
            elif isinstance(value, list):
                if key == "meta_data":
                    for meta in value:
                        k = meta.get("key")
                        v = meta.get("value")
                        if k:
                            row[f"–ú–µ—Ç–∞: {k}"] = _clean_text(v)
                elif key == "categories":
                    row["categories"] = ", ".join([_clean_text(v.get("name", "")) for v in value])
                elif key == "tags":
                    row["tags"] = ", ".join([_clean_text(v.get("name", "")) for v in value])
                elif key == "images":
                    for idx, img in enumerate(value, start=1):
                        row[f"image_{idx}_id"] = img.get("id", "")
                        row[f"image_{idx}_src"] = img.get("src", "")
                        row[f"image_{idx}_name"] = _clean_text(img.get("name", ""))
                        row[f"image_{idx}_alt"] = _clean_text(img.get("alt", ""))
                        row[f"image_{idx}_title"] = _clean_text(img.get("title", ""))
                        row[f"image_{idx}_caption"] = _clean_text(img.get("caption", ""))
                        row[f"image_{idx}_description"] = _clean_text(img.get("description", ""))
                else:
                    row[key] = ", ".join(map(_clean_text, map(str, value)))
            else:
                row[key] = _clean_text(value)

        # === –ü–µ—Ä–µ–∫–ª–∞–¥–∏ WPML ===
        try:
            wpml_resp = wcapi.get(f"products/{product_id}/translations")
            if wpml_resp.status_code == 200:
                translations = wpml_resp.json()
                for lang, tr in translations.items():
                    if isinstance(tr, dict):
                        row[f"wpml_{lang}_id"] = tr.get("id", "")
                        row[f"wpml_{lang}_name"] = _clean_text(tr.get("name", ""))
                        row[f"wpml_{lang}_slug"] = tr.get("slug", "")
                        row[f"wpml_{lang}_status"] = tr.get("status", "")
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è –ù–µ–º–æ–∂–ª–∏–≤–æ –æ—Ç—Ä–∏–º–∞—Ç–∏ WPML –ø–µ—Ä–µ–∫–ª–∞–¥–∏: {e}")

        # === –ó–∞–ø–∏—Å —É CSV ===
        file_exists = os.path.exists(csv_path)
        file_is_empty = not file_exists or os.path.getsize(csv_path) == 0

        with open(csv_path, "a", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=row.keys(), quoting=csv.QUOTE_ALL)
            if file_is_empty:
                writer.writeheader()
            writer.writerow(row)

        elapsed = int(time.time() - start_time)
        logging.info(f"‚úÖ –ï–∫—Å–ø–æ—Ä—Ç–æ–≤–∞–Ω–æ —Ç–æ–≤–∞—Ä ID {product_id} ({len(row)} –ø–æ–ª—ñ–≤) ‚Üí {csv_path} –∑–∞ {elapsed} —Å–µ–∫.")

    except Exception as e:
        logging.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –µ–∫—Å–ø–æ—Ä—Ç—É: {e}", exc_info=True)

# --- –û–ù–û–í–õ–ï–ù–ù–Ø SEO-–ê–¢–†–ò–ë–£–¢–Ü–í –ó–û–ë–†–ê–ñ–ï–ù–¨ ---
def update_image_seo_by_sku():
    """
    –û–Ω–æ–≤–ª—é—î SEO-–∞—Ç—Ä–∏–±—É—Ç–∏ –∑–æ–±—Ä–∞–∂–µ–Ω—å —Ç–æ–≤–∞—Ä—É –∑–∞ SKU.
    - –û–Ω–æ–≤–ª—é—î alt/title —á–µ—Ä–µ–∑ WooCommerce (wc/v3 products PUT).
    - –û–Ω–æ–≤–ª—é—î caption/description —á–µ—Ä–µ–∑ WP REST API (wp/v2/media/{id}) –∑ Basic Auth,
      –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ credentials –∑ settings.json: 'login' —ñ 'pass'.
    """
    logging.basicConfig(level=logging.INFO)
    print("üñºÔ∏è –ó–∞–ø—É—Å–∫–∞—é –æ–Ω–æ–≤–ª–µ–Ω–Ω—è SEO-–∞—Ç—Ä–∏–±—É—Ç—ñ–≤ –∑–æ–±—Ä–∞–∂–µ–Ω—å...")

    settings = load_settings()
    if not settings:
        logging.critical("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è.")
        return

    # --- –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è WooCommerce API (wc/v3) ---
    try:
        wcapi = get_wc_api(settings)
    except Exception as e:
        logging.critical(f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è —Å—Ç–≤–æ—Ä–∏—Ç–∏ –æ–±'—î–∫—Ç WooCommerce API: {e}")
        return

    base_url = settings.get("url", "").rstrip("/")
    api_key = settings.get("consumer_key")
    api_secret = settings.get("consumer_secret")

    wp_login = settings.get("login")   # username –∞–±–æ –ª–æ–≥—ñ–Ω
    wp_pass = settings.get("pass")     # application password –∞–±–æ –ø–∞—Ä–æ–ª—å

    if not base_url or not api_key or not api_secret:
        logging.critical("‚ùå –ù–µ–ø–æ–≤–Ω—ñ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è API (url, consumer_key –∞–±–æ consumer_secret).")
        return

    # 1) –í–≤–µ–¥–µ–Ω–Ω—è SKU
    sku = input("üîç –í–≤–µ–¥—ñ—Ç—å SKU —Ç–æ–≤–∞—Ä—É: ").strip()
    if not sku:
        print("‚ùå SKU –Ω–µ –≤–≤–µ–¥–µ–Ω–æ.")
        return

    # 2) –û—Ç—Ä–∏–º—É—î–º–æ —Ç–æ–≤–∞—Ä –ø–æ SKU
    try:
        resp = wcapi.get("products", params={"sku": sku})
        if resp.status_code != 200:
            logging.error(f"‚ùå WooCommerce products GET returned {resp.status_code}: {resp.text[:200]}")
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –ø–æ—à—É–∫—É —Ç–æ–≤–∞—Ä—É (—Å—Ç–∞—Ç—É—Å {resp.status_code}). –ü–µ—Ä–µ–≤—ñ—Ä –ª–æ–≥–∏.")
            return
        products = resp.json()
        if not products:
            print(f"‚ùå –¢–æ–≤–∞—Ä –∑—ñ SKU {sku} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
            return
        product = products[0]
    except Exception as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Ç—ñ –¥–æ WooCommerce: {e}", exc_info=True)
        print("‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑'—î–¥–Ω–∞–Ω–Ω—ñ –∑ WooCommerce.")
        return

    product_name = product.get("name", "").strip()
    product_id = product.get("id")
    image_list: List[Dict[str, Any]] = product.get("images", [])  # —Å–ø–∏—Å–æ–∫ dict –∑ keys: id, src, name, alt

    if not product_name:
        print("‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –Ω–∞–∑–≤–∏ —Ç–æ–≤–∞—Ä—É.")
        return

    if not image_list:
        print(f"‚ùå –¢–æ–≤–∞—Ä {product_name} –Ω–µ –º–∞—î –ø—Ä–∏–≤'—è–∑–∞–Ω–∏—Ö –∑–æ–±—Ä–∞–∂–µ–Ω—å —É –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ WC API.")
        return

    print(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä: {product_name}")
    print(f"üñºÔ∏è –ó–Ω–∞–π–¥–µ–Ω–æ {len(image_list)} –ø—Ä–∏–≤'—è–∑–∞–Ω–∏—Ö –∑–æ–±—Ä–∞–∂–µ–Ω—å. –ü–æ—á–∏–Ω–∞—é –æ–Ω–æ–≤–ª–µ–Ω–Ω—è...")

    seo_data = {
        "title": product_name,
        "alt": f"–ö—É–ø–∏—Ç–∏ —Ç–æ–≤–∞—Ä {product_name} –≤ —Å–µ–∫—Å-—à–æ–ø—ñ Eros.in.ua",
        "caption": f"{product_name} ‚Äì —ñ–Ω–Ω–æ–≤–∞—Ü—ñ–π–Ω–∞ —Å–µ–∫—Å-—ñ–≥—Ä–∞—à–∫–∞ –¥–ª—è –≤–∞—à–æ–≥–æ –∑–∞–¥–æ–≤–æ–ª–µ–Ω–Ω—è",
        "description": f"{product_name} –∫—É–ø–∏—Ç–∏ –≤ —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω—ñ Eros.in.ua. –í–µ–ª–∏–∫–∏–π –≤–∏–±—ñ—Ä —Å–µ–∫—Å-—ñ–≥—Ä–∞—à–æ–∫, –Ω–∏–∑—å–∫–∞ —Ü—ñ–Ω–∞, —à–≤–∏–¥–∫–∞ –±–µ–∑–∫–æ—à—Ç–æ–≤–Ω–∞ –¥–æ—Å—Ç–∞–≤–∫–∞."
    }

    # --- –î–æ–ø–æ–º—ñ–∂–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è: –∑–Ω–∞–π—Ç–∏ media id –ø–æ filename —á–µ—Ä–µ–∑ WP REST API (search) ---
    def find_media_id_by_filename(filename: str) -> int:
        """
        –ü–æ–≤–µ—Ä—Ç–∞—î media id –∞–±–æ None. –ü—Ä–∞—Ü—é—î —á–µ—Ä–µ–∑ /wp-json/wp/v2/media?search=<filename>
        –ü–æ—Ç—Ä—ñ–±–Ω–∞ –∞—É—Ç–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ—è, —è–∫—â–æ WP –∑–∞–∫—Ä–∏—Ç–∏–π. –ú–∏ —Å–ø—Ä–æ–±—É—î–º–æ –±–µ–∑ auth –ø–µ—Ä—à–∏–º, –ø–æ—Ç—ñ–º –∑ auth.
        """
        search_url = f"{base_url}/wp-json/wp/v2/media"
        params = {"search": filename, "per_page": 10}
        headers = {"Accept": "application/json"}

        # –°–ø—Ä–æ–±–∞ –±–µ–∑ auth
        try:
            r = requests.get(search_url, params=params, headers=headers, timeout=15, verify=True)
            if r.status_code == 200:
                items = r.json()
                for it in items:
                    src = it.get("source_url", "") or it.get("guid", {}).get("rendered", "")
                    if filename.lower() in (os.path.basename(src).lower()):
                        return it.get("id")
            # —è–∫—â–æ –Ω–µ –≤–¥–∞–ª–æ—Å—å –∞–±–æ –ø–æ—Ä–æ–∂–Ω—å–æ ‚Äî —Å–ø—Ä–æ–±—É—î–º–æ –∑ auth —è–∫—â–æ —î
        except Exception as e:
            logging.debug(f"find_media_id_by_filename (no auth) error: {e}")

        if wp_login and wp_pass:
            try:
                r = requests.get(search_url, params=params, headers=headers, auth=(wp_login, wp_pass), timeout=15, verify=True)
                if r.status_code == 200:
                    items = r.json()
                    for it in items:
                        src = it.get("source_url", "") or it.get("guid", {}).get("rendered", "")
                        if filename.lower() in (os.path.basename(src).lower()):
                            return it.get("id")
                else:
                    logging.debug(f"find_media_id_by_filename (auth) status {r.status_code}: {r.text[:200]}")
            except Exception as e:
                logging.debug(f"find_media_id_by_filename (auth) exception: {e}")

        return None

    # --- –û—Å–Ω–æ–≤–Ω–∏–π —Ü–∏–∫–ª –æ–Ω–æ–≤–ª–µ–Ω–Ω—è ---
    updated = 0
    failed = 0

    # We'll batch update product images alt/title via product PUT if possible
    # Prepare a copy of current images with alt changes to minimize number of product PUTs.
    wc_images_update = []
    for img in image_list:
        media_id = img.get("id")
        src = img.get("src") or ""
        filename = os.path.basename(src) if src else None

        # Prefer media_id; if missing, try to find by filename
        if not media_id and filename:
            found_id = find_media_id_by_filename(filename)
            if found_id:
                media_id = found_id
                logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ media_id {media_id} –ø–æ —Ñ–∞–π–ª—É {filename}")
            else:
                logging.warning(f"–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ media record –¥–ª—è {filename}. –ü—Ä–æ–ø—É—Å–∫–∞—é.")
                continue

        if media_id:
            # For WooCommerce product update we will set alt and name (title)
            wc_images_update.append({"id": media_id, "alt": seo_data["alt"], "name": seo_data["title"]})

    # If we have any image updates for WooCommerce ‚Äî send one PUT to products/{id}
    if wc_images_update and product_id:
        try:
            resp_put = wcapi.put(f"products/{product_id}", {"images": wc_images_update})
            if resp_put.status_code == 200:
                logging.info("‚úÖ WooCommerce: alt/title –æ–Ω–æ–≤–ª–µ–Ω–æ —á–µ—Ä–µ–∑ products PUT")
                updated += len(wc_images_update)
            else:
                logging.error(f"‚ùå WooCommerce products PUT returned {resp_put.status_code}: {resp_put.text[:300]}")
                # don't return ‚Äî try per-media WP updates below
        except Exception as e:
            logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ WooCommerce products PUT: {e}")

    # Now, attempt to update caption/description via wp/v2/media for each image
    for img in image_list:
        media_id = img.get("id")
        src = img.get("src") or ""
        filename = os.path.basename(src) if src else None

        if not media_id:
            if filename:
                media_id = find_media_id_by_filename(filename)
                if media_id:
                    logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ media_id {media_id} –¥–ª—è {filename} —á–µ—Ä–µ–∑ –ø–æ—à—É–∫.")
                else:
                    logging.warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–Ω–∞–π—Ç–∏ media_id –¥–ª—è {filename}. –ü—Ä–æ–ø—É—Å–∫–∞—é WP media update.")
                    failed += 1
                    continue
            else:
                logging.warning("–ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è –Ω–µ –º–∞—î src —Ç–∞ id ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é.")
                failed += 1
                continue

        media_endpoint = f"{base_url}/wp-json/wp/v2/media/{media_id}"
        update_data = {
            "title": seo_data["title"],
            "alt_text": seo_data["alt"],
            "caption": seo_data["caption"],
            "description": seo_data["description"]
        }

        # –¢—Ä–µ–±–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü—ñ—è –¥–ª—è wp/v2/media (Application Password –∞–±–æ user/pass)
        if not wp_login or not wp_pass:
            logging.warning("‚ö†Ô∏è –í settings.json –Ω–µ –∑–Ω–∞–π–¥–µ–Ω—ñ 'login' —Ç–∞ 'pass' ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é –æ–Ω–æ–≤–ª–µ–Ω–Ω—è caption/description —á–µ—Ä–µ–∑ wp/v2/media.")
            failed += 1
            continue

        try:
            r = requests.put(media_endpoint, auth=(wp_login, wp_pass), json=update_data, timeout=20, verify=True)
            if r.status_code == 200:
                print(f"‚úÖ –û–Ω–æ–≤–ª–µ–Ω–æ –º–µ–¥—ñ–∞ ID {media_id} ({filename if filename else ''})")
                updated += 1
            else:
                logging.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è ID {media_id}. –°—Ç–∞—Ç—É—Å: {r.status_code}. –ü–æ–º–∏–ª–∫–∞: {r.text[:300]}")
                failed += 1
        except requests.exceptions.RequestException as e:
            logging.error(f"–ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Ç—É –¥–ª—è {media_id}: {e}")
            failed += 1

    print(f"üéØ –ó–∞–≤–µ—Ä—à–µ–Ω–æ. –£—Å–ø—ñ—à–Ω–æ –æ–Ω–æ–≤–ª–µ–Ω–æ: {updated}, –Ω–µ –≤–¥–∞–ª–æ—Å—è: {failed}.")

# --- –ó–ê–ü–û–í–ù–ï–ù–ù–Ø –ö–û–õ–û–ù–ö–ò _wpml_import_translation_group ---
def fill_wpml_translation_group():
    """
    –®—É–∫–∞—î trid (_wpml_import_translation_group) —É –±–∞–∑—ñ WordPress
    –∑–∞ SKU —ñ –æ–Ω–æ–≤–ª—é—î —Ü–µ–π —Å–∞–º–∏–π CSV-—Ñ–∞–π–ª (–±–µ–∑ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ–ø—ñ—ó).
    """
    log_message_to_existing_file()
    logging.info("üöÄ –ü–æ—á–∞—Ç–æ–∫ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –∫–æ–ª–æ–Ω–∫–∏ _wpml_import_translation_group")

    settings = load_settings()
    csv_path = settings["paths"].get("csv_path_sl_new_prod_ru")
    db_conf = settings.get("db")

    # üîπ –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó
    if not db_conf:
        logging.error("‚ùå –£ settings.json –≤—ñ–¥—Å—É—Ç–Ω—ñ–π —Ä–æ–∑–¥—ñ–ª 'db' –∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö")
        return

    # üîπ –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ MySQL
    conn = mysql.connector.connect(
        host=db_conf["host"],
        user=db_conf["user"],
        password=db_conf["password"],
        database=db_conf["database"],
        charset="utf8mb4"
    )
    cursor = conn.cursor(dictionary=True)

    # üîπ –ó—á–∏—Ç—É—î–º–æ —É—Å—ñ —Ä—è–¥–∫–∏ –∑ —Ñ–∞–π–ª—É
    with open(csv_path, "r", encoding="utf-8") as infile:
        reader = csv.DictReader(infile)
        rows = list(reader)
        fieldnames = reader.fieldnames
        if "_wpml_import_translation_group" not in fieldnames:
            fieldnames.append("_wpml_import_translation_group")

    logging.info("üöÄ –ü–æ—á–∞—Ç–æ–∫ –ø–æ—à—É–∫—É trid –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ SKU...")

    # üîπ –û–±—Ä–æ–±–∫–∞ –∫–æ–∂–Ω–æ–≥–æ SKU
    for idx, row in enumerate(rows, start=2):
        sku = row.get("Sku")
        if not sku:
            logging.warning(f"–†—è–¥–æ–∫ {idx}: –ø—Ä–æ–ø—É—â–µ–Ω–æ —á–µ—Ä–µ–∑ –≤—ñ–¥—Å—É—Ç–Ω—ñ–π SKU")
            continue

        # –ó–Ω–∞–π—Ç–∏ product_id –∑–∞ SKU
        cursor.execute("""
            SELECT pm.post_id
            FROM wp_postmeta pm
            JOIN wp_posts p ON p.ID = pm.post_id
            WHERE pm.meta_key = '_sku' AND pm.meta_value = %s AND p.post_type = 'product'
            LIMIT 1;
        """, (sku,))
        res = cursor.fetchone()

        if not res:
            logging.warning(f"‚ö†Ô∏è SKU {sku}: —Ç–æ–≤–∞—Ä –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ —É –±–∞–∑—ñ")
            continue

        product_id = res["post_id"]

        # –ó–Ω–∞–π—Ç–∏ trid
        cursor.execute("""
            SELECT trid
            FROM wp_icl_translations
            WHERE element_type = 'post_product' AND element_id = %s
            LIMIT 1;
        """, (product_id,))
        trid_res = cursor.fetchone()

        if trid_res:
            trid = trid_res["trid"]
            row["_wpml_import_translation_group"] = trid
            logging.info(f"‚úÖ SKU {sku}: –∑–Ω–∞–π–¥–µ–Ω–æ trid = {trid}")
        else:
            logging.warning(f"‚ö†Ô∏è SKU {sku}: –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ trid —É wp_icl_translations")

    # üîπ –ó–∞–ø–∏—Å—É—î–º–æ –Ω–∞–∑–∞–¥ —É —Ç–æ–π —Å–∞–º–∏–π —Ñ–∞–π–ª
    with open(csv_path, "w", encoding="utf-8", newline="") as outfile:
        writer = csv.DictWriter(outfile, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(rows)

    cursor.close()
    conn.close()

    logging.info(f"üèÅ –û–Ω–æ–≤–ª–µ–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –î–∞–Ω—ñ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É {csv_path}")

# --- –ü–ï–†–ï–ö–õ–ê–î CSV –£–ö–† ‚Üí –†–£–° ---
def clean_text(text):
    """
    –í–∏–¥–∞–ª—è—î HTML —Ç–µ–≥–∏ —Ç–∞ –∑–∞–π–≤—ñ –ø—Ä–æ–±—ñ–ª–∏.
    """
    if not text:
        return ""
    # text = re.sub(r'<[^>]+>', '', text)  # –≤–∏–¥–∞–ª–∏—Ç–∏ HTML —Ç–µ–≥–∏
    text = re.sub(r'\s+', ' ', text).strip()  # –∑–∞–π–≤—ñ –ø—Ä–æ–±—ñ–ª–∏ —Ç–∞ –ø–µ—Ä–µ–≤–æ–¥–∏ —Ä—è–¥–∫—ñ–≤
    return text

def get_deepl_usage(api_key, api_url="https://api-free.deepl.com/v2/usage"):
    """
    –ü–µ—Ä–µ–≤—ñ—Ä—è—î –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —Å–∏–º–≤–æ–ª—ñ–≤ DeepL API (Free –∞–±–æ Pro).
    –ü–æ–≤–µ—Ä—Ç–∞—î —Å–ª–æ–≤–Ω–∏–∫ –∑ used_characters, limit, remaining.
    """
    try:
        response = requests.get(api_url, headers={"Authorization": f"DeepL-Auth-Key {api_key}"}, timeout=15)
        response.raise_for_status()
        data = response.json()
        used = data.get("character_count", 0)
        limit = data.get("character_limit", 0)
        remaining = limit - used if limit else None
        logging.info(f"üîπ –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ {used:,} —ñ–∑ {limit:,} —Å–∏–º–≤–æ–ª—ñ–≤ DeepL (–∑–∞–ª–∏—à–∏–ª–æ—Å—å {remaining:,})")
        return {"used": used, "limit": limit, "remaining": remaining}
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –ª—ñ–º—ñ—Ç DeepL: {e}")
        return None

def translate_text_deepl(text, target_lang="RU", api_key=None, api_url=None):
    """
    –ü–µ—Ä–µ–∫–ª–∞–¥ —á–µ—Ä–µ–∑ DeepL —ñ–∑ –ë–ï–ó–ü–ï–ß–ù–ò–ú –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è–º HTML:
    - HTML-—Ç–µ–≥–∏ (<strong>, <em>, <p> —Ç–æ—â–æ) –ù–ï –≤—ñ–¥–ø—Ä–∞–≤–ª—è—é—Ç—å—Å—è –≤ DeepL —ñ –ø–æ–≤–µ—Ä—Ç–∞—é—Ç—å—Å—è —è–∫ —î.
    - –ü–µ—Ä–µ–∫–ª–∞–¥–∞—é—Ç—å—Å—è –ª–∏—à–µ —Ç–µ–∫—Å—Ç–æ–≤—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –º—ñ–∂ —Ç–µ–≥–∞–º–∏.
    - –°–µ–≥–º–µ–Ω—Ç–∏ –±–µ–∑ –∫–∏—Ä–∏–ª–∏—Ü—ñ (–∞–Ω–≥–ª/—Ü–∏—Ñ—Ä–∏) –Ω–µ –ø–µ—Ä–µ–∫–ª–∞–¥–∞—é—Ç—å—Å—è –≤–∑–∞–≥–∞–ª—ñ.
    - –î–æ–≤–≥—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ —Ä—ñ–∂—É—Ç—å—Å—è –Ω–∞ —á–∞—Å—Ç–∏–Ω–∏ ‚â§ 500 —Å–∏–º–≤–æ–ª—ñ–≤.
    """
    if not text or not text.strip():
        return text
    if not api_key:
        logging.error("API –∫–ª—é—á DeepL –Ω–µ –≤–∫–∞–∑–∞–Ω–æ!")
        return text
    if not api_url:
        api_url = "https://api-free.deepl.com/v2/translate"

    # 1) –†–æ–∑–¥—ñ–ª–∏—Ç–∏ —Ä—è–¥–æ–∫ –Ω–∞ HTML-—Ç–µ–≥–∏ —ñ –ø—Ä–æ—Å—Ç—ñ —Ç–µ–∫—Å—Ç–æ–≤—ñ —Å–µ–≥–º–µ–Ω—Ç–∏
    #    –ü—Ä–∏–∫–ª–∞–¥: ["<p>", "–¢–µ–∫—Å—Ç ", "<strong>", "–∂–∏—Ä–Ω–∏–π", "</strong>", "</p>"]
    tokens = re.split(r'(<[^>]+>)', text)
    out = []

    # —Ä–µ–≥–µ–∫—Å–ø –¥–ª—è –≤–∏—è–≤–ª–µ–Ω–Ω—è –∫–∏—Ä–∏–ª–∏—Ü—ñ (—É–∫—Ä/ru)
    has_cyrillic = re.compile(r'[–ê-–Ø–∞-—è–Å—ë–á—ó–Ü—ñ–Ñ—î“ê“ë]')

    def translate_chunk(chunk: str) -> str:
        """–ü–µ—Ä–µ–∫–ª–∞—Å—Ç–∏ –æ–¥–∏–Ω –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç–æ–≤–∏–π —à–º–∞—Ç–æ–∫ (‚â§500 —Å–∏–º–≤–æ–ª—ñ–≤)."""
        # —è–∫—â–æ –Ω–µ–º–∞—î –∫–∏—Ä–∏–ª–∏—Ü—ñ ‚Äî –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ —è–∫ —î (–∞–Ω–≥–ª/—Ü–∏—Ñ—Ä–∏ –Ω–µ —á—ñ–ø–∞—î–º–æ)
        if not has_cyrillic.search(chunk):
            return chunk
        # –∂–æ–¥–Ω–∏—Ö —Å–ª—É–∂–±–æ–≤–∏—Ö <i>-—Ç–µ–≥—ñ–≤ —É—Å–µ—Ä–µ–¥–∏–Ω—É ‚Äî –ø—Ä–∞—Ü—é—î–º–æ –∑ —á–∏—Å—Ç–∏–º —Ç–µ–∫—Å—Ç–æ–º
        try:
            resp = requests.post(
                api_url,
                data={
                    "auth_key": api_key,
                    "text": chunk,
                    "target_lang": target_lang,
                },
                timeout=30
            )
            resp.raise_for_status()
            translated = resp.json()["translations"][0]["text"]
            # –Ø–∫—â–æ DeepL —Ä–∞–ø—Ç–æ–º –ø–æ–≤–µ—Ä–Ω—É–≤ &lt;strong&gt; —É —Ç–µ–∫—Å—Ç—ñ, —Ä–æ–∑–∫–æ–¥—É—î–º–æ —Å—É—Ç–Ω–æ—Å—Ç—ñ —Ç—ñ–ª—å–∫–∏ –≤ —Ç–µ–∫—Å—Ç—ñ
            return html.unescape(translated)
        except Exception as e:
            logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø–µ—Ä–µ–∫–ª–∞–¥—É: {e}")
            return chunk

    # 2) –û–±—Ä–æ–±–∏—Ç–∏ –∫–æ–∂–µ–Ω —Ç–æ–∫–µ–Ω
    for tok in tokens:
        if not tok:
            continue
        if tok.startswith("<") and tok.endswith(">"):
            # –¶–µ HTML-—Ç–µ–≥ ‚Äî –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ —è–∫ —î
            out.append(tok)
            continue

        # –¶–µ –ø—Ä–æ—Å—Ç–∏–π —Ç–µ–∫—Å—Ç ‚Äî —Ä—ñ–∂–µ–º–æ –Ω–∞ —á–∞—Å—Ç–∏–Ω–∏ ‚â§ 500 —Å–∏–º–≤–æ–ª—ñ–≤ –ø–æ –≥—Ä–∞–Ω–∏—Ü—è—Ö —Ä–µ—á–µ–Ω—å/—Ä—è–¥–∫—ñ–≤
        text_part = tok
        if not text_part.strip():
            out.append(text_part)
            continue

        # –º‚Äô—è–∫–µ —Ä–µ—á–µ–Ω–Ω—è/–ø–∞—Ä–∞–≥—Ä–∞—Ñ–Ω–µ –¥—ñ–ª–µ–Ω–Ω—è
        pieces = []
        current = ""
        # —Ä–æ–∑–±–∏–≤–∞—î–º–æ –∑–∞ –∫—ñ–Ω—Ü–µ–º —Ä–µ—á–µ–Ω–Ω—è –∞–±–æ –ø–µ—Ä–µ–Ω–æ—Å–æ–º, –∑–±–µ—Ä—ñ–≥–∞—é—á–∏ —Ä–æ–∑–¥—ñ–ª—å–Ω–∏–∫–∏
        for seg in re.split(r'(\. |\n)', text_part):
            if len(current) + len(seg) <= 500:
                current += seg
            else:
                if current:
                    pieces.append(current)
                current = seg
        if current:
            pieces.append(current)

        # –ø–µ—Ä–µ–∫–ª–∞–¥ –∫–æ–∂–Ω–æ–≥–æ —à–º–∞—Ç–∫–∞
        translated_pieces = []
        for p in pieces:
            translated_pieces.append(translate_chunk(p))
            time.sleep(0.4)  # –ª–µ–≥–∫–∏–π —Ç—Ä–æ—Ç–ª—ñ–Ω–≥

        out.append("".join(translated_pieces))

    return "".join(out)

def translate_csv_to_ru():
    """
    –ü–µ—Ä–µ–∫–ª–∞–¥–∞—î content —Ç–∞ excerpt –∑ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó –Ω–∞ —Ä–æ—Å—ñ–π—Å—å–∫—É
    —Ç–∞ –∑–±–µ—Ä—ñ–≥–∞—î —É SL_new_prod_ru.csv
    """
    log_message_to_existing_file()
    logging.info("üöÄ –ü–æ—á–∞—Ç–æ–∫ –ø–µ—Ä–µ–∫–ª–∞–¥—É CSV –Ω–∞ —Ä–æ—Å—ñ–π—Å—å–∫—É...")

    settings = load_settings()
    if not settings:
        logging.error("‚ùå –ù–µ–º–æ–∂–ª–∏–≤–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ settings.json")
        return

    input_path = settings["paths"].get("csv_path_sl_new_prod")
    output_path = settings["paths"].get("csv_path_sl_new_prod_ru")
    api_key = settings.get("deepl_api_key")
    api_url = settings.get("DEEPL_API_URL", "https://api-free.deepl.com/v2/translate")

    if not all([input_path, output_path, api_key]):
        logging.error("‚ùå –ù–µ –≤–∫–∞–∑–∞–Ω—ñ –≤—Å—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ —É settings.json")
        return

    try:
        with open(input_path, 'r', encoding='utf-8') as f_in, \
             open(output_path, 'w', encoding='utf-8', newline='') as f_out:

            reader = csv.DictReader(f_in)
            output_headers = ["sku", "name", "content", "short_description", "rank_math_focus_keyword", "lang", "translation_of"]
            writer = csv.DictWriter(f_out, fieldnames=output_headers)
            writer.writeheader()

            for idx, row in enumerate(reader, start=2):
                new_row = {}

                # 1. SKU
                new_row["sku"] = row.get("sku", "")

                # 2. Name –±–µ–∑ –ø–µ—Ä–µ–∫–ª–∞–¥—É
                new_row["name"] = row.get("name", "")

                # 3. –ü–µ—Ä–µ–∫–ª–∞–¥ content
                content_text = clean_text(row.get("content", ""))
                new_row["content"] = translate_text_deepl(content_text, target_lang="RU", api_key=api_key, api_url=api_url)

                # 4. –ü–µ—Ä–µ–∫–ª–∞–¥ excerpt ‚Üí short_description
                excerpt_text = clean_text(row.get("excerpt", ""))
                new_row["short_description"] = translate_text_deepl(excerpt_text, target_lang="RU", api_key=api_key, api_url=api_url)

                # 5. Rank Math
                new_row["rank_math_focus_keyword"] = row.get("rank_math_focus_keyword", "")

                # 6. WPML
                new_row["lang"] = "ru"
                new_row["translation_of"] = ""  # –º–æ–∂–Ω–∞ –ø—ñ–¥—Å—Ç–∞–≤–∏—Ç–∏ ID –æ—Ä–∏–≥—ñ–Ω–∞–ª—É

                writer.writerow(new_row)
                logging.info(f"–†—è–¥–æ–∫ {idx}: –ø–µ—Ä–µ–∫–ª–∞–¥ content —Ç–∞ short_description –∑–∞–≤–µ—Ä—à–µ–Ω–æ")

        logging.info(f"‚úÖ –ü–µ—Ä–µ–∫–ª–∞–¥ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –§–∞–π–ª –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {output_path}")

    except FileNotFoundError:
        logging.error(f"‚ùå –í—Ö—ñ–¥–Ω–∏–π —Ñ–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {input_path}")
    except Exception as e:
        logging.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–∫–ª–∞–¥—ñ CSV: {e}")

# --- –õ–û–ì–£–í–ê–ù–ù–Ø –ì–õ–û–ë–ê–õ–¨–ù–ò–• –ê–¢–†–ò–ë–£–¢–Ü–í WOO ---
def log_global_attributes():
    """
    –û—Ç—Ä–∏–º—É—î —Å–ø–∏—Å–æ–∫ –≥–ª–æ–±–∞–ª—å–Ω–∏—Ö –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ WooCommerce (pa_*)
    —ñ –≤–∏–≤–æ–¥–∏—Ç—å —ó—Ö —É –ª–æ–≥ —ñ–∑ ID, slug —Ç–∞ –Ω–∞–∑–≤–æ—é.
    """
    log_message_to_existing_file()
    logging.info("üîç –ü–æ—á–∏–Ω–∞—é –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Å–ø–∏—Å–∫—É –≥–ª–æ–±–∞–ª—å–Ω–∏—Ö –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ WooCommerce...")

    settings = load_settings()
    if not settings:
        logging.error("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ settings.json")
        return

    wcapi = get_wc_api(settings)
    if not wcapi:
        logging.error("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è —Å—Ç–≤–æ—Ä–∏—Ç–∏ –æ–±'—î–∫—Ç WooCommerce API.")
        return

    try:
        page = 1
        all_attributes = []
        MAX_PAGES = 5  # üîí –±–µ–∑–ø–µ—á–Ω–∞ –º–µ–∂–∞, –±–æ –≥–ª–æ–±–∞–ª—å–Ω–∏—Ö –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ –º–∞–∫—Å–∏–º—É–º –∫—ñ–ª—å–∫–∞ –¥–µ—Å—è—Ç–∫—ñ–≤

        while page <= MAX_PAGES:
            response = wcapi.get("products/attributes", params={"per_page": 100, "page": page})
            logging.info(f"‚û°Ô∏è –û—Ç—Ä–∏–º–∞–Ω–æ —Å—Ç–æ—Ä—ñ–Ω–∫—É {page} (—Å—Ç–∞—Ç—É—Å {response.status_code})")

            if response.status_code != 200:
                logging.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Ç—ñ –¥–æ WooCommerce API: {response.status_code} - {response.text}")
                break

            data = response.json()
            if not data:
                logging.info("üì≠ –ë—ñ–ª—å—à–µ —Å—Ç–æ—Ä—ñ–Ω–æ–∫ –Ω–µ–º–∞—î ‚Äî –∑–∞–≤–µ—Ä—à—É—é –∑–∞–ø–∏—Ç.")
                break

            all_attributes.extend(data)
            if len(data) < 100:
                break  # –º–µ–Ω—à–µ 100 ‚Äî –∑–Ω–∞—á–∏—Ç—å, –æ—Å—Ç–∞–Ω–Ω—è —Å—Ç–æ—Ä—ñ–Ω–∫–∞
            page += 1

        if not all_attributes:
            logging.warning("‚ö†Ô∏è –ì–ª–æ–±–∞–ª—å–Ω—ñ –∞—Ç—Ä–∏–±—É—Ç–∏ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
            return

        logging.info("üß© --- –ì–ª–æ–±–∞–ª—å–Ω—ñ –∞—Ç—Ä–∏–±—É—Ç–∏ WooCommerce ---")
        for attr in all_attributes:
            attr_id = attr.get("id")
            name = attr.get("name")
            slug = attr.get("slug")
            type_ = attr.get("type")
            orderby = attr.get("order_by")
            logging.info(f"ID={attr_id:>3} | slug={slug:<20} | name={name:<25} | type={type_} | orderby={orderby}")

        logging.info(f"‚úÖ –í—Å—å–æ–≥–æ –∑–Ω–∞–π–¥–µ–Ω–æ {len(all_attributes)} –≥–ª–æ–±–∞–ª—å–Ω–∏—Ö –∞—Ç—Ä–∏–±—É—Ç—ñ–≤.")

    except Exception as e:
        logging.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—ñ —Å–ø–∏—Å–∫—É –∞—Ç—Ä–∏–±—É—Ç—ñ–≤: {e}", exc_info=True)

# --- –ö–û–ù–í–ï–†–¢–ê–¶–Ü–Ø –õ–û–ö–ê–õ–¨–ù–ò–• –ê–¢–†–ò–ë–£–¢–Ü–í –£ –ì–õ–û–ë–ê–õ–¨–ù–Ü ---
def convert_local_attributes_to_global():
    """
    –ü–∞–∫–µ—Ç–Ω–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è –ª–æ–∫–∞–ª—å–Ω–∏—Ö –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ —É –≥–ª–æ–±–∞–ª—å–Ω—ñ
    –¥–ª—è —Ç–æ–≤–∞—Ä—ñ–≤, —Å—Ç–≤–æ—Ä–µ–Ω–∏—Ö –ø—ñ—Å–ª—è 1 –≤–µ—Ä–µ—Å–Ω—è 2025 —Ä–æ–∫—É.
    """
    from datetime import datetime
    import re

    log_message_to_existing_file()
    logging.info("üöÄ –ü–æ—á–∞—Ç–æ–∫ –ø–∞–∫–µ—Ç–Ω–æ—ó –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—ó –ª–æ–∫–∞–ª—å–Ω–∏—Ö –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ —É –≥–ª–æ–±–∞–ª—å–Ω—ñ –¥–ª—è –æ—Å—Ç–∞–Ω–Ω—ñ—Ö —Ç–æ–≤–∞—Ä—ñ–≤...")

    settings = load_settings()
    if not settings:
        logging.critical("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è.")
        return

    wcapi = get_wc_api(settings)
    if not wcapi:
        logging.critical("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –ø—ñ–¥–∫–ª—é—á–∏—Ç–∏—Å—è –¥–æ WooCommerce API.")
        return

    global_attr_map = settings.get("global_attr_map", {})
    numeric_attrs = ["pa_diameter", "pa_length", "pa_height"]
    cutoff_date = datetime(2025, 9, 1)

    def _smart_split_attr(attr_name, val):
        if not val:
            return []
        if attr_name in numeric_attrs:
            parts = [p.strip() for p in re.split(r'[|;,]', val) if p.strip()]
            return [",".join(parts)]
        else:
            parts = [p.strip() for p in re.split(r'[|;,]', val) if p.strip()]
            return parts

    try:
        # –û—Ç—Ä–∏–º—É—î–º–æ –≤—Å—ñ —Ç–æ–≤–∞—Ä–∏ –ø—ñ—Å–ª—è cutoff_date (–ø–æ—Å—Ç–∞—î–º–æ —É —Å—Ç–æ—Ä—ñ–Ω–∫–∞—Ö –ø–æ 100)
        page = 1
        per_page = 10
        total_checked = 0
        total_updated = 0

        while True:
            response = wcapi.get("products", params={
                "per_page": per_page,
                "page": page,
                "after": cutoff_date.isoformat(),
                "orderby": "date",
                "order": "asc"
            })
            if response.status_code != 200:
                logging.error(f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ —Ç–æ–≤–∞—Ä–∏: {response.text}")
                break

            products = response.json()
            if not products:
                break  # –∫—ñ–Ω–µ—Ü—å —Å–ø–∏—Å–∫—É

            for product in products:
                product_id = product["id"]
                product_name = product.get("name", "")
                attributes = product.get("attributes", [])
                local_attrs = []
                global_attrs = []

                for attr in attributes:
                    attr_name = attr.get("name")
                    attr_id = attr.get("id")
                    if attr_id and attr_id in global_attr_map.values():
                        global_attrs.append(attr)
                    else:
                        local_attrs.append(attr)

                logging.info(f"–¢–æ–≤–∞—Ä ID={product_id}, Name='{product_name}': {len(global_attrs)} –≥–ª–æ–±–∞–ª—å–Ω–∏—Ö, {len(local_attrs)} –ª–æ–∫–∞–ª—å–Ω–∏—Ö –∞—Ç—Ä–∏–±—É—Ç—ñ–≤")

                if not local_attrs:
                    total_checked += 1
                    continue

                # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è –ª–æ–∫–∞–ª—å–Ω–∏—Ö —É –≥–ª–æ–±–∞–ª—å–Ω—ñ
                changes = 0
                for attr in local_attrs:
                    name = attr.get("name")
                    value = "|".join(attr.get("options", []))
                    attr["options"] = _smart_split_attr(name, value)
                    if name in global_attr_map:
                        attr["id"] = global_attr_map[name]
                        changes += 1

                if changes:
                    product_data = {"id": product_id, "attributes": attributes}
                    resp_update = wcapi.put(f"products/{product_id}", product_data)
                    if resp_update.status_code == 200:
                        logging.info(f"‚úÖ –û–Ω–æ–≤–ª–µ–Ω–æ {changes} –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ –¥–ª—è SKU={product.get('sku','')} / ID={product_id}")
                        total_updated += changes
                    else:
                        logging.error(f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –æ–Ω–æ–≤–∏—Ç–∏ —Ç–æ–≤–∞—Ä ID={product_id}: {resp_update.text}")

                total_checked += 1

            page += 1

        logging.info(f"--- üèÅ –ü—ñ–¥—Å—É–º–æ–∫ ---")
        logging.info(f"–í—Å—å–æ–≥–æ –ø–µ—Ä–µ–≤—ñ—Ä–µ–Ω–æ —Ç–æ–≤–∞—Ä—ñ–≤: {total_checked}")
        logging.info(f"–í—Å—å–æ–≥–æ –æ–Ω–æ–≤–ª–µ–Ω–æ –∞—Ç—Ä–∏–±—É—Ç—ñ–≤: {total_updated}")

    except Exception as e:
        logging.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}", exc_info=True)

# --- –ü–ï–†–ï–í–Ü–†–ö–ê –î–û–°–¢–£–ü–£ –î–û GOOGLE SEARCH CONSOLE ---
def test_search_console_access():
    """
    –ü–µ—Ä–µ–≤—ñ—Ä—è—î –¥–æ—Å—Ç—É–ø –¥–æ Google Search Console API —á–µ—Ä–µ–∑ Service Account.
    –í–∏–≤–æ–¥–∏—Ç—å —É –ª–æ–≥ —ñ –∫–æ–Ω—Å–æ–ª—å —Å–ø–∏—Å–æ–∫ —Å–∞–π—Ç—ñ–≤, –¥–æ —è–∫–∏—Ö —î –¥–æ—Å—Ç—É–ø.
    """
    # --- 1. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ª–æ–≥—É–≤–∞–Ω–Ω—è ---
    log_message_to_existing_file()
    logging.info("üöÄ –ü–æ—á–∞—Ç–æ–∫ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –¥–æ—Å—Ç—É–ø—É –¥–æ Google Search Console...")

    # --- 2. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å ---
    settings = load_settings()
    if not settings:
        logging.critical("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ settings.json.")
        return

    json_path = settings["paths"].get("google_json")
    if not json_path:
        logging.critical("‚ùå –ù–µ –≤–∫–∞–∑–∞–Ω–æ —à–ª—è—Ö –¥–æ Google JSON –∫–ª—é—á–∞ —É settings.json (paths.google_json).")
        print("‚ùå –ù–µ –≤–∫–∞–∑–∞–Ω–æ —à–ª—è—Ö –¥–æ Google JSON —É settings.json (paths.google_json).")
        return

    # –Ø–∫—â–æ —à–ª—è—Ö –≤—ñ–¥–Ω–æ—Å–Ω–∏–π ‚Äî –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ –Ω–∞ –∞–±—Å–æ–ª—é—Ç–Ω–∏–π
    if not os.path.isabs(json_path):
        base_dir = os.path.join(os.path.dirname(__file__), "..")
        json_path = os.path.normpath(os.path.join(base_dir, json_path))

    if not os.path.exists(json_path):
        logging.critical(f"‚ùå –§–∞–π–ª –∫–ª—é—á–∞ Google JSON –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {json_path}")
        print(f"‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª –∫–ª—é—á–∞ Google JSON:\n{json_path}")
        return

    # --- 3. –Ü–º–ø–æ—Ä—Ç –±—ñ–±–ª—ñ–æ—Ç–µ–∫ Google API ---
    try:
        from google.oauth2 import service_account
        from googleapiclient.discovery import build
    except ImportError:
        msg = "‚ùå –ù–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ñ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏ google-auth —Ç–∞ google-api-python-client."
        logging.critical(msg)
        print(f"{msg}\n–í—Å—Ç–∞–Ω–æ–≤–∏ —ó—Ö –∫–æ–º–∞–Ω–¥–æ—é:\n  pip install google-auth google-auth-oauthlib google-api-python-client")
        return

    # --- 4. –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ Search Console API ---
    SCOPES = ["https://www.googleapis.com/auth/webmasters.readonly"]
    try:
        credentials = service_account.Credentials.from_service_account_file(json_path, scopes=SCOPES)
        service = build("searchconsole", "v1", credentials=credentials)
        response = service.sites().list().execute()
    except Exception as e:
        logging.critical(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—ñ –∑'—î–¥–Ω–∞–Ω–Ω—è –∞–±–æ –∑–∞–ø–∏—Ç—ñ –¥–æ Search Console: {e}", exc_info=True)
        print(f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –ø—ñ–¥–∫–ª—é—á–∏—Ç–∏—Å—è –¥–æ Search Console API.\n–ü–æ–º–∏–ª–∫–∞: {e}")
        return

    # --- 5. –û–±—Ä–æ–±–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ ---
    site_list = response.get("siteEntry", [])
    if not site_list:
        logging.warning("‚ö†Ô∏è –°–µ—Ä–≤—ñ—Å–Ω–∏–π –∞–∫–∞—É–Ω—Ç –Ω–µ –º–∞—î –¥–æ—Å—Ç—É–ø—É –¥–æ –∂–æ–¥–Ω–æ–≥–æ —Å–∞–π—Ç—É —É Search Console.")
        print("‚ö†Ô∏è –ê–∫–∞—É–Ω—Ç –Ω–µ –º–∞—î –¥–æ—Å—Ç—É–ø—É –¥–æ —Å–∞–π—Ç—ñ–≤ —É Search Console.\n–ü–µ—Ä–µ–≤—ñ—Ä, —á–∏ –¥–æ–¥–∞–Ω–æ —Ü–µ–π email —É Search Console –∑ –ø—Ä–∞–≤–∞–º–∏ Full.")
        return

    print("‚úÖ –°–∞–π—Ç–∏, –¥–æ—Å—Ç—É–ø–Ω—ñ —Ü—å–æ–º—É –∞–∫–∞—É–Ω—Ç—É:")
    logging.info("‚úÖ –û—Ç—Ä–∏–º–∞–Ω–æ —Å–ø–∏—Å–æ–∫ —Å–∞–π—Ç—ñ–≤ —É Search Console:")
    for site in site_list:
        url = site.get("siteUrl", "")
        level = site.get("permissionLevel", "")
        print(f" - {url} ({level})")
        logging.info(f" - {url} ({level})")

    logging.info("üéØ –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ Search Console –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø—ñ—à–Ω–æ.")

# --- –ü–ï–†–ï–í–Ü–†–ö–ê –¢–ê –Ü–ù–î–ï–ö–°–ê–¶–Ü–Ø –û–î–ù–Ü–Ñ–á –°–¢–û–†–Ü–ù–ö–ò –í GOOGLE ---
def check_and_index_url_in_google():
    """
    –ó–∞–ø–∏—Ç—É—î URL —Å—Ç–æ—Ä—ñ–Ω–∫–∏, –ø–µ—Ä–µ–≤—ñ—Ä—è—î —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—é –≤ Search Console API.
    –Ø–∫—â–æ –Ω–µ —ñ–Ω–¥–µ–∫—Å–æ–≤–∞–Ω–∞ ‚Äî –Ω–∞–¥—Å–∏–ª–∞—î –∑–∞–ø–∏—Ç –Ω–∞ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—é.
    –õ–æ–≥—É—î –≤—Å—é —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é.
    """
    log_message_to_existing_file()
    logging.info("üöÄ –ü–æ—á–∞—Ç–æ–∫ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —Ç–∞ –º–æ–∂–ª–∏–≤–æ—ó —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏ Google Search Console...")

    # 1Ô∏è‚É£ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å
    settings = load_settings()
    if not settings:
        logging.critical("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ settings.json.")
        return

    json_path = settings["paths"].get("google_json")
    if not json_path:
        print("‚ùå –ù–µ –≤–∫–∞–∑–∞–Ω–æ —à–ª—è—Ö –¥–æ JSON –∫–ª—é—á–∞ —É settings.json")
        logging.critical("‚ùå –ù–µ –≤–∫–∞–∑–∞–Ω–æ —à–ª—è—Ö –¥–æ Google JSON —É settings.json.")
        return

    if not os.path.isabs(json_path):
        base_dir = os.path.join(os.path.dirname(__file__), "..")
        json_path = os.path.normpath(os.path.join(base_dir, json_path))

    if not os.path.exists(json_path):
        print(f"‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª –∫–ª—é—á–∞ Google JSON:\n{json_path}")
        logging.critical(f"‚ùå –§–∞–π–ª –∫–ª—é—á–∞ Google JSON –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {json_path}")
        return

    try:
        from google.oauth2 import service_account
        from googleapiclient.discovery import build
    except ImportError:
        print("‚ö†Ô∏è –í—Å—Ç–∞–Ω–æ–≤–∏ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏: pip install google-auth google-auth-oauthlib google-api-python-client")
        logging.critical("‚ùå –í—ñ–¥—Å—É—Ç–Ω—ñ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏ Google API.")
        return

    # 2Ô∏è‚É£ –í–≤–µ–¥–µ–Ω–Ω—è URL –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–µ–º
    url = input("üîó –í–≤–µ–¥—ñ—Ç—å URL —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏: ").strip()
    if not url.startswith("http"):
        print("‚ùå –ù–µ–∫–æ—Ä–µ–∫—Ç–Ω–∏–π URL.")
        return

    site_url = "https://eros.in.ua/"  # –º–æ–∂–Ω–∞ –≤–∏—Ç—è–≥–∞—Ç–∏ –∑ settings["site_url"], —è–∫—â–æ —Ç—Ä–µ–±–∞

    try:
        # –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü—ñ—è
        credentials = service_account.Credentials.from_service_account_file(
            json_path, scopes=["https://www.googleapis.com/auth/webmasters"]
        )
        service = build("searchconsole", "v1", credentials=credentials)

        # 3Ô∏è‚É£ –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó
        logging.info(f"üîç –ü–µ—Ä–µ–≤—ñ—Ä—è—é —Å—Ç–∞–Ω —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó –¥–ª—è: {url}")
        inspect_body = {"inspectionUrl": url, "siteUrl": site_url}

        try:
            result = service.urlInspection().index().inspect(body=inspect_body).execute()
            index_result = result.get("inspectionResult", {}).get("indexStatusResult", {})
            verdict = index_result.get("verdict", "UNKNOWN")
            coverage = index_result.get("coverageState", "N/A")
            last_crawl = index_result.get("lastCrawlTime", "N/A")
            page_fetch = index_result.get("pageFetchState", "N/A")

            if verdict == "PASS" or "Indexed" in coverage:
                print(f"‚úÖ –°—Ç–æ—Ä—ñ–Ω–∫–∞ –≤–∂–µ –≤ —ñ–Ω–¥–µ–∫—Å—ñ ({coverage}).")
                logging.info(f"‚úÖ –°—Ç–æ—Ä—ñ–Ω–∫–∞ {url} –≤–∂–µ —ñ–Ω–¥–µ–∫—Å–æ–≤–∞–Ω–∞. –û—Å—Ç–∞–Ω–Ω—î —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è: {last_crawl}, —Å—Ç–∞—Ç—É—Å: {page_fetch}")
                return
            else:
                print(f"‚ö†Ô∏è –°—Ç–æ—Ä—ñ–Ω–∫–∞ –Ω–µ —ñ–Ω–¥–µ–∫—Å–æ–≤–∞–Ω–∞ ({coverage}).")
                logging.info(f"‚ö†Ô∏è –ù–µ—ñ–Ω–¥–µ–∫—Å–æ–≤–∞–Ω–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∞: {url}, —Å—Ç–∞—Ç—É—Å: {coverage}")
        except Exception as e:
            print("‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—é:", e)
            logging.warning(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Ç—ñ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó: {e}")

        # 4Ô∏è‚É£ –Ø–∫—â–æ –Ω–µ —ñ–Ω–¥–µ–∫—Å–æ–≤–∞–Ω–∞ ‚Äî –ø—Ä–æ–±—É—î–º–æ –Ω–∞–¥—ñ—Å–ª–∞—Ç–∏ –Ω–∞ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—é
        print("üì§ –í—ñ–¥–ø—Ä–∞–≤–ª—è—é —Å—Ç–æ—Ä—ñ–Ω–∫—É –Ω–∞ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—é...")
        try:
            response = service.urlInspection().index().inspect(
                body={"inspectionUrl": url, "siteUrl": site_url}
            ).execute()
            logging.info(f"üìÖ {datetime.now().isoformat()} ‚Äî –í—ñ–¥–ø—Ä–∞–≤–ª–µ–Ω–æ URL –Ω–∞ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—é: {url}")
            print("‚úÖ –ó–∞–ø–∏—Ç –Ω–∞ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—é –≤—ñ–¥–ø—Ä–∞–≤–ª–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ.")
        except Exception as e:
            logging.error(f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –≤—ñ–¥–ø—Ä–∞–≤–∏—Ç–∏ URL –Ω–∞ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—é: {e}", exc_info=True)
            print("‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –Ω–∞–¥—Å–∏–ª–∞–Ω–Ω—ñ –Ω–∞ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—é:", e)

    except Exception as e:
        logging.critical(f"‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ Google API: {e}", exc_info=True)
        print("‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞:", e)

def process_indexing_for_new_products():
    """
    –û–Ω–æ–≤–ª–µ–Ω–∞ –≤–µ—Ä—Å—ñ—è:
    1. –ó—á–∏—Ç—É—î SKU –∑ SL_new_prod.csv.
    2. –ó–Ω–∞—Ö–æ–¥–∏—Ç—å –æ–±–∏–¥–≤—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ (UA, RU) —á–µ—Ä–µ–∑ _wpml_import_translation_group.
    3. –ü–µ—Ä–µ–≤—ñ—Ä—è—î —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—é —É Google Search Console.
    4. –û–Ω–æ–≤–ª—é—î index_google.csv —Ç–∞ none_index.csv (–±–µ–∑ –¥—É–±–ª—ñ–≤).
    5. –Ø–∫—â–æ –ø–µ—Ä–µ–≤–∏—â–µ–Ω–æ –∫–≤–æ—Ç—É API (429 –∞–±–æ Quota exceeded) ‚Äî URL –¥–æ–¥–∞—î—Ç—å—Å—è –≤ index_none_quota.csv.
    """
    import csv, os, time, logging, mysql.connector
    from datetime import datetime
    from googleapiclient.errors import HttpError

    log_message_to_existing_file()
    logging.info("üöÄ –ü–æ—á–∞—Ç–æ–∫ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —Ç–∞ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ —É Google Search Console...")

    # --- –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å ---
    settings = load_settings()
    if not settings:
        logging.critical("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ settings.json.")
        return

    paths = settings.get("paths", {})
    csv_path = paths.get("csv_path_sl_new_prod")
    index_log_path = paths.get("index_google")
    none_index_path = paths.get("none_index")
    index_none_quota_path = paths.get("index_none_quota")
    json_path = paths.get("google_json")

    if not all([csv_path, index_log_path, none_index_path, json_path]):
        logging.critical("‚ùå –í—ñ–¥—Å—É—Ç–Ω—ñ –Ω–µ–æ–±—Ö—ñ–¥–Ω—ñ —à–ª—è—Ö–∏ —É settings.json.")
        return

    db_conf = settings.get("db")
    if not db_conf:
        logging.critical("‚ùå –í—ñ–¥—Å—É—Ç–Ω—ñ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –±–∞–∑–∏ –¥–∞–Ω–∏—Ö —É settings.json.")
        return

    # --- –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ MySQL ---
    try:
        conn = mysql.connector.connect(
            host=db_conf["host"],
            user=db_conf["user"],
            password=db_conf["password"],
            database=db_conf["database"],
            charset="utf8mb4"
        )
        cursor = conn.cursor(dictionary=True)
    except Exception as e:
        logging.critical(f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –ø—ñ–¥–∫–ª—é—á–∏—Ç–∏—Å—è –¥–æ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö: {e}")
        return

    # --- –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ Google Search Console API ---
    try:
        from google.oauth2 import service_account
        from googleapiclient.discovery import build
        credentials = service_account.Credentials.from_service_account_file(
            json_path, scopes=["https://www.googleapis.com/auth/webmasters"]
        )
        service = build("searchconsole", "v1", credentials=credentials)
    except Exception as e:
        logging.critical(f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è —Å—Ç–≤–æ—Ä–∏—Ç–∏ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ Search Console API: {e}")
        return

    # --- –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —ñ—Å–Ω—É—é—á–∏—Ö URL ---
    existing_urls = set()
    if os.path.exists(index_log_path):
        with open(index_log_path, "r", encoding="utf-8") as f:
            for row in csv.DictReader(f):
                existing_urls.add(row["URL"].strip())

    none_index_urls = set()
    if os.path.exists(none_index_path):
        with open(none_index_path, "r", encoding="utf-8") as f:
            for row in csv.DictReader(f):
                none_index_urls.add(row["URL"].strip())

    # --- –ó—á–∏—Ç—É—î–º–æ SKU ---
    try:
        with open(csv_path, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            skus = [r["sku"].strip() for r in reader if r.get("sku")]
    except Exception as e:
        logging.critical(f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –ø—Ä–æ—á–∏—Ç–∞—Ç–∏ —Ñ–∞–π–ª SKU: {e}")
        return

    logging.info(f"üì¶ –ó–Ω–∞–π–¥–µ–Ω–æ {len(skus)} SKU —É —Ñ–∞–π–ª—ñ SL_new_prod.csv")

    # --- –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ CSV –¥–ª—è index_google.csv ---
    fieldnames = [
        "URL", "–¢–∏–ø —Å—Ç–æ—Ä—ñ–Ω–∫–∏", "–°—Ç–∞–Ω —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó", "–í–µ—Ä–¥–∏–∫—Ç (verdict)", "CoverageState",
        "Last Crawl Time", "Page Fetch State", "Indexing Allowed", "–î–∞—Ç–∞ –∑–∞–ø–∏—Ç—É",
        "–í—ñ–¥–ø—Ä–∞–≤–ª–µ–Ω–æ –Ω–∞ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—é", "–î–∞—Ç–∞ –Ω–∞–¥—Å–∏–ª–∞–Ω–Ω—è", "–ü–æ–º–∏–ª–∫–∞ API", "–û–ø–∏—Å –ø–æ–º–∏–ª–∫–∏",
        "HTTP —Å—Ç–∞—Ç—É—Å —Å—Ç–æ—Ä—ñ–Ω–∫–∏", "–ö–æ–º–µ–Ω—Ç–∞—Ä", "ResponseTime", "Tries"
    ]
    index_file_exists = os.path.exists(index_log_path)
    with open(index_log_path, "a", encoding="utf-8", newline="") as index_file:
        writer = csv.DictWriter(index_file, fieldnames=fieldnames)
        if not index_file_exists:
            writer.writeheader()

        # --- –û–±—Ä–æ–±–∫–∞ SKU ---
        for sku in skus:
            try:
                # --- 1Ô∏è‚É£ –ó–Ω–∞–π—Ç–∏ —Ç–æ–≤–∞—Ä –ø–æ SKU ---
                cursor.execute("""
                    SELECT p.ID, p.post_name
                    FROM wp_posts p
                    JOIN wp_postmeta m ON p.ID = m.post_id
                    WHERE m.meta_key = '_sku' AND m.meta_value = %s
                    AND p.post_type = 'product' AND p.post_status = 'publish'
                    LIMIT 1;
                """, (sku,))
                product = cursor.fetchone()

                if not product:
                    logging.warning(f"‚ö†Ô∏è SKU {sku}: –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä —É –±–∞–∑—ñ.")
                    continue

                product_id = product["ID"]

                # --- 2Ô∏è‚É£ –û—Ç—Ä–∏–º—É—î–º–æ trid —ñ –º–æ–≤–∏ ---
                cursor.execute("""
                    SELECT trid, language_code
                    FROM wp_icl_translations
                    WHERE element_type = 'post_product' AND element_id = %s
                    LIMIT 1;
                """, (product_id,))
                tinfo = cursor.fetchone()
                trid = tinfo["trid"] if tinfo else None

                urls_to_check = []
                if trid:
                    cursor.execute("""
                        SELECT element_id, language_code
                        FROM wp_icl_translations
                        WHERE trid = %s AND element_type = 'post_product';
                    """, (trid,))
                    translations = cursor.fetchall()
                    for tr in translations:
                        lang = tr["language_code"]
                        pid = tr["element_id"]
                        cursor.execute("""
                            SELECT post_name FROM wp_posts 
                            WHERE ID = %s AND post_status = 'publish' LIMIT 1;
                        """, (pid,))
                        slug_row = cursor.fetchone()
                        if not slug_row:
                            continue
                        slug = slug_row["post_name"]
                        if lang == "uk":
                            urls_to_check.append(("UA", f"https://eros.in.ua/product/{slug}/"))
                        elif lang == "ru":
                            urls_to_check.append(("RU", f"https://eros.in.ua/ru/product/{slug}/"))

                # --- 3Ô∏è‚É£ –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó ---
                for lang, url in urls_to_check:
                    if url in existing_urls:
                        continue

                    result = {
                        "URL": url,
                        "–¢–∏–ø —Å—Ç–æ—Ä—ñ–Ω–∫–∏": "product",
                        "–î–∞—Ç–∞ –∑–∞–ø–∏—Ç—É": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        "Tries": 1
                    }
                    start_time = time.time()

                    try:
                        inspect_body = {"inspectionUrl": url, "siteUrl": "https://eros.in.ua/"}
                        res = service.urlInspection().index().inspect(body=inspect_body).execute()
                        info = res.get("inspectionResult", {}).get("indexStatusResult", {})

                        result["–í–µ—Ä–¥–∏–∫—Ç (verdict)"] = info.get("verdict", "")
                        result["CoverageState"] = info.get("coverageState", "")
                        result["Last Crawl Time"] = info.get("lastCrawlTime", "")
                        result["Page Fetch State"] = info.get("pageFetchState", "")
                        result["Indexing Allowed"] = info.get("indexingState", "")
                        result["ResponseTime"] = round(time.time() - start_time, 2)

                        if "Indexed" in info.get("coverageState", ""):
                            result["–°—Ç–∞–Ω —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó"] = "Indexed"
                            result["–í—ñ–¥–ø—Ä–∞–≤–ª–µ–Ω–æ –Ω–∞ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—é"] = "No"
                            logging.info(f"‚úÖ SKU {sku} ({lang}) Indexed: {url}")
                        else:
                            result["–°—Ç–∞–Ω —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó"] = "Not Indexed"
                            result["–í—ñ–¥–ø—Ä–∞–≤–ª–µ–Ω–æ –Ω–∞ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—é"] = "Yes"
                            result["–î–∞—Ç–∞ –Ω–∞–¥—Å–∏–ª–∞–Ω–Ω—è"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                            if url not in none_index_urls:
                                with open(none_index_path, "a", encoding="utf-8", newline="") as f:
                                    writer_none = csv.DictWriter(f, fieldnames=["URL"])
                                    if os.path.getsize(none_index_path) == 0:
                                        writer_none.writeheader()
                                    writer_none.writerow({"URL": url})
                                    none_index_urls.add(url)
                            logging.warning(f"‚ö†Ô∏è SKU {sku} ({lang}) Not Indexed ‚Äî –≤—ñ–¥–ø—Ä–∞–≤–ª–µ–Ω–æ: {url}")

                    except HttpError as e:
                        status = getattr(e, "resp", None).status if hasattr(e, "resp") else None
                        msg = str(e)
                        result["–°—Ç–∞–Ω —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó"] = "Error"
                        result["–ü–æ–º–∏–ª–∫–∞ API"] = status
                        result["–û–ø–∏—Å –ø–æ–º–∏–ª–∫–∏"] = msg
                        logging.error(f"‚ùå SKU {sku} ({lang}) –ü–æ–º–∏–ª–∫–∞ API: {msg}")

                        # --- —è–∫—â–æ –ø–µ—Ä–µ–≤–∏—â–µ–Ω–æ –∫–≤–æ—Ç—É ---
                        if status == 429 or "quota" in msg.lower():
                            logging.warning(f"üö´ –õ—ñ–º—ñ—Ç –∫–≤–æ—Ç–∏! URL –¥–æ–¥–∞–Ω–æ —É index_none_quota.csv ‚Üí {url}")
                            if index_none_quota_path:
                                try:
                                    with open(index_none_quota_path, "a", encoding="utf-8", newline="") as f:
                                        writer_quota = csv.DictWriter(f, fieldnames=["URL"])
                                        if os.path.getsize(index_none_quota_path) == 0:
                                            writer_quota.writeheader()
                                        writer_quota.writerow({"URL": url})
                                except Exception as file_err:
                                    logging.error(f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–ø–∏—Å–∞—Ç–∏ URL —É index_none_quota.csv: {file_err}")

                    except Exception as e:
                        result["–°—Ç–∞–Ω —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó"] = "Error"
                        result["–û–ø–∏—Å –ø–æ–º–∏–ª–∫–∏"] = str(e)
                        logging.error(f"‚ùå SKU {sku} ({lang}) –ü–æ–º–∏–ª–∫–∞ API (–Ω–µ–≤—ñ–¥–æ–º–∞): {e}")

                    # --- –ó–∞–ø–∏—Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É ---
                    writer.writerow(result)
                    existing_urls.add(url)

            except Exception as e:
                logging.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—Ä–æ–±—Ü—ñ SKU {sku}: {e}", exc_info=True)

    cursor.close()
    conn.close()
    logging.info("üèÅ –ü–µ—Ä–µ–≤—ñ—Ä–∫—É —Ç–∞ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—é –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")

# --- –ü–û–í–¢–û–†–ù–ê –ü–ï–†–ï–í–Ü–†–ö–ê NONE_INDEX.CSV ---
def recheck_none_indexed_pages():
    """
    –ü–µ—Ä–µ–≤—ñ—Ä—è—î URL —ñ–∑ none_index.csv:
    - –Ø–∫—â–æ –≤–∂–µ —ñ–Ω–¥–µ–∫—Å–æ–≤–∞–Ω–∏–π ‚Üí –æ–Ω–æ–≤–ª—é—î –∞–±–æ –¥–æ–¥–∞—î —É index_google.csv —ñ –≤–∏–¥–∞–ª—è—î –∑ none_index.csv.
    - –Ø–∫—â–æ –Ω—ñ ‚Üí –ø–æ–≤—Ç–æ—Ä–Ω–æ –≤—ñ–¥–ø—Ä–∞–≤–ª—è—î –Ω–∞ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—é —Ç–∞ –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—å —É –∫—ñ–Ω–µ—Ü—å none_index.csv.
    - –ó—É–ø–∏–Ω—è—î—Ç—å—Å—è, —è–∫—â–æ –≤–∏—á–µ—Ä–ø–∞–Ω–æ –∫–≤–æ—Ç—É API –∞–±–æ –ø—Ä–æ–π–¥–µ–Ω–æ –≤—Å—ñ URL.
    """
    import csv, time
    from datetime import datetime
    from googleapiclient.errors import HttpError

    log_message_to_existing_file()
    logging.info("üöÄ –ü–æ—á–∞—Ç–æ–∫ –ø–æ–≤—Ç–æ—Ä–Ω–æ—ó –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ none_index.csv (—Ä–µ—ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—è)")

    # --- 1Ô∏è‚É£ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å ---
    settings = load_settings()
    if not settings:
        logging.critical("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ settings.json.")
        return

    paths = settings.get("paths", {})
    none_index_path = paths.get("none_index")
    index_log_path = paths.get("index_google")
    json_path = paths.get("google_json")

    if not all([none_index_path, index_log_path, json_path]):
        logging.critical("‚ùå –í—ñ–¥—Å—É—Ç–Ω—ñ –Ω–µ–æ–±—Ö—ñ–¥–Ω—ñ —à–ª—è—Ö–∏ —É settings.json (none_index, index_google, google_json).")
        return

    # --- 2Ô∏è‚É£ –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ Google Search Console API ---
    try:
        from google.oauth2 import service_account
        from googleapiclient.discovery import build
        credentials = service_account.Credentials.from_service_account_file(
            json_path, scopes=["https://www.googleapis.com/auth/webmasters"]
        )
        service = build("searchconsole", "v1", credentials=credentials)
    except Exception as e:
        logging.critical(f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è —Å—Ç–≤–æ—Ä–∏—Ç–∏ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ Search Console API: {e}")
        return

    # --- 3Ô∏è‚É£ –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ none_index.csv ---
    if not os.path.exists(none_index_path):
        logging.warning(f"‚ö†Ô∏è –§–∞–π–ª {none_index_path} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
        return

    with open(none_index_path, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        urls = [row["URL"].strip() for row in reader if row.get("URL")]

    if not urls:
        logging.info("‚úÖ –§–∞–π–ª none_index.csv –ø–æ—Ä–æ–∂–Ω—ñ–π ‚Äî —É—Å—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –≤–∂–µ –ø—Ä–æ—ñ–Ω–¥–µ–∫—Å–æ–≤–∞–Ω—ñ.")
        return

    # --- 4Ô∏è‚É£ –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ index_google.csv ---
    index_data = []
    existing_urls = set()
    index_fieldnames = [
        "URL","–¢–∏–ø —Å—Ç–æ—Ä—ñ–Ω–∫–∏","–°—Ç–∞–Ω —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó","–í–µ—Ä–¥–∏–∫—Ç (verdict)","CoverageState",
        "Last Crawl Time","Page Fetch State","Indexing Allowed","–î–∞—Ç–∞ –∑–∞–ø–∏—Ç—É",
        "–í—ñ–¥–ø—Ä–∞–≤–ª–µ–Ω–æ –Ω–∞ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—é","–î–∞—Ç–∞ –Ω–∞–¥—Å–∏–ª–∞–Ω–Ω—è","–ü–æ–º–∏–ª–∫–∞ API","–û–ø–∏—Å –ø–æ–º–∏–ª–∫–∏",
        "HTTP —Å—Ç–∞—Ç—É—Å —Å—Ç–æ—Ä—ñ–Ω–∫–∏","–ö–æ–º–µ–Ω—Ç–∞—Ä","ResponseTime","Tries"
    ]

    if os.path.exists(index_log_path):
        with open(index_log_path, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            index_data = list(reader)
            for row in index_data:
                existing_urls.add(row["URL"].strip())

    # --- 5Ô∏è‚É£ –û—Å–Ω–æ–≤–Ω–∏–π —Ü–∏–∫–ª –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ ---
    processed = 0
    indexed_now = 0
    reindexed = 0

    original_urls = list(urls)
    updated_urls = []  # —Ç—É—Ç –∑–±–∏—Ä–∞—Ç–∏–º–µ–º–æ —Ç–µ, —â–æ –∑–∞–ª–∏—à–∏—Ç—å—Å—è –≤ none_index

    for url in original_urls:
        processed += 1
        logging.info(f"üîç –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ {url}")

        start_time = time.time()
        result_row = {
            "URL": url,
            "–¢–∏–ø —Å—Ç–æ—Ä—ñ–Ω–∫–∏": "product",
            "–î–∞—Ç–∞ –∑–∞–ø–∏—Ç—É": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "Tries": 1
        }

        try:
            inspect_body = {"inspectionUrl": url, "siteUrl": "https://eros.in.ua/"}
            res = service.urlInspection().index().inspect(body=inspect_body).execute()
            info = res.get("inspectionResult", {}).get("indexStatusResult", {})

            result_row["–í–µ—Ä–¥–∏–∫—Ç (verdict)"] = info.get("verdict", "")
            result_row["CoverageState"] = info.get("coverageState", "")
            result_row["Last Crawl Time"] = info.get("lastCrawlTime", "")
            result_row["Page Fetch State"] = info.get("pageFetchState", "")
            result_row["Indexing Allowed"] = info.get("indexingState", "")
            result_row["ResponseTime"] = round(time.time() - start_time, 2)

            coverage = info.get("coverageState", "")
            verdict = info.get("verdict", "")
            is_indexed = ("Indexed" in coverage) or (verdict == "PASS")

            # --- –Ø–∫—â–æ Indexed ---
            if is_indexed:
                indexed_now += 1
                result_row["–°—Ç–∞–Ω —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó"] = "Indexed"
                result_row["–í—ñ–¥–ø—Ä–∞–≤–ª–µ–Ω–æ –Ω–∞ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—é"] = "No"
                logging.info(f"‚úÖ {url} –≤–∂–µ –≤ —ñ–Ω–¥–µ–∫—Å—ñ ‚Äî –æ–Ω–æ–≤–ª—é—é index_google —ñ –≤–∏–¥–∞–ª—è—é –∑ none_index.")

                # –û–Ω–æ–≤–ª—é—î–º–æ –∞–±–æ –¥–æ–¥–∞—î–º–æ –¥–æ index_google
                updated = False
                for row in index_data:
                    if row["URL"] == url:
                        row.update(result_row)
                        updated = True
                        break
                if not updated:
                    index_data.append(result_row)

                # ‚ùå –ù–µ –¥–æ–¥–∞—î–º–æ –Ω–∞–∑–∞–¥ —É updated_urls (—Ç–æ–±—Ç–æ –≤–∏–¥–∞–ª—è—î–º–æ)
                continue

            # --- –Ø–∫—â–æ –Ω–µ Indexed ---
            reindexed += 1
            result_row["–°—Ç–∞–Ω —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó"] = "Not Indexed"
            result_row["–í—ñ–¥–ø—Ä–∞–≤–ª–µ–Ω–æ –Ω–∞ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—é"] = "Yes"
            result_row["–î–∞—Ç–∞ –Ω–∞–¥—Å–∏–ª–∞–Ω–Ω—è"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            logging.warning(f"‚ö†Ô∏è {url} –Ω–µ —ñ–Ω–¥–µ–∫—Å–æ–≤–∞–Ω–∞ ‚Äî –ø–æ–≤—Ç–æ—Ä–Ω–∞ –≤—ñ–¥–ø—Ä–∞–≤–∫–∞...")

            try:
                service.urlInspection().index().inspect(body=inspect_body).execute()
            except Exception as send_err:
                logging.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ–º—É –Ω–∞–¥—Å–∏–ª–∞–Ω–Ω—ñ {url}: {send_err}")

            # –û–Ω–æ–≤–ª—é—î–º–æ –∞–±–æ –¥–æ–¥–∞—î–º–æ —É index_google
            updated = False
            for row in index_data:
                if row["URL"] == url:
                    row.update(result_row)
                    updated = True
                    break
            if not updated:
                index_data.append(result_row)

            # üîÅ –î–æ–¥–∞—î–º–æ —É –∫—ñ–Ω–µ—Ü—å —á–µ—Ä–≥–∏ (–æ–Ω–æ–≤–ª–µ–Ω–∏–π —Å–ø–∏—Å–æ–∫)
            updated_urls.append(url)

        except HttpError as e:
            if "quota" in str(e).lower() or "resource_exhausted" in str(e).lower():
                logging.error("‚ö†Ô∏è –í–∏—á–µ—Ä–ø–∞–Ω–æ –¥–µ–Ω–Ω–∏–π –ª—ñ–º—ñ—Ç API. –ó—É–ø–∏–Ω—è—é –ø–µ—Ä–µ–≤—ñ—Ä–∫—É.")
                break
            else:
                result_row["–°—Ç–∞–Ω —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó"] = "Error"
                result_row["–ü–æ–º–∏–ª–∫–∞ API"] = getattr(e, "status_code", "")
                result_row["–û–ø–∏—Å –ø–æ–º–∏–ª–∫–∏"] = str(e)
                logging.error(f"‚ùå API Error {url}: {e}")
                updated_urls.append(url)  # –∑–±–µ—Ä—ñ–≥–∞—î–º–æ –≤ —á–µ—Ä–∑—ñ, –±–æ –Ω–µ –ø–µ—Ä–µ–≤—ñ—Ä–µ–Ω–æ
        except Exception as e:
            result_row["–°—Ç–∞–Ω —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó"] = "Error"
            result_row["–û–ø–∏—Å –ø–æ–º–∏–ª–∫–∏"] = str(e)
            logging.error(f"‚ùå –ù–µ–≤—ñ–¥–æ–º–∞ –ø–æ–º–∏–ª–∫–∞ {url}: {e}")
            updated_urls.append(url)

        # --- –û–Ω–æ–≤–ª—é—î–º–æ index_google.csv –ø—ñ—Å–ª—è –∫–æ–∂–Ω–æ–≥–æ URL ---
        with open(index_log_path, "w", encoding="utf-8", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=index_fieldnames)
            writer.writeheader()
            writer.writerows(index_data)

        time.sleep(1)

    # --- 6Ô∏è‚É£ –ó–∞–ø–∏—Å –æ–Ω–æ–≤–ª–µ–Ω–æ–≥–æ none_index.csv ---
    with open(none_index_path, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=["URL"])
        writer.writeheader()
        for u in updated_urls:
            writer.writerow({"URL": u})

    logging.info("üèÅ –ü–µ—Ä–µ–≤—ñ—Ä–∫—É –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")
    logging.info(f"üßÆ –í—Å—å–æ–≥–æ –ø–µ—Ä–µ–≤—ñ—Ä–µ–Ω–æ: {processed}")
    logging.info(f"‚úÖ –í —ñ–Ω–¥–µ–∫—Å—ñ: {indexed_now}")
    logging.info(f"üì§ –ü–æ–≤—Ç–æ—Ä–Ω–æ –≤—ñ–¥–ø—Ä–∞–≤–ª–µ–Ω–æ: {reindexed}")


# --- –ü–†–ï–õ–û–ê–î Fastcgi –ö–ï–®–£ ---
USER_AGENTS = {
    "DESKTOP": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "MOBILE":  "Mozilla/5.0 (Linux; Android 10; Mobile) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36",
}

def _read_urls(file_path: str):
    with open(file_path, "r", encoding="utf-8") as f:
        return [line.strip() for line in f if line.strip() and not line.strip().startswith("#")]

def preload_cache_from_urls(source: int = 1, timeout: int = 20, pause_sec: float = 0.5):
    """
    –ü—Ä–µ–ª–æ–∞–¥ –∫–µ—à—É Nginx –ë–ï–ó –æ—á–∏—â–µ–Ω–Ω—è.
    source=1 -> settings['paths']['base_url']
    source=2 -> settings['paths']['product_url']
    –î–ª—è –∫–æ–∂–Ω–æ–≥–æ URL –¥–≤—ñ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ (DESKTOP/MOBILE). –õ–æ–≥ ‚Äî —á–µ—Ä–µ–∑ logging.info().
    """
    # 1) –ü—ñ–¥–∫–ª—é—á–∞—î–º–æ –ª–æ–≥-—Ö–µ–Ω–¥–ª–µ—Ä –¥–æ —ñ—Å–Ω—É—é—á–æ–≥–æ —Ñ–∞–π–ª—É (–í–ê–ñ–õ–ò–í–û: –±–µ–∑ –∞—Ä–≥—É–º–µ–Ω—Ç—ñ–≤)
    log_message_to_existing_file()

    settings = load_settings()
    if not settings:
        logging.info("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ settings.json –¥–ª—è –ø—Ä–µ–ª–æ–∞–¥—É –∫–µ—à—É.")
        return

    paths = settings.get("paths", {})
    if source == 1:
        urls_file = paths.get("base_url")
        source_name = "base_url"
    elif source == 2:
        urls_file = paths.get("product_url")
        source_name = "product_url"
    else:
        logging.info("‚ùå –ü–∞—Ä–∞–º–µ—Ç—Ä source –º–∞—î –±—É—Ç–∏ 1 –∞–±–æ 2.")
        return

    if not urls_file or not os.path.exists(urls_file):
        logging.info(f"‚ùå –§–∞–π–ª —ñ–∑ URL –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ ({source_name}): {urls_file}")
        return

    urls = _read_urls(urls_file)
    total = len(urls)
    logging.info(f"üöÄ –°—Ç–∞—Ä—Ç –ø—Ä–µ–ª–æ–∞–¥—É –∫–µ—à—É (source={source_name}, URLs={total})")

    with requests.Session() as session:
        session.headers.update({"Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"})

        for i, url in enumerate(urls, start=1):
            for agent_name, agent_val in USER_AGENTS.items():
                headers = {"User-Agent": agent_val}
                t0 = time.perf_counter()
                status = None
                xfcc = "-"
                try:
                    resp = session.get(url, headers=headers, timeout=timeout)
                    status = resp.status_code
                    xfcc = resp.headers.get("X-FastCGI-Cache") or resp.headers.get("x-fastcgi-cache") or "-"
                except requests.RequestException as e:
                    status = "ERR"
                    xfcc = f"ERR:{type(e).__name__}"
                elapsed_ms = int((time.perf_counter() - t0) * 1000)

                logging.info(f"[{i}/{total}][{agent_name}] {url} -> {status}, X-FastCGI-Cache={xfcc}, {elapsed_ms}ms")

            if pause_sec and pause_sec > 0:
                time.sleep(pause_sec)

    logging.info("‚úÖ –ü—Ä–µ–ª–æ–∞–¥ –∫–µ—à—É –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")









