import csv
import os
import time
import requests
import shutil
import re
import pandas as pd
import mimetypes
from bs4 import BeautifulSoup
import random 
from PIL import Image
import logging
import html
from typing import Dict, Tuple, List, Optional, Any
from scr.base_function import get_wc_api, load_settings, setup_new_log_file, log_message_to_existing_file, load_attributes_csv, \
                                save_attributes_csv, load_category_csv, save_category_csv, load_poznachky_csv, \
                                _process_batch_update, find_media_ids_for_sku, _process_batch_create, clear_directory, \
                                download_product_images, move_gifs, convert_to_webp_square, sync_webp_column, copy_to_site, \
                                translate_text_deepl, get_deepl_usage, fill_wpml_translation_group
from datetime import datetime, timedelta


def find_new_products():
    """
    –ü–æ—Ä—ñ–≤–Ω—é—î –∞—Ä—Ç–∏–∫—É–ª–∏ —Ç–æ–≤–∞—Ä—ñ–≤ –∑ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç–∞ –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ –∑ –∞—Ä—Ç–∏–∫—É–ª–∞–º–∏,
    —â–æ —î –Ω–∞ —Å–∞–π—Ç—ñ, —ñ –∑–∞–ø–∏—Å—É—î –Ω–æ–≤—ñ —Ç–æ–≤–∞—Ä–∏ –≤ –æ–∫—Ä–µ–º–∏–π —Ñ–∞–π–ª.
    """
    # --- 1. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ª–æ–≥—É–≤–∞–Ω–Ω—è ---
    log_message_to_existing_file()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 1. –ü–æ—á–∏–Ω–∞—é –ø–æ—à—É–∫ –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤...")
    
    # --- 2. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å –∑ settings.json ---
    settings = load_settings()
    
    # --- 3. –û—Ç—Ä–∏–º–∞–Ω–Ω—è —à–ª—è—Ö—ñ–≤ –¥–æ –ø–æ—Ç—Ä—ñ–±–Ω–∏—Ö —Ñ–∞–π–ª—ñ–≤ ---
    zalishki_path = settings['paths']['csv_path_zalishki']                   # –ë–∞–∑–∞ —ñ—Å–Ω—É—é—á–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤
    supliers_new_path = settings['paths']['csv_path_supliers_1_new']         # –§–∞–π–ª, –∫—É–¥–∏ –±—É–¥–µ –∑–∞–ø–∏—Å–∞–Ω–æ –Ω–æ–≤—ñ —Ç–æ–≤–∞—Ä–∏
    supliers_csv_path = settings['suppliers']['1']['csv_path']               # –ü—Ä–∞–π—Å-–ª–∏—Å—Ç –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ 1
    delimiter = settings['suppliers']['1']['delimiter']                      # –†–æ–∑–¥—ñ–ª—å–Ω–∏–∫ —É CSV
    
    # --- 4. –û—Ç—Ä–∏–º–∞–Ω–Ω—è –¥–æ–ø–æ–º—ñ–∂–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ ---
    sku_prefix = settings['suppliers']['1']['search']                        # –ü—Ä–µ—Ñ—ñ–∫—Å –¥–ª—è –ø–æ—à—É–∫—É
    bad_words = [word.lower() for word in settings['suppliers']['1'].get('bad_words', [])]  # –ó–∞–±–æ—Ä–æ–Ω–µ–Ω—ñ —Å–ª–æ–≤–∞ (—Ñ—ñ–ª—å—Ç—Ä)
    
    # --- 5. –û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤ –Ω–æ–≤–æ–≥–æ —Ñ–∞–π–ª—É ---
    new_product_headers = [
        settings['column_supliers_1_new_name'][str(i)]
        for i in range(len(settings['column_supliers_1_new_name']))
    ]
    num_new_columns = len(new_product_headers)

    logging.info("–ó—á–∏—Ç—É—é —ñ—Å–Ω—É—é—á—ñ –∞—Ä—Ç–∏–∫—É–ª–∏ –∑ —Ñ–∞–π–ª—É, –≤–∫–∞–∑–∞–Ω–æ–≥–æ –∑–∞ –∫–ª—é—á–µ–º 'csv_path_zalishki'.")

    try:
        # --- 6. –ó—á–∏—Ç—É–≤–∞–Ω–Ω—è —ñ—Å–Ω—É—é—á–∏—Ö –∞—Ä—Ç–∏–∫—É–ª—ñ–≤ —ñ–∑ –±–∞–∑–∏ (zalishki.csv) ---
        with open(zalishki_path, mode='r', encoding='utf-8') as zalishki_file:
            zalishki_reader = csv.reader(zalishki_file)
            next(zalishki_reader, None)  # –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫
            existing_skus = {row[9].strip().lower() for row in zalishki_reader if len(row) > 9}
            logging.info(f"–ó—á–∏—Ç–∞–Ω–æ {len(existing_skus)} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –∞—Ä—Ç–∏–∫—É–ª—ñ–≤ —ñ–∑ –±–∞–∑–∏.")

        # --- 7. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –Ω–æ–≤–æ–≥–æ —Ñ–∞–π–ª—É –¥–ª—è –∑–∞–ø–∏—Å—É –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ ---
        logging.info("–í—ñ–¥–∫—Ä–∏–≤–∞—é —Ñ–∞–π–ª –¥–ª—è –∑–∞–ø–∏—Å—É –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤...")
        with open(supliers_new_path, mode='w', encoding='utf-8', newline='') as new_file:
            writer = csv.writer(new_file)
            writer.writerow(new_product_headers)  # –∑–∞–ø–∏—Å—É—î–º–æ –∑–∞–≥–æ–ª–æ–≤–∫–∏
            
            # --- 8. –ó—á–∏—Ç—É–≤–∞–Ω–Ω—è –ø—Ä–∞–π—Å-–ª–∏—Å—Ç–∞ –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ ---
            logging.info("–ü–æ—Ä—ñ–≤–Ω—é—é –¥–∞–Ω—ñ –∑ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç–æ–º –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ 1...")
            with open(supliers_csv_path, mode='r', encoding='utf-8') as supliers_file:
                supliers_reader = csv.reader(supliers_file, delimiter=delimiter)
                next(supliers_reader, None)  # –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫
                
                # --- 9. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ª—ñ—á–∏–ª—å–Ω–∏–∫—ñ–≤ ---
                new_products_count = 0
                filtered_out_count = 0

                # --- 10. –ì–æ–ª–æ–≤–Ω–∏–π —Ü–∏–∫–ª: –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∫–æ–∂–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä—É ---
                for row in supliers_reader:
                    if not row:
                        continue
                    
                    sku = row[0].strip().lower()
                    
                    # --- 11. –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Ç–æ–≤–∞—Ä –Ω–æ–≤–∏–π (–≤—ñ–¥—Å—É—Ç–Ω—ñ–π —É –±–∞–∑—ñ) ---
                    if sku and sku not in existing_skus:
                        
                        # --- 12. –§–æ—Ä–º—É—î–º–æ –Ω–æ–≤–∏–π —Ä—è–¥–æ–∫ –∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ—é SL_new.csv ---
                        new_row = [''] * num_new_columns
                        
                        # –î–æ–¥–∞—î–º–æ –ø—Ä–µ—Ñ—ñ–∫—Å –¥–æ SKU
                        sku_with_prefix = sku_prefix + row[0]
                        new_row[0] = sku_with_prefix

                        # --- 13. –ú–∞–ø—É–≤–∞–Ω–Ω—è –∫–æ–ª–æ–Ω–æ–∫ –∑ –ø—Ä–∞–π—Å—É —É –Ω–æ–≤–∏–π CSV ---
                        column_mapping = [
                            (0, 5),   # a(0) -> f(5)
                            (1, 6),   # b(1) -> g(6)
                            (2, 7),   # c(2) -> h(7)
                            (3, 8),   # d(3) -> i(8)
                            (6, 9),   # g(6) -> j(9)
                            (7, 10),  # h(7) -> k(10)
                            (8, 11),  # i(8) -> l(11)
                            (9, 12),  # j(9) -> m(12)
                            (10, 13), # k(10) -> n(13)
                            (11, 14), # l(11) -> o(14)
                        ]
                        for source_index, dest_index in column_mapping:
                            if len(row) > source_index:
                                new_row[dest_index] = row[source_index]
                                
                        # --- 14. –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –∑–∞–±–æ—Ä–æ–Ω–µ–Ω–∏—Ö —Å–ª—ñ–≤ ---
                        should_skip = False
                        check_columns_indices = [6, 7, 10]  # –∫–æ–ª–æ–Ω–∫–∏, –¥–µ —à—É–∫–∞—î–º–æ –∑–∞–±–æ—Ä–æ–Ω–µ–Ω—ñ —Å–ª–æ–≤–∞
                        
                        for index in check_columns_indices:
                            if len(new_row) > index:
                                cell_content = new_row[index].lower()
                                for bad_word in bad_words:
                                    if bad_word in cell_content:
                                        logging.info(
                                            f"–ü—Ä–æ–ø—É—Å–∫–∞—é —Ç–æ–≤–∞—Ä '{row[0]}' —á–µ—Ä–µ–∑ —Å–ª–æ–≤–æ '{bad_word}' "
                                            f"–≤ –∫–æ–ª–æ–Ω—Ü—ñ {index} –Ω–æ–≤–æ–≥–æ —Ñ–∞–π–ª—É."
                                        )
                                        should_skip = True
                                        filtered_out_count += 1
                                        break
                                if should_skip:
                                    break
                        
                        # --- 15. –Ø–∫—â–æ —Ç–æ–≤–∞—Ä –º–∞—î –∑–∞–±–æ—Ä–æ–Ω–µ–Ω–µ —Å–ª–æ–≤–æ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ ---
                        if should_skip:
                            continue
                        
                        # --- 16. –Ø–∫—â–æ –Ω—ñ ‚Äî –¥–æ–¥–∞—î–º–æ —É —Ñ–∞–π–ª –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ ---
                        new_products_count += 1
                        writer.writerow(new_row)

        # --- 17. –ü—ñ–¥—Å—É–º–∫–æ–≤–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è ---
        logging.info(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {new_products_count} –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤.")
        logging.info(f"üö´ –í—ñ–¥—Ñ—ñ–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {filtered_out_count} —Ç–æ–≤–∞—Ä—ñ–≤ –∑–∞ –∑–∞–±–æ—Ä–æ–Ω–µ–Ω–∏–º–∏ —Å–ª–æ–≤–∞–º–∏.")
        logging.info(f"–î–∞–Ω—ñ –∑–∞–ø–∏—Å–∞–Ω–æ —É —Ñ–∞–π–ª csv 'supliers_new_path'.")

    # --- 18. –û–±—Ä–æ–±–∫–∞ –ø–æ–º–∏–ª–æ–∫ ---
    except FileNotFoundError as e:
        logging.info(f"‚ùå –ü–æ–º–∏–ª–∫–∞: –§–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ - {e}")
    except Exception as e:
        logging.info(f"‚ùå –í–∏–Ω–∏–∫–ª–∞ –Ω–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")

def find_product_data():
    """
    –ó—á–∏—Ç—É—î —Ñ–∞–π–ª –∑ –Ω–æ–≤–∏–º–∏ —Ç–æ–≤–∞—Ä–∞–º–∏, –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç—å –∑–∞ URL-–∞–¥—Ä–µ—Å–æ—é,
    –∑–Ω–∞—Ö–æ–¥–∏—Ç—å URL-–∞–¥—Ä–µ—Å—É –ø—Ä–æ—Å—Ç–æ–≥–æ –∞–±–æ –≤–∞—Ä—ñ–∞—Ç–∏–≤–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä—É,
    —ñ –∑–∞–ø–∏—Å—É—î –∑–Ω–∞–π–¥–µ–Ω—É URL-–∞–¥—Ä–µ—Å—É –≤ –∫–æ–ª–æ–Ω–∫—É B(1) –≤ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª.
    """

    # --- 1. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ª–æ–≥—É–≤–∞–Ω–Ω—è (–ø—ñ–¥–∫–ª—é—á–∞—î–º–æ —ñ—Å–Ω—É—é—á–∏–π –ª–æ–≥-—Ñ–∞–π–ª) ---
    log_message_to_existing_file()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 2. –ü–æ—á–∏–Ω–∞—é –ø–æ—à—É–∫ URL-–∞–¥—Ä–µ—Å —Ç–æ–≤–∞—Ä—ñ–≤...")
    
    # --- 2. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å —Ç–∞ —Ñ–æ—Ä–º—É–≤–∞–Ω–Ω—è —à–ª—è—Ö—ñ–≤/—Ç–∏–º—á–∞—Å–æ–≤–æ–≥–æ —Ñ–∞–π–ª—É ---
    settings = load_settings()
    supliers_new_path = settings['paths']['csv_path_supliers_1_new']  # –≤—Ö—ñ–¥–Ω–∏–π CSV (1.csv)
    site_url = settings['suppliers']['1']['site']                    # –±–∞–∑–æ–≤–∏–π URL —Å–∞–π—Ç—É (—â–æ–± –¥–æ–¥–∞–≤–∞—Ç–∏ –≤—ñ–¥–Ω–æ—Å–Ω—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è)
    temp_file_path = supliers_new_path + '.temp'                     # —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª –ø—ñ–¥ —á–∞—Å –∑–∞–ø–∏—Å—É

    # --- –õ—ñ—á–∏–ª—å–Ω–∏–∫–∏ —ñ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ---
    total_rows = 0
    found_variant_count = 0
    found_simple_count = 0
    not_found_count = 0
    found_variant_rows = []
    not_found_rows = []

    try:
        # --- 3. –í—ñ–¥–∫—Ä–∏–≤–∞—î–º–æ –≤—Ö—ñ–¥–Ω–∏–π —Ñ–∞–π–ª –¥–ª—è —á–∏—Ç–∞–Ω–Ω—è ---
        with open(supliers_new_path, mode='r', encoding='utf-8') as input_file:
            reader = csv.reader(input_file)
            headers = next(reader)  # —á–∏—Ç–∞—î–º–æ —ñ –∑–±–µ—Ä—ñ–≥–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–∫–∏ (—â–æ–± –ø–µ—Ä–µ–ø–∏—Å–∞—Ç–∏ –≤ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª)

            # --- 4. –í—ñ–¥–∫—Ä–∏–≤–∞—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª –¥–ª—è –ø–æ—Å—Ç—É–ø–æ–≤–æ–≥–æ –∑–∞–ø–∏—Å—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ ---
            with open(temp_file_path, mode='w', encoding='utf-8', newline='') as output_file:
                writer = csv.writer(output_file)
                writer.writerow(headers) # –∑–∞–ø–∏—Å—É—î–º–æ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —É —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª

                # --- 5. –Ü—Ç–µ—Ä–∞—Ü—ñ—è –ø–æ —Ä—è–¥–∫–∞—Ö –≤—Ö—ñ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª—É ---
                for idx, row in enumerate(reader):
                    total_rows += 1
                    # 5.1. –í–∏—Ç—è–≥—É—î–º–æ –∫–ª—é—á–æ–≤—ñ –ø–æ–ª—è —ñ–∑ —Ä—è–¥–∫–∞
                    search_url = row[0].strip()    # —É –≤–∏—Ö—ñ–¥–Ω–æ–º—É —Ñ–∞–π–ª—ñ —É –∫–æ–ª–æ–Ω—Ü—ñ A –º–æ–∂–µ –±—É—Ç–∏ "–ø–æ—Å–∏–ª–∞–Ω–Ω—è –¥–ª—è –ø–æ—à—É–∫—É"
                    file_sku = row[5].strip()      # –∞—Ä—Ç–∏–∫—É–ª (SKU) –∑ –∫–æ–ª–æ–Ω–∫–∏, —è–∫–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î —ñ–Ω–¥–µ–∫—Å—É 5

                    # --- 6. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –≤–∞–ª—ñ–¥–Ω–æ—Å—Ç—ñ URL –¥–ª—è –ø–æ—à—É–∫—É ---
                    # –Ø–∫—â–æ URL –ø—É—Å—Ç–∏–π –∞–±–æ –≤–∂–µ –ø–æ–∑–Ω–∞—á–µ–Ω–∏–π —è–∫ –ø–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Ç—É, –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ —Ä—è–¥–æ–∫
                    if not search_url or search_url.startswith('–ü–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Ç—É'):
                        writer.writerow(row)
                        continue

                    try:
                        # --- 7. –í–∏–∫–æ–Ω–∞–Ω–Ω—è HTTP-–∑–∞–ø–∏—Ç—É –¥–æ search_url —ñ –ø–∞—Ä—Å–∏–Ω–≥ HTML ---
                        response = requests.get(search_url)
                        response.raise_for_status()
                        soup = BeautifulSoup(response.text, 'html.parser')
                        found_type = None  # 'variant' –∞–±–æ 'simple'
                        found_url = None  # —Å—é–¥–∏ –∑–∞–ø–∏—à–µ–º–æ –∑–Ω–∞–π–¥–µ–Ω—É —Ä–µ–∞–ª—å–Ω—É URL-–∞–¥—Ä–µ—Å—É —Ç–æ–≤–∞—Ä—É
                        
                        # --- 8. –ü–æ—à—É–∫ –≤–∞—Ä—ñ–∞—Ç–∏–≤–Ω–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ (input.variant_control[data-code]) ---
                        # –®—É–∫–∞—î–º–æ input —Ç–µ–≥–∏ –∑ –∫–ª–∞—Å–æ–º variant_control —Ç–∞ –∞—Ç—Ä–∏–±—É—Ç–æ–º data-code,
                        # –ø–æ—Ä—ñ–≤–Ω—é—î–º–æ data-code –∑ file_sku ‚Äî —è–∫—â–æ —Å–ø—ñ–≤–ø–∞–¥—ñ–Ω–Ω—è, –±–µ—Ä–µ–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è —É –±–∞—Ç—å–∫—ñ–≤—Å—å–∫–æ–º—É –±–ª–æ—Ü—ñ.
                        variant_inputs = soup.find_all('input', class_='variant_control', attrs={'data-code': True})
                        for input_tag in variant_inputs:
                            site_sku = input_tag.get('data-code', '').strip()
                            if file_sku == site_sku:
                                parent_div = input_tag.find_parent('div', class_='card-block')
                                if parent_div:
                                    link_tag = parent_div.find('h4', class_='card-title').find('a')
                                    if link_tag and link_tag.has_attr('href'):
                                        # –§–æ—Ä–º—É—î–º–æ –ø–æ–≤–Ω–∏–π URL (–¥–æ–¥–∞—î–º–æ site_url –¥–æ –≤—ñ–¥–Ω–æ—Å–Ω–æ–≥–æ —à–ª—è—Ö—É)
                                        found_url = site_url + link_tag['href']
                                        found_type = 'variant'
                                        break

                        # --- 9. –Ø–∫—â–æ –Ω–µ –∑–Ω–∞–π—à–ª–∏ —Å–µ—Ä–µ–¥ –≤–∞—Ä—ñ–∞–Ω—Ç—ñ–≤ ‚Äî —à—É–∫–∞—î–º–æ –ø—Ä–æ—Å—Ç—ñ —Ç–æ–≤–∞—Ä–∏ ---
                        if not found_url:
                            # –î–ª—è –ø—Ä–æ—Å—Ç–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ —à—É–∫–∞—î–º–æ div –∑ –∫–ª–∞—Å–æ–º 'radio', –±–µ—Ä–µ–º–æ —Ç–µ–∫—Å—Ç —è–∫ SKU,
                            # —ñ –∑–∞ —Ç–∞–∫–∏–º –∂–µ –ø—ñ–¥—Ö–æ–¥–æ–º –∑–Ω–∞—Ö–æ–¥–∏–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è —É –±–ª–æ—Ü—ñ card-block.
                            simple_divs = soup.find_all('div', class_='radio')
                            for div_tag in simple_divs:
                                site_sku = div_tag.get_text(strip=True).strip()
                                if file_sku == site_sku:
                                    parent_div = div_tag.find_parent('div', class_='card-block')
                                    if parent_div:
                                        link_tag = parent_div.find('h4', class_='card-title').find('a')
                                        if link_tag and link_tag.has_attr('href'):
                                            found_url = site_url + link_tag['href']
                                            found_type = 'simple'
                                            break

                        # --- 10. –ó–∞–ø–∏—Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É —É –∫–æ–ª–æ–Ω–∫—É B (—ñ–Ω–¥–µ–∫—Å 1) –∞–±–æ –ª–æ–≥—É–≤–∞–Ω–Ω—è —è–∫—â–æ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ ---
                        if found_url:
                            row[1] = found_url
                            if found_type == 'variant':
                                found_variant_count += 1
                                found_variant_rows.append(idx + 2)  # +2, –±–æ —Ä—è–¥–∫–∏ CSV —Ä–∞—Ö—É—é—Ç—å—Å—è –∑ 1 + –∑–∞–≥–æ–ª–æ–≤–æ–∫
                            elif found_type == 'simple':
                                found_simple_count += 1
                        else:
                            not_found_count += 1
                            not_found_rows.append(idx + 2)

                        # –ó–∞–ø–∏—Å—É—î–º–æ (–∑–Ω–∞–π–¥–µ–Ω–∏–π –∞–±–æ –Ω–µ–∑–º—ñ–Ω–µ–Ω–∏–π) —Ä—è–¥–æ–∫ —É —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª
                        writer.writerow(row)

                    except requests.RequestException as e:
                        # --- 11. –û–±—Ä–æ–±–∫–∞ –ø–æ–º–∏–ª–æ–∫ HTTP-–∑–∞–ø–∏—Ç—É: –ª–æ–≥—É–≤–∞–Ω–Ω—è —Ç–∞ –º–∞—Ä–∫—É–≤–∞–Ω–Ω—è —Ä—è–¥–∫–∞ ---
                        logging.error(f"–†—è–¥–æ–∫ {idx + 2}: –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Ç—ñ –¥–æ —É—Ä–ª: {e}")
                        row[0] = f'–ü–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Ç—É: {e}'  # –ø–æ–∑–Ω–∞—á–∞—î–º–æ –ø–æ–ª–µ –ø–æ—à—É–∫—É —è–∫ –ø–æ–º–∏–ª–∫–æ–≤–µ
                        writer.writerow(row)
                    
                    # --- 12. –î–æ–¥–∞—Ç–∫–æ–≤–∞ –ø–∞—É–∑–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏ (—Ä–∞–Ω–¥–æ–º—ñ–∑–æ–≤–∞–Ω–∞) –¥–ª—è —É–Ω–∏–∫–Ω–µ–Ω–Ω—è –±–∞–Ω–∞/DDOS ---
                    time.sleep(random.uniform(1, 3))
  
        # --- 13. –ü—ñ—Å–ª—è —É—Å–ø—ñ—à–Ω–æ—ó –æ–±—Ä–æ–±–∫–∏: –∑–∞–º—ñ–Ω–∞ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª—É —Ç–∏–º—á–∞—Å–æ–≤–∏–º ---
        os.replace(temp_file_path, supliers_new_path)

        # --- 14. –ó–≤–µ–¥–µ–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ---
        logging.info("=== –ü–Ü–î–°–£–ú–ö–û–í–ê –Ü–ù–§–û–†–ú–ê–¶–Ü–Ø ===")
        logging.info(f"–í—Å—å–æ–≥–æ —Ä—è–¥–∫—ñ–≤ –∑ —Ç–æ–≤–∞—Ä–∞–º–∏: {total_rows}")
        logging.info(
            f"–ó–Ω–∞–π–¥–µ–Ω–æ URL –≤–∞—Ä—ñ–∞—Ç–∏–≤–Ω–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤: {found_variant_count}"
            + (f" (–†—è–¥–∫–∏ {', '.join(map(str, found_variant_rows))})" if found_variant_rows else "")
        )
        logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ URL –ø—Ä–æ—Å—Ç–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤: {found_simple_count}")
        logging.info(
            f"–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ URL: {not_found_count}"
            + (f" (–†—è–¥–∫–∏ {', '.join(map(str, not_found_rows))})" if not_found_rows else "")
        )

    except FileNotFoundError as e:
        # --- 15. –û–±—Ä–æ–±–∫–∞ –ø–æ–º–∏–ª–∫–∏: –≤—Ö—ñ–¥–Ω–∏–π —Ñ–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ ---
        logging.error(f"–ü–æ–º–∏–ª–∫–∞: –§–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ - {e}")
    except Exception as e:
        # --- 16. –ì–∞—Ä–∞–Ω—Ç—ñ–π–Ω–µ –ø—Ä–∏–±–∏—Ä–∞–Ω–Ω—è: –≤–∏–¥–∞–ª—è—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª –ø—Ä–∏ –ø–æ–º–∏–ª—Ü—ñ, —â–æ–± –Ω–µ –∑–∞–ª–∏—à–∏—Ç–∏ —Å–º—ñ—Ç—Ç—è ---
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)
        logging.error(f"–í–∏–Ω–∏–∫–ª–∞ –Ω–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")

def parse_product_attributes():
    """
    –ü–∞—Ä—Å–∏—Ç—å —Å—Ç–æ—Ä—ñ–Ω–∫–∏ —Ç–æ–≤–∞—Ä—ñ–≤, –∑–∞—Å—Ç–æ—Å–æ–≤—É—î –∑–∞–º—ñ–Ω—É –∑ attribute.csv (–±–ª–æ—á–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞) 
    —ñ –¥–æ–¥–∞—î –Ω–æ–≤—ñ –Ω–µ–≤—ñ–¥–æ–º—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –æ–¥—Ä–∞–∑—É –ø–µ—Ä–µ–¥ –Ω–∞—Å—Ç—É–ø–Ω–∏–º –±–ª–æ–∫–æ–º-–∑–∞–≥–æ–ª–æ–≤–∫–æ–º.
    –õ–æ–≥—É–≤–∞–Ω–Ω—è –≤–∫–ª—é—á–∞—î –ø—ñ–¥—Å—É–º–æ–∫ –¥–æ–¥–∞–Ω–∏—Ö –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ –ø–æ –∫–æ–ª–æ–Ω–∫–∞—Ö.
    """
    log_message_to_existing_file()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 3. –ü–æ—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç–æ—Ä—ñ–Ω–æ–∫ —Ç–æ–≤–∞—Ä—ñ–≤ –¥–ª—è –≤–∏–ª—É—á–µ–Ω–Ω—è –∞—Ç—Ä–∏–±—É—Ç—ñ–≤...")

    # --- 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å ---
    settings = load_settings()
    try:
        supliers_new_path = settings['paths']['csv_path_supliers_1_new']
        product_data_map = settings['suppliers']['1']['product_data_columns']
        other_attrs_index = settings['suppliers']['1']['other_attributes_column']
    except (TypeError, KeyError) as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –¥–æ—Å—Ç—É–ø—É –¥–æ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å. –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ settings.json: {e}")
        return

    # --- 2. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–∞–ø–∏ –¥–ª—è –æ–±—Ä–æ–±–∫–∏ (–±–µ–∑ –®—Ç—Ä–∏—Ö-–∫–æ–¥—É) ---
    processing_map = {k: v for k, v in product_data_map.items() if k != "–®—Ç—Ä–∏—Ö-–∫–æ–¥"}

    # --- 3. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø—Ä–∞–≤–∏–ª –∑–∞–º—ñ–Ω–∏ —Ç–∞ —Å–∏—Ä–∏—Ö –¥–∞–Ω–∏—Ö ---
    replacements_map, raw_data = load_attributes_csv()
    changes_made = False
    max_raw_row_len = len(raw_data[0]) if raw_data and raw_data[0] else 10

    # --- 4. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–æ—á–æ–∫ –≤—Å—Ç–∞–≤–∫–∏ –Ω–æ–≤–∏—Ö –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ ---
    insertion_points = {}
    current_col_index = None
    for i, row in enumerate(raw_data[1:], start=1):
        if row and row[0].strip().isdigit():
            col_index = int(row[0].strip())
            if current_col_index is not None and current_col_index not in insertion_points:
                insertion_points[current_col_index] = i
            current_col_index = col_index
            insertion_points[col_index] = i + 1
        elif current_col_index is not None:
            insertion_points[current_col_index] = i + 1

    logging.debug(f"–¢–æ—á–∫–∏ –≤—Å—Ç–∞–≤–∫–∏ (insertion_points): {insertion_points}")

    # --- 5. –°–ª–æ–≤–Ω–∏–∫ –¥–ª—è –ø—ñ–¥—Ä–∞—Ö—É–Ω–∫—É –Ω–æ–≤–∏—Ö –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ –ø–æ –∫–æ–ª–æ–Ω–∫–∞—Ö ---
    new_attributes_counter = {}  # {col_index: count}

    # --- 5.1 –°–ø–∏—Å–æ–∫ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—é —Ä—è–¥–∫—ñ–≤ –±–µ–∑ —à—Ç—Ä–∏—Ö–∫–æ–¥—ñ–≤ ---
    missing_shk_rows = []  # [—Ä—è–¥–æ–∫_—É_csv]

    # --- 6. –û–±—Ä–æ–±–∫–∞ CSV –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ ---
    temp_file_path = supliers_new_path + '.temp'
    try:
        with open(supliers_new_path, mode='r', encoding='utf-8') as input_file, \
             open(temp_file_path, mode='w', encoding='utf-8', newline='') as output_file:

            reader = csv.reader(input_file)
            writer = csv.writer(output_file)
            headers = next(reader)
            writer.writerow(headers)

            for idx, row in enumerate(reader):
                product_url = row[1].strip()
                file_sku = row[5].strip()

                # –†–æ–∑—à–∏—Ä–µ–Ω–Ω—è —Ä—è–¥–∫–∞, —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
                max_index = max(max(product_data_map.values(), default=0), other_attrs_index)
                if len(row) <= max_index:
                    row.extend([''] * (max_index + 1 - len(row)))

                if not product_url or product_url.startswith('–ü–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Ç—É'):
                    writer.writerow(row)
                    continue

                # --- 6.1 –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ ---
                try:
                    response = requests.get(product_url)
                    response.raise_for_status()
                    soup = BeautifulSoup(response.text, 'html.parser')
                    characteristics_div = soup.find('div', id='w0-tab0')
                    parsed_attributes = {}
                    if characteristics_div and characteristics_div.find('table'):
                        for tr in characteristics_div.find('table').find_all('tr'):
                            cells = tr.find_all('td')
                            if len(cells) == 2:
                                key = cells[0].get_text(strip=True).replace(':', '')
                                value = cells[1].get_text(strip=True)
                                parsed_attributes[key] = value

                    other_attributes = []

                    # --- 6.2 –û–±—Ä–æ–±–∫–∞ –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ ---
                    for attr_name, attr_value in parsed_attributes.items():
                        target_col_index = processing_map.get(attr_name)
                        original_value_lower = attr_value.strip().lower()

                        if target_col_index is not None:
                            replacement_rules = replacements_map.get(target_col_index, {})
                            new_value = replacement_rules.get(original_value_lower)

                            if new_value is not None and new_value != "":
                                row[target_col_index] = new_value
                            else:
                                if original_value_lower not in replacement_rules:
                                    insert_index = insertion_points.get(target_col_index)
                                    if insert_index is None:
                                        logging.error(f"–ê—Ç—Ä–∏–±—É—Ç '{attr_value}' (I={target_col_index}) –Ω–µ –¥–æ–¥–∞–Ω–æ: –≤—ñ–¥—Å—É—Ç–Ω—è —Ç–æ—á–∫–∞ –≤—Å—Ç–∞–≤–∫–∏.")
                                        row[target_col_index] = attr_value
                                        continue

                                    # –î–æ–¥–∞—î–º–æ –Ω–æ–≤–∏–π –∞—Ç—Ä–∏–±—É—Ç —É raw_data
                                    new_raw_row = [''] * max_raw_row_len
                                    new_raw_row[2] = original_value_lower
                                    raw_data.insert(insert_index, new_raw_row)
                                    replacements_map.setdefault(target_col_index, {})[original_value_lower] = ""
                                    changes_made = True

                                    # –ó—Å—É–≤–∞—î–º–æ —Ç–æ—á–∫–∏ –≤—Å—Ç–∞–≤–∫–∏
                                    for col, point in insertion_points.items():
                                        if point >= insert_index:
                                            insertion_points[col] += 1

                                    # –ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ –Ω–æ–≤–∏—Ö –∞—Ç—Ä–∏–±—É—Ç—ñ–≤
                                    new_attributes_counter[target_col_index] = new_attributes_counter.get(target_col_index, 0) + 1

                                row[target_col_index] = attr_value

                        elif attr_name == "–®—Ç—Ä–∏—Ö-–∫–æ–¥":
                            shk_index = product_data_map.get("–®—Ç—Ä–∏—Ö-–∫–æ–¥")
                            if shk_index is not None:
                                row[shk_index] = attr_value.strip()

                        else:
                            other_attributes.append(f"{attr_name}:{attr_value}")

                    # --- 6.3 –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ —à—Ç—Ä–∏—Ö–∫–æ–¥—É ---
                    shk_index = product_data_map.get("–®—Ç—Ä–∏—Ö-–∫–æ–¥")
                    if shk_index is not None:
                        if not row[shk_index].strip():
                            missing_shk_rows.append(idx + 2)  # +2, –±–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ = —Ä—è–¥–æ–∫ 1

                    if other_attributes:
                        row[other_attrs_index] = ', '.join(other_attributes)

                    writer.writerow(row)

                except requests.RequestException as req_err:
                    logging.error(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Ç—É –¥–ª—è URL {product_url}: {req_err}")
                    writer.writerow(row)
                except Exception as e:
                    logging.error(f"–ù–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É –¥–ª—è URL {product_url}: {e}")
                    writer.writerow(row)

                time.sleep(random.uniform(1, 3))

        os.replace(temp_file_path, supliers_new_path)
        logging.info("–ü–∞—Ä—Å–∏–Ω–≥ –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –§–∞–π–ª 1.csv –æ–Ω–æ–≤–ª–µ–Ω–æ.")

        # --- 7. –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è attribute.csv —Ç–∞ –ø—ñ–¥—Å—É–º–∫–æ–≤–∏–π –ª–æ–≥ ---
        if changes_made:
            save_attributes_csv(raw_data)
        else:
            logging.info("–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è attribute.csv –Ω–µ –ø–æ—Ç—Ä—ñ–±–Ω–µ. –ó–º—ñ–Ω: False.")

        # --- 7.1 –ü—ñ–¥—Å—É–º–∫–æ–≤–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è –Ω–æ–≤–∏—Ö –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ ---
        if new_attributes_counter:
            logging.info("–ü—ñ–¥—Å—É–º–æ–∫ –¥–æ–¥–∞–Ω–∏—Ö –Ω–æ–≤–∏—Ö –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ –ø–æ –∫–æ–ª–æ–Ω–∫–∞—Ö:")
            for col_index, count in sorted(new_attributes_counter.items()):
                logging.info(f"–ê—Ç—Ä–∏–±—É—Ç {col_index}, –¥–æ–¥–∞–Ω–æ {count} –Ω–æ–≤–∏—Ö –∞—Ç—Ä–∏–±—É—Ç—ñ–≤")
        else:
            logging.info("–ù–æ–≤—ñ –∞—Ç—Ä–∏–±—É—Ç–∏ –Ω–µ –¥–æ–¥–∞–Ω—ñ —É –∂–æ–¥–Ω—É –∫–æ–ª–æ–Ω–∫—É.")

        # --- 7.2 –õ–æ–≥—É–≤–∞–Ω–Ω—è –≤—ñ–¥—Å—É—Ç–Ω—ñ—Ö —à—Ç—Ä–∏—Ö–∫–æ–¥—ñ–≤ ---
        if missing_shk_rows:
            rows_str = ', '.join(map(str, missing_shk_rows))
            logging.warning(f"–£–í–ê–ì–ê! –ù–µ–º–∞—î —à—Ç—Ä–∏—Ö–∫–æ–¥—ñ–≤: {len(missing_shk_rows)} —à—Ç—É–∫–∏ (—Ä—è–¥–∫–∏ {rows_str})")
        else:
            logging.info("–£—Å—ñ —Ç–æ–≤–∞—Ä–∏ –º–∞—é—Ç—å —à—Ç—Ä–∏—Ö–∫–æ–¥–∏.")

    except Exception as e:
        logging.error(f"–í–∏–Ω–∏–∫–ª–∞ –Ω–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)

def apply_final_standardization():
    """
    –ó–∞—Å—Ç–æ—Å–æ–≤—É—î —Ñ—ñ–Ω–∞–ª—å–Ω—ñ –ø—Ä–∞–≤–∏–ª–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü—ñ—ó –∑ attribute.csv –¥–æ —Ñ–∞–π–ª—É 1.csv.
    –ó–∞–º—ñ–Ω—é—î –∞—Ç—Ä–∏–±—É—Ç–∏ –Ω–∞ –∑–Ω–∞—á–µ–Ω–Ω—è –∑ –∫–æ–ª–æ–Ω–∫–∏ 'attr_site_name', —è–∫—â–æ –≤–æ–Ω–æ —ñ—Å–Ω—É—î.
    –ü—Ä–æ—ñ–≥–Ω–æ—Ä–æ–≤–∞–Ω—ñ –∞—Ç—Ä–∏–±—É—Ç–∏ (–∑ –ø–æ—Ä–æ–∂–Ω—ñ–º 'attr_site_name') –æ—á–∏—â–∞—é—Ç—å—Å—è.
    –ê—Ç—Ä–∏–±—É—Ç–∏, –¥–ª—è —è–∫–∏—Ö –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –ø—Ä–∞–≤–∏–ª, –∑–∞–ª–∏—à–∞—é—Ç—å—Å—è –±–µ–∑ –∑–º—ñ–Ω.
    –õ–æ–≥—É–≤–∞–Ω–Ω—è –≤–∫–ª—é—á–∞—î —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–º—ñ–Ω —Ç–∞ –æ—á–∏—â–µ–Ω—å.
    """
    log_message_to_existing_file()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 4. –ü–æ—á–∏–Ω–∞—é —Ñ—ñ–Ω–∞–ª—å–Ω—É —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü—ñ—é –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ —É 1.csv...")

    # --- 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å ---
    settings = load_settings()
    try:
        csv_path = settings['paths']['csv_path_supliers_1_new']
        product_map = settings['suppliers']['1']['product_data_columns']
    except TypeError as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –¥–æ—Å—Ç—É–ø—É –¥–æ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å: {e}")
        return

    # --- 2. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–∞–ø–∏ –¥–ª—è –æ–±—Ä–æ–±–∫–∏ (–±–µ–∑ –®—Ç—Ä–∏—Ö-–∫–æ–¥—É) ---
    processing_map = {k: v for k, v in product_map.items() if k != "–®—Ç—Ä–∏—Ö-–∫–æ–¥"}

    # --- 3. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø—Ä–∞–≤–∏–ª –∑–∞–º—ñ–Ω–∏ ---
    replacements_map, _ = load_attributes_csv()

    # --- 4. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∑–∞–º—ñ–Ω ---
    replacement_counter = {}  # {col_index: count}
    cleared_counter = {}      # {col_index: count}

    # --- 5. –û–±—Ä–æ–±–∫–∞ CSV ---
    temp_file_path = csv_path + '.final_temp'
    try:
        with open(csv_path, 'r', encoding='utf-8') as infile, \
             open(temp_file_path, 'w', encoding='utf-8', newline='') as outfile:

            reader = csv.reader(infile)
            writer = csv.writer(outfile)
            headers = next(reader)
            writer.writerow(headers)

            # –°–ª–æ–≤–Ω–∏–∫ –¥–ª—è –ª–æ–≥—É–≤–∞–Ω–Ω—è –Ω–∞–∑–≤ –∫–æ–ª–æ–Ω–æ–∫
            column_names = {index: name for name, index in processing_map.items()}

            for idx, row in enumerate(reader):
                max_index = max(product_map.values(), default=0)
                if len(row) <= max_index:
                    row.extend([''] * (max_index + 1 - len(row)))

                for col_index, rules in replacements_map.items():
                    if col_index >= len(row):
                        continue

                    current_value = row[col_index].strip()
                    if not current_value:
                        continue

                    current_lower = current_value.lower()
                    col_name = column_names.get(col_index, f"I={col_index}")
                    new_value = rules.get(current_lower)

                    if new_value is not None:
                        if new_value:
                            if new_value != current_value:
                                row[col_index] = new_value
                                replacement_counter[col_index] = replacement_counter.get(col_index, 0) + 1
                                logging.info(f"–†—è–¥–æ–∫ {idx + 2}: –ó–ê–ú–Ü–ù–ê ({col_name}): '{current_value}' -> '{new_value}'")
                        else:
                            row[col_index] = ""
                            cleared_counter[col_index] = cleared_counter.get(col_index, 0) + 1
                            logging.warning(f"–†—è–¥–æ–∫ {idx + 2}: –Ü–ì–ù–û–†–£–í–ê–ù–ù–Ø/–û–ß–ò–©–ï–ù–ù–Ø ({col_name}): '{current_value}' –æ—á–∏—â–µ–Ω–æ")

                writer.writerow(row)

        os.replace(temp_file_path, csv_path)
        logging.info("–§—ñ–Ω–∞–ª—å–Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü—ñ—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. csv –æ–Ω–æ–≤–ª–µ–Ω–æ.")

        # --- 6. –ü—ñ–¥—Å—É–º–∫–æ–≤–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è ---
        if replacement_counter:
            for col, count in sorted(replacement_counter.items()):
                logging.info(f"–ê—Ç—Ä–∏–±—É—Ç {col}: –≤–∏–∫–æ–Ω–∞–Ω–æ {count} –∑–∞–º—ñ–Ω")
        if cleared_counter:
            for col, count in sorted(cleared_counter.items()):
                logging.info(f"–ê—Ç—Ä–∏–±—É—Ç {col}: –æ—á–∏—â–µ–Ω–æ {count} –∑–Ω–∞—á–µ–Ω—å")

    except FileNotFoundError as e:
        logging.error(f"–§–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {e}")
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)
    except Exception as e:
        logging.error(f"–ù–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü—ñ—ó: {e}")
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)

def fill_product_category():
    """
    –ó–∞–ø–æ–≤–Ω—é—î —Å–ª—É–∂–±–æ–≤—ñ –∫–æ–ª–æ–Ω–∫–∏ —É csv:
    - Q (–∫–∞—Ç–µ–≥–æ—Ä—ñ—è) –Ω–∞ –æ—Å–Ω–æ–≤—ñ M, N, O
    - T (–ø–æ–∑–Ω–∞—á–∫–∏) –Ω–∞ –æ—Å–Ω–æ–≤—ñ –Ω–∞–∑–≤–∏ —Ç–æ–≤–∞—Ä—É G
    - U (Rank Math) –Ω–∞ –æ—Å–Ω–æ–≤—ñ –Ω–∞–∑–≤–∏ —Ç–æ–≤–∞—Ä—É G
    - AV (pa_used) –Ω–∞ –æ—Å–Ω–æ–≤—ñ category.csv
    - V, W, X, Y, AZ —Ñ—ñ–∫—Å–æ–≤–∞–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏
    - Z (–∫–æ—Ä–æ—Ç–∫–∏–π –æ–ø–∏—Å) –∑ H
    - AX (–¥–∞—Ç–∞)
    –ü—Ä–∞—Ü—é—î —Ç—ñ–ª—å–∫–∏ –¥–ª—è –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ –∑ ID=1
    """
    log_message_to_existing_file()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 5. –ü–æ—á–∏–Ω–∞—é –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó —Ç–∞ —Å–ª—É–∂–±–æ–≤–∏—Ö –∫–æ–ª–æ–Ω–æ–∫...")

    settings = load_settings()
    try:
        csv_path = settings['paths']['csv_path_supliers_1_new']
        supplier_id = 1
        name_ukr = settings['suppliers']['1']['name_ukr']
    except (TypeError, KeyError) as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å: {e}")
        return

    # –Ü–Ω–¥–µ–∫—Å–∏ –∫–æ–ª–æ–Ω–æ–∫
    M, N, O = 12, 13, 14
    G, H = 6, 7
    Q, T, U = 16, 19, 20
    Z, V, W, X, Y = 25, 21, 22, 23, 24
    AV, AX, AZ = 47, 49, 51

    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø—Ä–∞–≤–∏–ª –∫–∞—Ç–µ–≥–æ—Ä—ñ–π —ñ –ø–æ–∑–Ω–∞—á–æ–∫
    category_map, raw_category = load_category_csv()
    rules_category = category_map.get(supplier_id, {})
    poznachky_list = load_poznachky_csv()
    changes_category = False
    max_row_len_category = len(raw_category[0]) if raw_category else 5

    # –°—Ç–≤–æ—Ä—é—î–º–æ –º–∞–ø—É –¥–ª—è pa_used
    pa_used_map = {}
    for row in raw_category:
        if len(row) > 5 and (row[0].strip() == str(supplier_id) or row[0].strip() == ''):
            key = tuple(v.strip().lower() for v in row[1:4])
            pa_used_map[key] = row[5].strip()

    logging.info(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(pa_used_map)} –ø—Ä–∞–≤–∏–ª pa_used")

    current_date = datetime.now().strftime('%Y-%m-%dT00:00:00')

    # –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –Ω–æ–≤–æ–≥–æ —Ä—è–¥–∫–∞ —É category.csv
    def get_insert_index(supplier_id, raw_data):
        insert_index = len(raw_data)
        found_block = False
        for i, r in enumerate(raw_data):
            if r and r[0].strip().isdigit():
                try:
                    cur_id = int(r[0].strip())
                    if cur_id == supplier_id:
                        found_block = True
                        insert_index = i + 1
                    elif cur_id > supplier_id and found_block:
                        return i
                except ValueError:
                    continue
            elif found_block:
                insert_index = i + 1
        return insert_index

    temp_path = csv_path + '.category_temp'
    try:
        with open(csv_path, 'r', encoding='utf-8') as infile, \
             open(temp_path, 'w', encoding='utf-8', newline='') as outfile:

            reader = csv.reader(infile)
            writer = csv.writer(outfile)
            headers = next(reader)
            writer.writerow(headers)

            for idx, row in enumerate(reader):
                # –†–æ–∑—à–∏—Ä—é—î–º–æ —Ä—è–¥–æ–∫ –∑–∞ –ø–æ—Ç—Ä–µ–±–∏
                max_col = max(M, N, O, Q, T, U, V, W, X, Y, Z, AV, AX, AZ, G, H)
                if len(row) <= max_col:
                    row.extend([''] * (max_col + 1 - len(row)))

                product_name = row[G].strip()
                product_desc = row[H]

                key = tuple(row[i].strip().lower() for i in (M, N, O))

                # --- –ö–∞—Ç–µ–≥–æ—Ä—ñ—è Q ---
                category_val = rules_category.get(key)
                if category_val is not None:
                    row[Q] = category_val or ""
                else:
                    # –î–æ–¥–∞—î–º–æ –Ω–æ–≤–∏–π —Ä—è–¥–æ–∫ —É category.csv
                    insert_idx = get_insert_index(supplier_id, raw_category)
                    new_row = [''] + list(row[M:O+1]) + [''] * (max_row_len_category - 4)
                    raw_category.insert(insert_idx, new_row)
                    rules_category[key] = ""
                    changes_category = True
                    logging.warning(f"–†—è–¥–æ–∫ {idx + 2}: –î–æ–¥–∞–Ω–∞ –Ω–æ–≤–∞ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—è –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó {key}")

                # --- –ü–æ–∑–Ω–∞—á–∫–∏ T ---
                if product_name and poznachky_list:
                    found_tags = []
                    covered = []
                    name_lower = product_name.lower()
                    for tag in poznachky_list:
                        if tag in name_lower:
                            start, end = name_lower.find(tag), name_lower.find(tag) + len(tag)
                            if not any(s <= start and end <= e for s, e in covered):
                                found_tags.append(tag.capitalize())
                                covered.append((start, end))
                                covered.sort(key=lambda x: x[1]-x[0], reverse=True)
                    if found_tags:
                        row[T] = ', '.join(found_tags)

                # --- Rank Math U ---
                if product_name:
                    cleaned = re.sub(r'[–∞-—è–ê-–Ø0-9]', '', product_name)
                    cleaned = re.sub(r'[^a-zA-Z\s]', '', cleaned)
                    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
                    row[U] = cleaned

                # --- pa_used AV ---
                pa_val = pa_used_map.get(key)
                if pa_val:
                    row[AV] = pa_val


                # --- –§—ñ–∫—Å–æ–≤–∞–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏ ---
                row[V] = name_ukr
                row[W] = "draft"
                row[X] = "yes"
                row[Y] = "none"
                row[AZ] = "simple"
                row[AX] = current_date

                # --- –ö–æ—Ä–æ—Ç–∫–∏–π –æ–ø–∏—Å Z ---
                if product_desc:
                    row[Z] = product_desc.split('\\n', 1)[0].strip()
                else:
                    row[Z] = ""

                writer.writerow(row)

        os.replace(temp_path, csv_path)
        logging.info("–ó–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–π —Ç–∞ —Å–ª—É–∂–±–æ–≤–∏—Ö –∫–æ–ª–æ–Ω–æ–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")

        if changes_category:
            save_category_csv(raw_category)
        else:
            logging.info("–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è category.csv –Ω–µ –ø–æ—Ç—Ä—ñ–±–Ω–µ. –ó–º—ñ–Ω: False.")

    except Exception as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—ñ –∫–æ–ª–æ–Ω–æ–∫: {e}")
        if os.path.exists(temp_path):
            os.remove(temp_path)

def refill_product_category():
    """
    –ü–æ–≤—Ç–æ—Ä–Ω–æ –∑–∞–ø–æ–≤–Ω—é—î –∫–æ–ª–æ–Ω–∫–∏ Q (–ö–∞—Ç–µ–≥–æ—Ä—ñ—è) —Ç–∞ AV (pa_used) —É 1.csv
    –Ω–∞ –æ—Å–Ω–æ–≤—ñ –æ–Ω–æ–≤–ª–µ–Ω–∏—Ö –ø—Ä–∞–≤–∏–ª —É category.csv.
    –ù–ï –¥–æ–¥–∞—î –Ω–æ–≤—ñ —Ä—è–¥–∫–∏ —É category.csv.
    –õ–æ–≥—É–≤–∞–Ω–Ω—è –ø–æ–∫–∞–∑—É—î, —è–∫—ñ —Ä—è–¥–∫–∏ –æ–Ω–æ–≤–ª–µ–Ω—ñ.
    """
    log_message_to_existing_file()
    logging.info("–§—É–Ω–∫—Ü—ñ—è 6. –ü–æ—á–∏–Ω–∞—é –ø–æ–≤—Ç–æ—Ä–Ω–µ –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–π —Ç–∞ pa_used —É 1.csv...")

    # --- 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å ---
    settings = load_settings()
    try:
        csv_path = settings['paths']['csv_path_supliers_1_new']
        supplier_id = 1
    except (TypeError, KeyError) as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –¥–æ—Å—Ç—É–ø—É –¥–æ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å: {e}")
        return

    # --- 2. –Ü–Ω–¥–µ–∫—Å–∏ –∫–æ–ª–æ–Ω–æ–∫ CSV ---
    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –æ–¥—Ä–∞–∑—É —á–∏—Å–ª–∞, –±–µ–∑ –¥–æ–≤–≥–∏—Ö –∑–º—ñ–Ω–Ω–∏—Ö
    M, N, O = 12, 13, 14        # name_1, name_2, name_3
    Q, AV = 16, 47              # –ö–∞—Ç–µ–≥–æ—Ä—ñ—è —Ç–∞ pa_used
    max_index = max(M, N, O, Q, AV)
    missing_category_rows = []  # —Å–ø–∏—Å–æ–∫ —Ä—è–¥–∫—ñ–≤ –∑ –ø–æ—Ä–æ–∂–Ω—å–æ—é –∫–∞—Ç–µ–≥–æ—Ä—ñ—î—é

    # --- 3. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø—Ä–∞–≤–∏–ª –∫–∞—Ç–µ–≥–æ—Ä—ñ–π —Ç–∞ pa_used ---
    category_map, raw_category = load_category_csv()
    rules_category = {}
    pa_used_map = {}
    supplier_str = str(supplier_id)

    for row in raw_category:
        if len(row) > 5:
            supplier_value = row[0].strip()
            if supplier_value == supplier_str or supplier_value == '':
                key = tuple(v.strip().lower() for v in row[1:4])  # –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—è M,N,O
                rules_category[key] = row[4].strip() if len(row) > 4 else ""
                pa_used_map[key] = row[5].strip() if len(row) > 5 else ""

    logging.info(f"–ó—á–∏—Ç–∞–Ω–æ {len(rules_category)} –ø—Ä–∞–≤–∏–ª –¥–ª—è –ö–∞—Ç–µ–≥–æ—Ä—ñ—ó (Q) —Ç–∞ {len(pa_used_map)} –ø—Ä–∞–≤–∏–ª –¥–ª—è pa_used (AV)")

    # --- 4. –û–±—Ä–æ–±–∫–∞ CSV ---
    temp_path = csv_path + '.refill_temp'
    updated_rows = 0

    try:
        with open(csv_path, 'r', encoding='utf-8') as infile, \
             open(temp_path, 'w', encoding='utf-8', newline='') as outfile:

            reader = csv.reader(infile)
            writer = csv.writer(outfile)

            headers = next(reader)
            writer.writerow(headers)

            for idx, row in enumerate(reader):
                # –†–æ–∑—à–∏—Ä—é—î–º–æ —Ä—è–¥–æ–∫, —â–æ–± –Ω–µ –≤–∏—Ö–æ–¥–∏—Ç–∏ –∑–∞ –º–µ–∂—ñ
                if len(row) <= max_index:
                    row.extend([''] * (max_index + 1 - len(row)))

                # --- 4.1 –ö–ª—é—á –ø–æ—à—É–∫—É ---
                key = tuple(row[i].strip().lower() for i in (M, N, O))
                initial_category = row[Q].strip()
                initial_pa_used = row[AV].strip()
                row_changed = False

                # --- 4.2 –ü–æ–≤—Ç–æ—Ä–Ω–µ –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –ö–∞—Ç–µ–≥–æ—Ä—ñ—ó Q ---
                category_val = rules_category.get(key)
                if category_val and category_val != initial_category:
                    row[Q] = category_val
                    row_changed = True
                    logging.info(f"–†—è–¥–æ–∫ {idx + 2}: Q (–ö–∞—Ç–µ–≥–æ—Ä—ñ—è) –æ–Ω–æ–≤–ª–µ–Ω–æ. –ö–ª—é—á: {key}, –ó–Ω–∞—á–µ–Ω–Ω—è: '{category_val}'")

                # --- 4.3 –ü–æ–≤—Ç–æ—Ä–Ω–µ –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è pa_used AV ---
                pa_val = pa_used_map.get(key)
                if pa_val and pa_val != initial_pa_used:
                    row[AV] = pa_val
                    row_changed = True
                    logging.info(f"–†—è–¥–æ–∫ {idx + 2}: AV (pa_used) –æ–Ω–æ–≤–ª–µ–Ω–æ. –ö–ª—é—á: {key}, –ó–Ω–∞—á–µ–Ω–Ω—è: '{pa_val}'")

                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø–æ—Ä–æ–∂–Ω—å–æ—ó –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó –ø—ñ—Å–ª—è –æ–Ω–æ–≤–ª–µ–Ω–Ω—è
                if not row[Q].strip():
                    missing_category_rows.append(idx + 2)  # –∑–±–µ—Ä—ñ–≥–∞—î–º–æ –Ω–æ–º–µ—Ä —Ä—è–¥–∫–∞ —É —Ñ–∞–π–ª—ñ

                if row_changed:
                    updated_rows += 1

                writer.writerow(row)

        # --- 5. –ó–∞–º—ñ–Ω—é—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π CSV ---
        os.replace(temp_path, csv_path)
        logging.info(f"–ü–æ–≤—Ç–æ—Ä–Ω–µ –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –û–Ω–æ–≤–ª–µ–Ω–æ {updated_rows} —Ä—è–¥–∫—ñ–≤.")

        # --- –õ–æ–≥—É–≤–∞–Ω–Ω—è —Ä—è–¥–∫—ñ–≤ –∑ –ø–æ—Ä–æ–∂–Ω—å–æ—é –∫–∞—Ç–µ–≥–æ—Ä—ñ—î—é ---
        for row_num in missing_category_rows:
            logging.warning(f"–£–í–ê–ì–ê —Ä—è–¥–æ–∫ {row_num} –Ω–µ –∑–∞–ø–æ–≤–Ω–µ–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è!")

    except Exception as e:
        logging.error(f"–ù–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ–º—É –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—ñ: {e}")
        if os.path.exists(temp_path):
            os.remove(temp_path)

def separate_existing_products():
    """
    –ó–≤—ñ—Ä—è—î —à—Ç—Ä–∏—Ö–∫–æ–¥–∏ 1.csv –∑ –±–∞–∑–æ—é (zalishki.csv),
    –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—å –∑–Ω–∞–π–¥–µ–Ω—ñ —Ç–æ–≤–∞—Ä–∏ —É old_prod_new_SHK.csv,
    –≤–∏–¥–∞–ª—è—î —ó—Ö –∑ 1.csv —Ç–∞ —Ñ–æ—Ä–º—É—î –ø—ñ–¥—Å—É–º–∫–æ–≤—É —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É.
    –ö–æ–ª–æ–Ω–∫–∏ —Ç–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ—Å—Ç—ñ old -> new –≤–∏–Ω–µ—Å–µ–Ω—ñ —É settings.json.
    """
    log_message_to_existing_file()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 7. –ü–æ—á–∏–Ω–∞—é –∑–≤—ñ—Ä–∫—É 1.csv –∑—ñ —à—Ç—Ä–∏—Ö–∫–æ–¥–∞–º–∏ –±–∞–∑–∏ (zalishki.csv)...")

    settings = load_settings()
    try:
        sl_new_path = settings['paths']['csv_path_supliers_1_new']
        zalishki_path = settings['paths']['csv_path_zalishki']
        sl_old_prod_shk_path = settings['paths']['csv_path_sl_old_prod_new_shk']
        column_mapping = settings['suppliers']['1']['column_mapping_sl_old_to_sl_new']
    except KeyError as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó. –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —à–ª—è—Ö –∞–±–æ –º–∞–ø—É –∫–æ–ª–æ–Ω–æ–∫: {e}")
        return

    # --- 0. –ó—á–∏—Ç—É–≤–∞–Ω–Ω—è —ñ—Å–Ω—É—é—á–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞ old_prod_new_SHK.csv ---
    sl_old_header = []
    try:
        if os.path.exists(sl_old_prod_shk_path):
            with open(sl_old_prod_shk_path, mode='r', encoding='utf-8') as f:
                reader = csv.reader(f)
                sl_old_header = next(reader, [])
        else:
            logging.warning("–§–∞–π–ª old_prod_new_SHK.csv –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ ‚Äî —Å—Ç–≤–æ—Ä—é—é –Ω–æ–≤–∏–π —ñ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º.")
            sl_old_header_base = [
                'id', 'sku', '–ú–µ—Ç–∞: url_lutsk', '–ú–µ—Ç–∞: shtrih_cod', '–ú–µ—Ç–∞: artykul_lutsk', '–ü–æ–∑–Ω–∞—á–∫–∏',
                'rank_math_focus_keyword', '–ú–µ—Ç–∞: postachalnyk', 'manage_stock', 'tax_status', 'excerpt'
            ]
            # –î–æ–¥–∞—î–º–æ –∞—Ç—Ä–∏–±—É—Ç–∏ —Ç–∞ –¥–æ–¥–∞—Ç–∫–æ–≤—ñ –∫–æ–ª–æ–Ω–∫–∏ (–±–µ–∑ attribute_none)
            sl_old_header = sl_old_header_base + [f'attribute_{i}' for i in range(1, 24)] + [
                'content', 'post_date', 'product_type'
            ]

        # –û—á–∏—â–∞—î–º–æ —Ñ–∞–π–ª, –∞–ª–µ –∑–∞–ª–∏—à–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫
        with open(sl_old_prod_shk_path, mode='w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(sl_old_header)

        logging.info("–§–∞–π–ª old_prod_new_SHK.csv –æ—á–∏—â–µ–Ω–æ, –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∑–∞–ª–∏—à–µ–Ω–æ –±–µ–∑ –∑–º—ñ–Ω.")
    except Exception as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó old_prod_new_SHK.csv: {e}")
        return

    # --- 1. –ó—á–∏—Ç—É–≤–∞–Ω–Ω—è –±–∞–∑–∏ —à—Ç—Ä–∏—Ö–∫–æ–¥—ñ–≤ ---
    zalishki_map = {}  # {shk: (id, sku)}
    try:
        with open(zalishki_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            next(reader)  # –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫
            for row in reader:
                if len(row) > 7:
                    shk = row[7].strip()
                    if shk:
                        zalishki_map[shk] = (row[0].strip(), row[1].strip())
        logging.info(f"–ó—á–∏—Ç–∞–Ω–æ {len(zalishki_map)} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —à—Ç—Ä–∏—Ö–∫–æ–¥—ñ–≤ –∑ –±–∞–∑–∏.")
    except Exception as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —á–∏—Ç–∞–Ω–Ω—ñ –±–∞–∑–∏: {e}")
        return

    # --- 2. –û–±—Ä–æ–±–∫–∞ 1.csv —Ç–∞ —Ñ–æ—Ä–º—É–≤–∞–Ω–Ω—è —Å–ø–∏—Å–∫—ñ–≤ ---
    items_to_keep = []
    items_to_move = []

    try:
        with open(sl_new_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            header = next(reader)
            items_to_keep.append(header)

            for row in reader:
                # –†–æ–∑—à–∏—Ä—é—î–º–æ —Ä—è–¥–æ–∫ –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —ñ–Ω–¥–µ–∫—Å—É —É –º–∞–ø—ñ
                max_index = max(column_mapping.values())
                if len(row) <= max_index:
                    row.extend([''] * (max_index + 1 - len(row)))

                shk_value = row[2].strip()  # C (–®—Ç—Ä–∏—Ö–∫–æ–¥)
                if shk_value in zalishki_map:
                    item_id, item_sku = zalishki_map[shk_value]

                    # –§–æ—Ä–º—É—î–º–æ –Ω–æ–≤–∏–π —Ä—è–¥–æ–∫ –¥–ª—è old_prod_new_SHK.csv
                    new_row = [''] * len(sl_old_header)
                    new_row[0] = item_id
                    new_row[1] = item_sku

                    for sl_old_idx_str, sl_new_idx in column_mapping.items():
                        sl_old_idx = int(sl_old_idx_str)  # –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ –∫–ª—é—á —É int
                        if sl_new_idx < len(row):
                            new_row[sl_old_idx] = row[sl_new_idx]

                    items_to_move.append(new_row)
                else:
                    items_to_keep.append(row)

        # --- 3. –ó–∞–ø–∏—Å –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ ---
        if items_to_move:
            with open(sl_old_prod_shk_path, 'a', encoding='utf-8', newline='') as f:
                writer = csv.writer(f)
                writer.writerows(items_to_move)
            logging.info(f"–ü–µ—Ä–µ–Ω–µ—Å–µ–Ω–æ {len(items_to_move)} —ñ—Å–Ω—É—é—á–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ —É old_prod_new_SHK.csv.")
        else:
            logging.info("–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∂–æ–¥–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä—É –∑ —ñ—Å–Ω—É—é—á–∏–º —à—Ç—Ä–∏—Ö–∫–æ–¥–æ–º —É –±–∞–∑—ñ.")

        # --- 4. –ó–∞–ø–∏—Å –æ–Ω–æ–≤–ª–µ–Ω–æ–≥–æ 1.csv ---
        temp_path = sl_new_path + '.temp'
        with open(temp_path, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerows(items_to_keep)
        os.replace(temp_path, sl_new_path)
        logging.info(f"1.csv –æ–Ω–æ–≤–ª–µ–Ω–æ. –ó–∞–ª–∏—à–∏–ª–æ—Å—å {len(items_to_keep)-1} –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ –¥–ª—è —ñ–º–ø–æ—Ä—Ç—É.")

    except Exception as e:
        logging.error(f"–ù–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –æ–±—Ä–æ–±–∫–∏ 1.csv: {e}")
        if 'temp_path' in locals() and os.path.exists(temp_path):
            os.remove(temp_path)

def assign_new_sku_to_products():
    """
    –ó–Ω–∞—Ö–æ–¥–∏—Ç—å –Ω–∞–π–±—ñ–ª—å—à–∏–π SKU —É zalishki.csv (—Å–æ—Ä—Ç—É—î –ø–æ –∫–æ–ª–æ–Ω—Ü—ñ B(1))
    —ñ –ø—Ä–∏—Å–≤–æ—é—î –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ SKU —Ç–æ–≤–∞—Ä–∞–º –±–µ–∑ SKU —É –∫–æ–ª–æ–Ω—Ü—ñ P(15) —Ñ–∞–π–ª—É 1.csv.
    """
    log_message_to_existing_file()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 8. –ü–æ—á–∏–Ω–∞—é –ø—Ä–∏—Å–≤–æ—î–Ω–Ω—è –Ω–æ–≤–∏—Ö SKU —Ç–æ–≤–∞—Ä–∞–º —É 1.csv...")

    # --- 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å ---
    settings = load_settings()
    try:
        sl_new_path = settings['paths']['csv_path_supliers_1_new']
        zalishki_path = settings['paths']['csv_path_zalishki']
    except KeyError as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó. –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —à–ª—è—Ö: {e}")
        return

    # --- 2. –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è —ñ–Ω–¥–µ–∫—Å—É SKU —É 1.csv ---
    SKU_COL_INDEX = 15  # P
    ZALISHKI_SKU_INDEX = 1  # B

    # --- 3. –ó–Ω–∞—Ö–æ–¥–∏–º–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π SKU —É zalishki.csv ---
    try:
        with open(zalishki_path, mode='r', encoding='utf-8') as f:
            reader = csv.reader(f)
            next(reader, None)  # –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫
            sku_list = []
            for row in reader:
                if len(row) > ZALISHKI_SKU_INDEX:
                    val = row[ZALISHKI_SKU_INDEX].strip()
                    if val.isdigit():
                        sku_list.append(int(val))

            if not sku_list:
                logging.warning("–£ –±–∞–∑—ñ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∂–æ–¥–Ω–æ–≥–æ —á–∏—Å–ª–æ–≤–æ–≥–æ SKU. –ü—Ä–∏—Å–≤–æ—î–Ω–Ω—è –Ω–µ–º–æ–∂–ª–∏–≤–µ.")
                return

            sku_list.sort()
            last_sku = sku_list[-1]
            logging.info(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π SKU —É –±–∞–∑—ñ: {last_sku}")

    except FileNotFoundError:
        logging.error(f"–§–∞–π–ª –±–∞–∑–∏ zalishki.csv –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∑–∞ —à–ª—è—Ö–æ–º: {zalishki_path}")
        return
    except Exception as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —á–∏—Ç–∞–Ω–Ω—ñ zalishki.csv: {e}")
        return

    # --- 4. –ü—Ä–∏—Å–≤–æ—î–Ω–Ω—è –Ω–æ–≤–∏—Ö SKU —É 1.csv ---
    next_sku = last_sku + 1
    assigned_count = 0
    temp_path = sl_new_path + '.temp'

    try:
        with open(sl_new_path, mode='r', encoding='utf-8', newline='') as input_file:
            reader = csv.reader(input_file)
            header = next(reader, None)
            rows = [header] if header else []

            for row in reader:
                if len(row) <= SKU_COL_INDEX:
                    row.extend([''] * (SKU_COL_INDEX + 1 - len(row)))

                current_sku = row[SKU_COL_INDEX].strip()
                if not current_sku:
                    row[SKU_COL_INDEX] = str(next_sku)
                    assigned_count += 1
                    next_sku += 1

                rows.append(row)

        # --- 5. –ó–∞–ø–∏—Å –æ–Ω–æ–≤–ª–µ–Ω–æ–≥–æ CSV ---
        if assigned_count > 0:
            with open(temp_path, mode='w', encoding='utf-8', newline='') as f:
                writer = csv.writer(f)
                writer.writerows(rows)
            os.replace(temp_path, sl_new_path)
            logging.info(f"‚úÖ –£—Å–ø—ñ—à–Ω–æ –ø—Ä–∏—Å–≤–æ—î–Ω–æ {assigned_count} –Ω–æ–≤–∏—Ö SKU. –ù–∞—Å—Ç—É–ø–Ω–∏–π SKU –±—É–¥–µ {next_sku}.")
        else:
            logging.info("–£—Å—ñ —Ç–æ–≤–∞—Ä–∏ –≤–∂–µ –º–∞—é—Ç—å SKU. –ó–º—ñ–Ω –Ω–µ –≤–Ω–µ—Å–µ–Ω–æ.")

    except FileNotFoundError:
        logging.error(f"–§–∞–π–ª 1.csv –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∑–∞ —à–ª—è—Ö–æ–º")
    except Exception as e:
        logging.error(f"–ù–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –ø—Ä–∏—Å–≤–æ—î–Ω–Ω—è SKU: {e}")
        if os.path.exists(temp_path):
            os.remove(temp_path)

def download_images_for_product():
    """
    6 –µ—Ç–∞–ø—ñ–≤:
      1. –û—á–∏—Å—Ç–∫–∞ –ø–∞–ø–æ–∫ JPG —Ç–∞ WEBP
      2. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω—å —É –ø–∞–ø–∫—É JPG
      3. –ü–µ—Ä–µ–º—ñ—â–µ–Ω–Ω—è GIF —É WEBP
      4. –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è JPG —É WEBP
      5. –û–Ω–æ–≤–ª–µ–Ω–Ω—è CSV
      6. –ö–æ–ø—ñ—é–≤–∞–Ω–Ω—è –Ω–∞ —Å–∞–π—Ç
    """
    log_message_to_existing_file()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 9. –ü–æ—á–∞—Ç–æ–∫ –ø—Ä–æ—Ü–µ—Å—É –æ–±—Ä–æ–±–∫–∏ –∑–æ–±—Ä–∞–∂–µ–Ω—å...")

    settings = load_settings()
    try:
        sl_new = settings['paths']['csv_path_supliers_1_new']
        jpg_path = settings['paths']['img_path_jpg']
        webp_path = settings['paths']['img_path_webp']
        cat_map = settings['categories']
        site_path = settings['paths']['site_path_images']
    except KeyError as e:
        logging.error(f"‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —à–ª—è—Ö —É settings.json: {e}")
        return

    URL, SKU, CAT, IMG_LIST, WEBP_LIST = 1, 15, 16, 17, 18

    # 1Ô∏è‚É£ –û—á–∏—Å—Ç–∫–∞
    clear_directory(jpg_path)
    clear_directory(webp_path)
    logging.info("1. ‚úÖ –û—á–∏—Å—Ç–∫–∞ –ø–∞–ø–æ–∫ JPG —Ç–∞ WEBP –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

    # 2Ô∏è‚É£ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
    rows = []
    with open(sl_new, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        header = next(reader)
        rows.append(header)
        for row in reader:
            if len(row) <= IMG_LIST:
                row.extend([''] * (IMG_LIST - len(row) + 1))
            url, sku, cat = row[URL].strip(), row[SKU].strip(), row[CAT].strip()
            if url and sku and cat:
                imgs = download_product_images(url, sku, cat, jpg_path, cat_map)
                row[IMG_LIST] = ', '.join(imgs) if imgs else ''
            rows.append(row)
            time.sleep(random.uniform(0.5, 1.5))

    with open(sl_new, 'w', encoding='utf-8', newline='') as f:
        csv.writer(f).writerows(rows)
    logging.info(f"2. üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω—å –∑–∞–≤–µ—Ä—à–µ–Ω–æ ({len(rows)-1} —Ä—è–¥–∫—ñ–≤).")

    # 3Ô∏è‚É£ GIF
    move_gifs(jpg_path, webp_path)
    logging.info("3. ‚úÖ –ü–µ—Ä–µ–º—ñ—â–µ–Ω–Ω—è GIF –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")

    # 4Ô∏è‚É£ WEBP
    convert_to_webp_square(jpg_path, webp_path)
    logging.info("4. ‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è JPG —É WEBP –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

    # 5Ô∏è‚É£ CSV sync
    sync_webp_column(sl_new, webp_path, WEBP_LIST, SKU)
    logging.info("5. ‚úÖ –û–Ω–æ–≤–ª–µ–Ω–Ω—è –∫–æ–ª–æ–Ω–∫–∏ WEBP —É CSV –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")

    # 6Ô∏è‚É£ –ö–æ–ø—ñ—é–≤–∞–Ω–Ω—è
    copy_to_site(webp_path, site_path)
    logging.info("6. ‚úÖ –ö–æ–ø—ñ—é–≤–∞–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω—å –Ω–∞ —Å–∞–π—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")

    logging.info("‚úÖ –£—Å—ñ 6 –µ—Ç–∞–ø—ñ–≤ –æ–±—Ä–æ–±–∫–∏ –∑–æ–±—Ä–∞–∂–µ–Ω—å –≤–∏–∫–æ–Ω–∞–Ω–æ —É—Å–ø—ñ—à–Ω–æ.")

def create_new_products_import_file():
    """
    –°—Ç–≤–æ—Ä—é—î –æ–Ω–æ–≤–ª–µ–Ω–∏–π —Ñ–∞–π–ª `new_prod.csv` –¥–ª—è —ñ–º–ø–æ—Ä—Ç—É –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤.

    üß© –õ–æ–≥—ñ–∫–∞ —Ä–æ–±–æ—Ç–∏:
    1Ô∏è‚É£ –ó—á–∏—Ç—É—î –ø–æ—Ç–æ—á–Ω–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ñ–∞–π–ª—É new_prod.csv (–≤—ñ–Ω –ó–ê–õ–ò–®–ê–Ñ–¢–¨–°–Ø –±–µ–∑ –∑–º—ñ–Ω).
    2Ô∏è‚É£ –û—á–∏—â—É—î —Ñ–∞–π–ª –≤—ñ–¥ —Å—Ç–∞—Ä–∏—Ö –¥–∞–Ω–∏—Ö.
    3Ô∏è‚É£ –ó—á–∏—Ç—É—î new.csv.
    4Ô∏è‚É£ –ü–µ—Ä–µ–Ω–æ—Å–∏—Ç—å –¥–∞–Ω—ñ –∑–≥—ñ–¥–Ω–æ –∑ COLUMN_MAP.
    5Ô∏è‚É£ –ó–∞–ø–∏—Å—É—î –æ–Ω–æ–≤–ª–µ–Ω—ñ –¥–∞–Ω—ñ —É new_prod.csv, –∑–∞–ª–∏—à–∞—é—á–∏ —Å—Ç–∞—Ä–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫.

    ‚ö†Ô∏è –Ø–∫—â–æ —Ñ–∞–π–ª—É new_prod.csv —â–µ –Ω–µ–º–∞—î, –π–æ–≥–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ —Å—Ç–≤–æ—Ä–∏—Ç–∏ –≤—Ä—É—á–Ω—É –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏.
    """

    # -----------------------------------------------------------
    # üî¢ –ú–∞–ø–∞ –∫–æ–ª–æ–Ω–æ–∫: {—ñ–Ω–¥–µ–∫—Å —É new.csv ‚Üí —ñ–Ω–¥–µ–∫—Å —É new_prod.csv}
    # -----------------------------------------------------------
    COLUMN_MAP = {
        15: 0, 1: 1, 2: 2, 5: 3, 6: 4, 7: 5, 8: 6, 9: 7, 16: 8, 18: 9,
        19: 10, 20: 11, 21: 12, 22: 13, 23: 14, 24: 15, 25: 16, 26: 17,
        27: 18, 28: 19, 29: 20, 30: 21, 31: 22, 32: 23, 33: 24, 34: 25,
        35: 26, 36: 27, 37: 28, 38: 29, 39: 30, 40: 31, 41: 32, 42: 33,
        43: 34, 44: 35, 45: 36, 46: 37, 47: 38, 48: 39, 49: 40, 51: 41
    }
    log_message_to_existing_file()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 10. –ü–æ—á–∏–Ω–∞—é —Ñ–æ—Ä–º—É–≤–∞–Ω–Ω—è —Ñ–∞–π–ª—É new_prod.csv –¥–ª—è —ñ–º–ø–æ—Ä—Ç—É –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤.")

    # -----------------------------------------------------------
    # üß© –ö—Ä–æ–∫ 1: –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å —ñ —à–ª—è—Ö—ñ–≤
    # -----------------------------------------------------------
    settings = load_settings()
    try:
        sl_new_path = settings['paths']['csv_path_supliers_1_new']
        sl_new_prod_path = settings['paths']['csv_path_sl_new_prod']
    except KeyError as e:
        logging.critical(f"‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —à–ª—è—Ö —É settings.json: {e}")
        return

    temp_file = sl_new_prod_path + '.temp'

    # -----------------------------------------------------------
    # üìÑ –ö—Ä–æ–∫ 2: –ó—á–∏—Ç—É—î–º–æ —ñ—Å–Ω—É—é—á–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ new_prod.csv
    # -----------------------------------------------------------
    if not os.path.exists(sl_new_prod_path):
        logging.critical(
            f"‚ö†Ô∏è –§–∞–π–ª new_prod_path –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ!\n"
            "–°—Ç–≤–æ—Ä—ñ—Ç—å –π–æ–≥–æ –≤—Ä—É—á–Ω—É –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º."
        )
        return

    try:
        with open(sl_new_prod_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            header = next(reader, None)
            if not header:
                logging.critical(f"‚ùå –£ —Ñ–∞–π–ª—ñ {sl_new_prod_path} –≤—ñ–¥—Å—É—Ç–Ω—ñ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫.")
                return
        logging.info("‚úÖ –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∑–±–µ—Ä–µ–∂–µ–Ω–æ, —Å—Ç–∞—Ä—ñ –¥–∞–Ω—ñ –±—É–¥–µ –æ—á–∏—â–µ–Ω–æ.")
    except Exception as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —á–∏—Ç–∞–Ω–Ω—ñ –∑–∞–≥–æ–ª–æ–≤–∫–∞: {e}", exc_info=True)
        return

    # -----------------------------------------------------------
    # üì¶ –ö—Ä–æ–∫ 3: –ó—á–∏—Ç—É—î–º–æ SL_new.csv
    # -----------------------------------------------------------
    if not os.path.exists(sl_new_path):
        logging.critical(f"‚ùå –§–∞–π–ª new_path –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
        return

    try:
        with open(sl_new_path, 'r', encoding='utf-8') as f:
            reader = list(csv.reader(f))
    except Exception as e:
        logging.critical(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —á–∏—Ç–∞–Ω–Ω—ñ new_path: {e}", exc_info=True)
        return

    if len(reader) <= 1:
        logging.warning("‚ö†Ô∏è SL_new.csv –ø–æ—Ä–æ–∂–Ω—ñ–π –∞–±–æ –º—ñ—Å—Ç–∏—Ç—å –ª–∏—à–µ –∑–∞–≥–æ–ª–æ–≤–æ–∫.")
        # –ó–∞–ø–∏—Å—É—î–º–æ –ª–∏—à–µ –∑–∞–≥–æ–ª–æ–≤–æ–∫ —É –Ω–æ–≤–∏–π —Ñ–∞–π–ª
        with open(sl_new_prod_path, 'w', encoding='utf-8', newline='') as f:
            csv.writer(f).writerow(header)
        return

    source_rows = reader[1:]  # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫

    # -----------------------------------------------------------
    # üîÑ –ö—Ä–æ–∫ 4: –§–æ—Ä–º—É—î–º–æ –Ω–æ–≤—ñ —Ä—è–¥–∫–∏ –∑–≥—ñ–¥–Ω–æ –∑ –º–∞–ø–æ—é COLUMN_MAP
    # -----------------------------------------------------------
    rows_to_write = [header]
    processed = 0
    max_src = max(COLUMN_MAP.keys())

    for i, src_row in enumerate(source_rows, start=2):  # —Ä—è–¥–∫–∏ –ø–æ—á–∏–Ω–∞—é—Ç—å—Å—è –∑ 2 (1 ‚Äî –∑–∞–≥–æ–ª–æ–≤–æ–∫)
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –¥–æ–≤–∂–∏–Ω—É —Ä—è–¥–∫–∞ (—â–æ–± —É–Ω–∏–∫–Ω—É—Ç–∏ IndexError)
        if len(src_row) <= max_src:
            logging.warning(f"‚ö†Ô∏è –†—è–¥–æ–∫ {i}: –ø—Ä–æ–ø—É—â–µ–Ω–æ ‚Äî –Ω–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –∫–æ–ª–æ–Ω–æ–∫ ({len(src_row)}/{max_src+1}).")
            continue

        # –°—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ–≤–∏–π —Ä—è–¥–æ–∫ —ñ–∑ —Ç–∞–∫–æ—é —Å–∞–º–æ—é –∫—ñ–ª—å–∫—ñ—Å—Ç—é –∫–æ–ª–æ–Ω–æ–∫, —è–∫ —É –∑–∞–≥–æ–ª–æ–≤–∫—É
        tgt_row = [''] * len(header)

        # –ü–µ—Ä–µ–Ω–æ—Å–∏–º–æ –∑–Ω–∞—á–µ–Ω–Ω—è –∑–≥—ñ–¥–Ω–æ –∑ COLUMN_MAP
        for src_idx, tgt_idx in COLUMN_MAP.items():
            if tgt_idx < len(tgt_row):
                tgt_row[tgt_idx] = src_row[src_idx].strip()

        rows_to_write.append(tgt_row)
        processed += 1

    logging.info(f"üîÅ –û–±—Ä–æ–±–ª–µ–Ω–æ {processed} —Ä—è–¥–∫—ñ–≤ –¥–ª—è –∑–∞–ø–∏—Å—É —É new_prod.csv.")

    # -----------------------------------------------------------
    # üíæ –ö—Ä–æ–∫ 5: –ó–∞–ø–∏—Å —É —Ñ–∞–π–ª (–æ—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä–∏—Ö –¥–∞–Ω–∏—Ö, –∞–ª–µ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∑–±–µ—Ä—ñ–≥–∞—î—Ç—å—Å—è)
    # -----------------------------------------------------------
    try:
        with open(temp_file, 'w', encoding='utf-8', newline='') as f:
            csv.writer(f).writerows(rows_to_write)
        os.replace(temp_file, sl_new_prod_path)
        logging.info(f"‚úÖ –§–∞–π–ª new_prod_path –æ–Ω–æ–≤–ª–µ–Ω–æ ({processed} —Ä—è–¥–∫—ñ–≤).")
    except Exception as e:
        logging.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Å—É new_prod_path: {e}", exc_info=True)
        if os.path.exists(temp_file):
            os.remove(temp_file)

def update_existing_products_batch():
    """
    –û–Ω–æ–≤–ª—é—î —ñ—Å–Ω—É—é—á—ñ —Ç–æ–≤–∞—Ä–∏ —É WooCommerce –Ω–∞ –æ—Å–Ω–æ–≤—ñ CSV-—Ñ–∞–π–ª—É old_prod_new_SHK.csv.
    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –≥–ª–æ–±–∞–ª—å–Ω—ñ –∞—Ç—Ä–∏–±—É—Ç–∏ WooCommerce —ñ–∑ global_attr_map.
    """

    log_message_to_existing_file()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 11. –ü–æ—á–∏–Ω–∞—é –ø–∞–∫–µ—Ç–Ω–µ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è —ñ—Å–Ω—É—é—á–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ –∑ old_prod_new_SHK.csv...")

    settings = load_settings()
    if not settings:
        logging.critical("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è.")
        return

    try:
        csv_path = settings['paths']['csv_path_sl_old_prod_new_shk']
        global_attr_map = settings.get('global_attr_map', {})
    except KeyError:
        logging.error("‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —à–ª—è—Ö –¥–æ CSV —É –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è—Ö.")
        return

    wcapi = get_wc_api(settings)
    if not wcapi:
        logging.critical("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è —Å—Ç–≤–æ—Ä–∏—Ç–∏ –æ–±'—î–∫—Ç WooCommerce API.")
        return

    BATCH_SIZE = 5
    products_to_update = []
    total_products_read = 0
    total_updated = 0
    total_skipped = 0
    errors_list = []
    start_time = time.time()

    try:
        with open(csv_path, mode='r', encoding='utf-8') as f:
            reader = csv.reader(f)
            headers = next(reader)
            field_map = {header: idx for idx, header in enumerate(headers)}

            STANDARD_FIELDS = ['sku', 'post_date', 'product_type', 'tax_status']
            ACF_PREFIX = '–ú–µ—Ç–∞: '
            ATTRIBUTE_PREFIX = 'attribute:'

            for row in reader:
                total_products_read += 1

                # --- –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ ID ---
                product_id_str = row[field_map.get('id', -1)].strip()
                if not product_id_str.isdigit():
                    errors_list.append(f"–†—è–¥–æ–∫ {total_products_read}: –ü—Ä–æ–ø—É—â–µ–Ω–æ. –ù–µ–∫–æ—Ä–µ–∫—Ç–Ω–∏–π ID.")
                    total_skipped += 1
                    continue

                product_id = int(product_id_str)
                product_data = {"id": product_id}
                meta_data = []
                new_attributes = []
                tags = []

                # --- –ü—Ä–æ—Ö–æ–¥–∏–º–æ –ø–æ –≤—Å—ñ—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö ---
                for key, index in field_map.items():
                    if index >= len(row):
                        continue
                    value = row[index].strip()
                    if not value:
                        continue

                    # --- –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ñ –ø–æ–ª—è ---
                    if key in STANDARD_FIELDS:
                        if key == 'post_date':
                            product_data['date_created'] = value
                            try:
                                dt = datetime.fromisoformat(value)
                                product_data['date_created_gmt'] = (dt - timedelta(hours=3)).isoformat()
                            except ValueError:
                                errors_list.append(
                                    f"‚ö†Ô∏è –†—è–¥–æ–∫ {total_products_read}: –Ω–µ–∫–æ—Ä–µ–∫—Ç–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç post_date '{value}'"
                                )
                        else:
                            product_data[key] = value

                    # --- Meta Data ---
                    elif key.startswith(ACF_PREFIX) or key == 'rank_math_focus_keyword':
                        meta_key = key.replace(ACF_PREFIX, '') if key.startswith(ACF_PREFIX) else key
                        meta_data.append({"key": meta_key, "value": value})

                    # --- –¢–µ–≥–∏ ---
                    elif key == '–ü–æ–∑–Ω–∞—á–∫–∏':
                        tag_names = [t.strip() for t in value.split(',') if t.strip()]
                        tags.extend([{"name": t} for t in tag_names])

                    # --- –ê—Ç—Ä–∏–±—É—Ç–∏ ---
                    elif key.startswith(ATTRIBUTE_PREFIX):
                        attr_name = key.replace(ATTRIBUTE_PREFIX, '')
                        import re
                        # –†–æ–∑—É–º–Ω–∏–π —Å–ø–ª—ñ—Ç, —â–æ–± —á–∏—Å–ª–∞ –∑ –∫–æ–º–æ—é –Ω–µ —Ä–æ–∑—Ä–∏–≤–∞–ª–∏—Å—è
                        def _smart_split(val):
                            if not val:
                                return []
                            parts = [p.strip() for p in re.split(r'[;,|]', val) if p.strip()]
                            return parts
                        options = _smart_split(value)
                        if options:
                            attr_dict = {
                                "name": attr_name,
                                "position": len(new_attributes),
                                "visible": True,
                                "variation": False,
                                "options": options
                            }
                            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –≥–ª–æ–±–∞–ª—å–Ω–∏–π –∞—Ç—Ä–∏–±—É—Ç, —è–∫—â–æ –≤—ñ–Ω —î
                            if attr_name in global_attr_map:
                                attr_dict["id"] = global_attr_map[attr_name]
                            new_attributes.append(attr_dict)

                if meta_data:
                    product_data['meta_data'] = meta_data
                if tags:
                    product_data['tags'] = tags

                # --- Merge –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ –∑ —ñ—Å–Ω—É—é—á–∏–º–∏ ---
                if new_attributes:
                    try:
                        existing_attributes = wcapi.get(f"products/{product_id}").json().get("attributes", [])
                        attr_map = {attr['name']: attr for attr in existing_attributes}
                        for new_attr in new_attributes:
                            name = new_attr['name']
                            if name in attr_map:
                                attr_map[name]['options'] = new_attr['options']
                                attr_map[name]['position'] = new_attr['position']
                                attr_map[name]['visible'] = new_attr['visible']
                                attr_map[name]['variation'] = new_attr['variation']
                                if 'id' in new_attr:
                                    attr_map[name]['id'] = new_attr['id']
                            else:
                                attr_map[name] = new_attr
                        product_data['attributes'] = list(attr_map.values())
                    except Exception as e:
                        logging.error(f"–†—è–¥–æ–∫ {total_products_read}: –ü–æ–º–∏–ª–∫–∞ merge –∞—Ç—Ä–∏–±—É—Ç—ñ–≤: {e}")

                products_to_update.append(product_data)

                if len(products_to_update) >= BATCH_SIZE:
                    total_updated += _process_batch_update(wcapi, products_to_update, errors_list)
                    products_to_update = []

            if products_to_update:
                total_updated += _process_batch_update(wcapi, products_to_update, errors_list)

    except Exception as e:
        logging.critical(f"‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—Ä–æ–±—Ü—ñ CSV: {e}", exc_info=True)
        return

    # --- –ü—ñ–¥—Å—É–º–æ–∫ ---
    elapsed_time = int(time.time() - start_time)
    logging.info("--- üèÅ –ü—ñ–¥—Å—É–º–æ–∫ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è —ñ—Å–Ω—É—é—á–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ ---")
    logging.info(f"–í—Å—å–æ–≥–æ —Ä—è–¥–∫—ñ–≤: {total_products_read}")
    logging.info(f"–£—Å–ø—ñ—à–Ω–æ –æ–Ω–æ–≤–ª–µ–Ω–æ: {total_updated}")
    logging.info(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ/–∑ –ø–æ–º–∏–ª–∫–∞–º–∏: {total_products_read - total_updated}")
    logging.info(f"–ó–∞–≥–∞–ª—å–Ω–∞ —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å: {elapsed_time} —Å–µ–∫.")

    if errors_list:
        logging.warning(f"‚ö†Ô∏è –ó–Ω–∞–π–¥–µ–Ω–æ {len(errors_list)} –ø–æ–º–∏–ª–æ–∫. –ü–µ—Ä—à—ñ 5:")
        for err in errors_list[:5]:
            logging.warning(f"-> {err}")
    else:
        logging.info("‚úÖ –û–Ω–æ–≤–ª–µ–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ –±–µ–∑ –ø–æ–º–∏–ª–æ–∫.")

def create_new_products_batch():
    """–ü–∞–∫–µ—Ç–Ω–µ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ —É WooCommerce –∑ CSV (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –≥–ª–æ–±–∞–ª—å–Ω—ñ –∞—Ç—Ä–∏–±—É—Ç–∏)."""

    log_message_to_existing_file()
    logging.info("üöÄ –ü–æ—á–∏–Ω–∞—é –ø–∞–∫–µ—Ç–Ω–µ –°–¢–í–û–†–ï–ù–ù–Ø –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ –∑ SL_new_prod.csv...")

    settings = load_settings()
    if not settings:
        logging.critical("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è.")
        return

    try:
        csv_path = settings['paths']['csv_path_sl_new_prod']
        uploads_path = '/var/www/html/erosinua/public_html/wp-content/uploads/products'
        global_attr_map = settings.get('global_attr_map', {})
    except KeyError as e:
        logging.error(f"‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —à–ª—è—Ö –¥–æ CSV –∞–±–æ uploads_path: {e}")
        return

    wcapi = get_wc_api(settings)
    if not wcapi:
        logging.critical("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è —Å—Ç–≤–æ—Ä–∏—Ç–∏ –æ–±'—î–∫—Ç WooCommerce API.")
        return

    # --- –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Å–ø–∏—Å–∫—É –∫–∞—Ç–µ–≥–æ—Ä—ñ–π ---
    categories_map = {}
    try:
        cat_path = settings['paths'].get('product_categories')
        if cat_path and os.path.exists(cat_path):
            with open(cat_path, mode='r', encoding='utf-8-sig') as cat_file:
                reader = csv.DictReader(cat_file, delimiter=',', quotechar='"')

                # –î–æ–¥–∞—Ç–∫–æ–≤–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏
                first_row = next(reader, None)
                if first_row:
                    logging.debug(f"üîç –ü–µ—Ä—à–∞ —Å—Ç—Ä–æ–∫–∞ –∑ —Ñ–∞–π–ª—É –∫–∞—Ç–µ–≥–æ—Ä—ñ–π: {first_row}")
                    # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –∫—É—Ä—Å–æ—Ä –Ω–∞ –ø–æ—á–∞—Ç–æ–∫ –¥–ª—è –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ –∑—á–∏—Ç—É–≤–∞–Ω–Ω—è
                    cat_file.seek(0)
                    reader = csv.DictReader(cat_file, delimiter=',', quotechar='"')

                log_count = 0
                for row in reader:
                    name = row.get('name', '').strip().lower()
                    term_id = row.get('term_id', '').strip()
                    if name and term_id.isdigit():
                        categories_map[name] = int(term_id)
                        if log_count < 5:
                            logging.debug(f"üîç [MAP] –î–æ–¥–∞–Ω–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—é: '{name}' -> ID: {term_id}")
                            log_count += 1

            logging.info(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(categories_map)} –∫–∞—Ç–µ–≥–æ—Ä—ñ–π –∑ {cat_path}")
        else:
            logging.warning("‚ö†Ô∏è –§–∞–π–ª –∫–∞—Ç–µ–≥–æ—Ä—ñ–π –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∞–±–æ —à–ª—è—Ö –ø–æ—Ä–æ–∂–Ω—ñ–π.")
    except Exception as e:
        logging.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ –∫–∞—Ç–µ–≥–æ—Ä—ñ–π: {e}")
        categories_map = {}

    # --- –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è ---
    BATCH_SIZE = 50
    products_to_create = []
    total_products_read = 0
    total_created = 0
    total_skipped = 0
    errors_list = []
    start_time = time.time()

    STANDARD_FIELDS = ['sku', 'post_date', 'excerpt', 'content', 'product_type']
    ACF_PREFIX = '–ú–µ—Ç–∞: '
    ATTRIBUTE_PREFIX = 'attribute:'

    try:
        with open(csv_path, mode='r', encoding='utf-8') as f:
            reader = csv.reader(f)
            headers = next(reader)
            field_map = {header: idx for idx, header in enumerate(headers)}

            for row in reader:
                total_products_read += 1
                sku = row[field_map.get('sku', -1)].strip()
                name = row[field_map.get('name', -1)].strip()

                if not sku or not name:
                    errors_list.append(f"–†—è–¥–æ–∫ {total_products_read}: –ü—Ä–æ–ø—É—â–µ–Ω–æ. –í—ñ–¥—Å—É—Ç–Ω—ñ–π SKU –∞–±–æ Name.")
                    total_skipped += 1
                    continue

                # --- –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞, —á–∏ —Ç–æ–≤–∞—Ä –∑—ñ SKU –≤–∂–µ —ñ—Å–Ω—É—î —É WooCommerce ---
                try:
                    existing_products = wcapi.get("products", params={"sku": sku}).json()
                    if isinstance(existing_products, list) and existing_products:
                        logging.warning(f"‚ö†Ô∏è –¢–æ–≤–∞—Ä –∑—ñ SKU {sku} –≤–∂–µ —ñ—Å–Ω—É—î ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é.")
                        total_skipped += 1
                        continue
                except Exception as e:
                    logging.error(f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –ø–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ SKU {sku} —É WooCommerce: {e}")

                product_data = {"sku": sku, "name": name, "status": "draft"}
                meta_data = []
                attributes = []
                tags = []
                images = []

                for key, index in field_map.items():
                    if index >= len(row):
                        continue
                    value = row[index].strip()
                    if not value:
                        continue

                    # --- —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ñ –ø–æ–ª—è ---
                    if key in STANDARD_FIELDS:
                        if key == 'product_type':
                            product_data['type'] = value or 'simple'
                        elif key == 'excerpt':
                            product_data['short_description'] = value
                        elif key == 'content':
                            product_data['description'] = value
                        elif key == 'post_date':
                            product_data['date_created'] = value

                    # --- meta_data ---
                    elif key.startswith(ACF_PREFIX):
                        meta_data.append({"key": key.replace(ACF_PREFIX, ''), "value": value})
                    elif key == 'rank_math_focus_keyword':
                        meta_data.append({"key": key, "value": value})

                    # --- –∞—Ç—Ä–∏–±—É—Ç–∏ ---
                    elif key.startswith(ATTRIBUTE_PREFIX):
                        attr_name = key.replace(ATTRIBUTE_PREFIX, '')
                        options = [v.strip() for v in re.split(r'[;|]', value) if v.strip()]
                        if options:
                            attr = {
                                "name": attr_name,
                                "position": len(attributes),
                                "visible": True,
                                "variation": False,
                                "options": options
                            }
                            if attr_name in global_attr_map:
                                attr["id"] = global_attr_map[attr_name]
                            attributes.append(attr)

                    # --- –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó ---
                    elif key == 'categories':
                        category_names = [c.strip().lower() for c in value.split('|') if c.strip()]
                        category_ids = []
                        for c in category_names:
                            if c in categories_map:
                                category_ids.append({"id": categories_map[c]})
                            else:
                                logging.warning(f"‚ö†Ô∏è –ö–∞—Ç–µ–≥–æ—Ä—ñ—è '{c}' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞ —É –ª–æ–∫–∞–ª—å–Ω–æ–º—É —Ñ–∞–π–ª—ñ –∫–∞—Ç–µ–≥–æ—Ä—ñ–π.")
                        if category_ids:
                            product_data['categories'] = category_ids

                    # --- —Ç–µ–≥–∏ ---
                    elif key == '–ü–æ–∑–Ω–∞—á–∫–∏':
                        tags.extend([{"name": t.strip()} for t in value.split(',') if t.strip()])

                    # --- –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è ---
                    elif key == 'image_name':
                        images = find_media_ids_for_sku(wcapi, sku, uploads_path)

                    # --- —Ü—ñ–Ω–∞, –∑–∞–ø–∞—Å–∏, —Å—Ç–∞—Ç—É—Å ---
                    elif key == 'regular_price':
                        product_data['regular_price'] = str(value)
                    elif key == 'manage_stock':
                        product_data['manage_stock'] = value.strip() in ['1', 'yes', 'true']
                    elif key == 'tax_status':
                        product_data['tax_status'] = 'taxable' if value.strip() in ['1', 'yes', 'true'] else 'none'
                    elif key == 'status':
                        product_data['status'] = 'publish' if value.strip() in ['1', 'yes', 'true'] else 'draft'
                    else:
                        product_data[key] = value

                if meta_data:
                    product_data['meta_data'] = meta_data
                if attributes:
                    product_data['attributes'] = attributes
                if tags:
                    product_data['tags'] = tags
                if images:
                    product_data['images'] = images

                products_to_create.append(product_data)

                if len(products_to_create) >= BATCH_SIZE:
                    total_created += _process_batch_create(wcapi, products_to_create, errors_list)
                    products_to_create = []

            if products_to_create:
                total_created += _process_batch_create(wcapi, products_to_create, errors_list)

    except FileNotFoundError:
        logging.critical(f"‚ùå –§–∞–π–ª {csv_path} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
        return
    except Exception as e:
        logging.critical(f"‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —á–∏—Ç–∞–Ω–Ω—ñ CSV: {e}", exc_info=True)
        return

    elapsed_time = int(time.time() - start_time)
    logging.info("--- üèÅ –ü—ñ–¥—Å—É–º–æ–∫ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ ---")
    logging.info(f"–í—Å—å–æ–≥–æ –ø—Ä–æ—á–∏—Ç–∞–Ω–æ —Ä—è–¥–∫—ñ–≤: {total_products_read}")
    logging.info(f"–£—Å–ø—ñ—à–Ω–æ —Å—Ç–≤–æ—Ä–µ–Ω–æ —Ç–æ–≤–∞—Ä—ñ–≤: {total_created}")
    logging.info(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ/–∑ –ø–æ–º–∏–ª–∫–∞–º–∏: {total_products_read - total_created}")
    logging.info(f"–ó–∞–≥–∞–ª—å–Ω–∞ —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å: {elapsed_time} —Å–µ–∫.")

    if errors_list:
        logging.warning(f"‚ö†Ô∏è –ó–Ω–∞–π–¥–µ–Ω–æ {len(errors_list)} –ø–æ–º–∏–ª–æ–∫/–ø—Ä–æ–ø—É—Å–∫—ñ–≤. –ü–µ—Ä—à—ñ 5:")
        for error in errors_list[:5]:
            logging.warning(f"-> {error}")
    else:
        logging.info("‚úÖ –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ.")

def update_image_seo_from_csv():
    """
    –û–Ω–æ–≤–ª—é—î SEO-–∞—Ç—Ä–∏–±—É—Ç–∏ –∑–æ–±—Ä–∞–∂–µ–Ω—å —É–∫—Ä–∞—ó–Ω—Å—å–∫–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ –∑ —Ñ–∞–π–ª—É csv_path_sl_new_prod.
    –õ–æ–≥—É–≤–∞–Ω–Ω—è —É —Å—Ç–∏–ª—ñ upload_ru_translation_to_wp().
    """
    log_message_to_existing_file()
    logging.info("üöÄ –ü–æ—á–∞—Ç–æ–∫ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è SEO-–∞—Ç—Ä–∏–±—É—Ç—ñ–≤ (UA) –¥–ª—è —Ç–æ–≤–∞—Ä—ñ–≤ —ñ–∑ CSV...")

    settings = load_settings()
    if not settings:
        logging.critical("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è.")
        return

    csv_path = settings["paths"].get("csv_path_sl_new_prod")
    seo_tag_path = settings["paths"].get("seo_tag")
    if not csv_path or not seo_tag_path:
        logging.critical("‚ùå –ù–µ –≤–∫–∞–∑–∞–Ω—ñ —à–ª—è—Ö–∏ –¥–æ CSV –∞–±–æ —Ñ–∞–π–ª—É —Ç–µ–≥—ñ–≤ —É settings.json.")
        return

    # --- –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —à–∞–±–ª–æ–Ω —Ç–µ–≥—ñ–≤ ---
    seo_tags_map = {}
    try:
        with open(seo_tag_path, mode='r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                category = row['category'].strip()
                seo_tags_map[category] = row
    except Exception as e:
        logging.critical(f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ñ–∞–π–ª SEO —Ç–µ–≥—ñ–≤: {e}")
        return

    # --- –û—Ç—Ä–∏–º—É—î–º–æ —Å–ø–∏—Å–æ–∫ SKU –∑ CSV ---
    skus = []
    try:
        with open(csv_path, mode='r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for idx, row in enumerate(reader, start=2):
                sku = row.get('sku') or row.get('SKU')
                category = row.get('categories') or row.get('–ö–∞—Ç–µ–≥–æ—Ä—ñ—è') or ""
                name = row.get('name') or row.get('–ù–∞–∑–≤–∞') or ""
                if not sku:
                    logging.warning(f"–†—è–¥–æ–∫ {idx}: –ø—Ä–æ–ø—É—â–µ–Ω–æ —á–µ—Ä–µ–∑ –≤—ñ–¥—Å—É—Ç–Ω—ñ–π SKU")
                    continue
                skus.append({"sku": sku.strip(), "category": category.strip(), "name": name.strip(), "row_idx": idx})
    except Exception as e:
        logging.critical(f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –ø—Ä–æ—á–∏—Ç–∞—Ç–∏ CSV: {e}")
        return

    # --- –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ WooCommerce ---
    try:
        wcapi = get_wc_api(settings)
    except Exception as e:
        logging.critical(f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è —Å—Ç–≤–æ—Ä–∏—Ç–∏ –æ–±'—î–∫—Ç WooCommerce API: {e}")
        return

    base_url = settings.get("url", "").rstrip("/")
    wp_login = settings.get("login")
    wp_pass = settings.get("pass")

    updated_count = 0
    failed_count = 0

    for item in skus:
        sku = item["sku"]
        product_name = item["name"]
        category = item["category"]
        idx = item["row_idx"]

        # --- –§–æ—Ä–º—É—î–º–æ SEO-—Ç–µ–≥–∏ –¥–ª—è —Ç–æ–≤–∞—Ä—É ---
        seo_row = seo_tags_map.get(category)
        if seo_row:
            alt = seo_row.get("alt_ukr", "").replace("{product_name}", product_name)
            caption = seo_row.get("caption_ukr", "").replace("{product_name}", product_name)
            description = seo_row.get("desc_ukr", "").replace("{product_name}", product_name)
            title = seo_row.get("name_ukr", "").replace("{product_name}", product_name)
        else:
            alt = f"–ö—É–ø–∏—Ç–∏ —Ç–æ–≤–∞—Ä {product_name} –≤ —Å–µ–∫—Å-—à–æ–ø—ñ Eros.in.ua"
            caption = f"{product_name} ‚Äì —ñ–Ω–Ω–æ–≤–∞—Ü—ñ–π–Ω–∞ —Å–µ–∫—Å-—ñ–≥—Ä–∞—à–∫–∞"
            description = f"{product_name} –∫—É–ø–∏—Ç–∏ –≤ —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω—ñ Eros.in.ua."
            title = product_name

        # --- –û—Ç—Ä–∏–º—É—î–º–æ —Ç–æ–≤–∞—Ä –∑ WooCommerce ---
        try:
            resp = wcapi.get("products", params={"sku": sku, "lang": "uk"})
            if resp.status_code != 200 or not resp.json():
                logging.warning(f"–†—è–¥–æ–∫ {idx}, SKU {sku}: —Ç–æ–≤–∞—Ä –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∞–±–æ –ø–æ–º–∏–ª–∫–∞ GET.")
                failed_count += 1
                continue
            product = resp.json()[0]
        except Exception as e:
            logging.error(f"–†—è–¥–æ–∫ {idx}, SKU {sku}: WooCommerce GET exception: {e}")
            failed_count += 1
            continue

        product_id = product.get("id")
        images = product.get("images", [])

        if not images:
            logging.warning(f"–†—è–¥–æ–∫ {idx}, SKU {sku}: —Ç–æ–≤–∞—Ä –Ω–µ –º–∞—î –∑–æ–±—Ä–∞–∂–µ–Ω—å")
            failed_count += 1
            continue

        wc_images_update = []

        # --- –û–Ω–æ–≤–ª–µ–Ω–Ω—è –∫–æ–∂–Ω–æ–≥–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è ---
        for img in images:
            img_id = img.get("id")
            src = img.get("src")

            if not img_id and src and wp_login and wp_pass:
                filename = os.path.basename(src)
                try:
                    media_search = requests.get(
                        f"{base_url}/wp-json/wp/v2/media",
                        params={"search": filename},
                        auth=(wp_login, wp_pass)
                    )
                    if media_search.status_code == 200 and media_search.json():
                        img_id = media_search.json()[0].get("id")
                except Exception as e:
                    logging.warning(f"–†—è–¥–æ–∫ {idx}, SKU {sku}: –Ω–µ –≤–¥–∞–ª–æ—Å—è –∑–Ω–∞–π—Ç–∏ –º–µ–¥—ñ–∞ —á–µ—Ä–µ–∑ WP API: {e}")

            if img_id:
                # --- –î–µ–∫–æ–¥—É—î–º–æ HTML —Å–ø–µ—Ü—ñ–∞–ª—å–Ω—ñ —Å–∏–º–≤–æ–ª–∏ –≤ —Ç–∞–π—Ç–ª—ñ ---
                safe_title = html.unescape(title)

                wc_images_update.append({"id": img_id, "alt": alt, "name": safe_title})

                # --- –û–Ω–æ–≤–ª–µ–Ω–Ω—è caption/description —á–µ—Ä–µ–∑ WP REST API ---
                if wp_login and wp_pass:
                    try:
                        media_endpoint = f"{base_url}/wp-json/wp/v2/media/{img_id}"
                        requests.put(
                            media_endpoint,
                            auth=(wp_login, wp_pass),
                            json={
                                "title": safe_title,
                                "alt_text": alt,
                                "caption": caption,
                                "description": description
                            }
                        )
                    except Exception as e:
                        logging.warning(f"–†—è–¥–æ–∫ {idx}, SKU {sku}, img_id {img_id}: –Ω–µ –≤–¥–∞–ª–æ—Å—è –æ–Ω–æ–≤–∏—Ç–∏ WP media: {e}")

        # --- –û–Ω–æ–≤–ª–µ–Ω–Ω—è —á–µ—Ä–µ–∑ WooCommerce PUT ---
        if wc_images_update and product_id:
            try:
                resp_put = wcapi.put(f"products/{product_id}", {"images": wc_images_update})
                if resp_put.status_code == 200:
                    logging.info(f"–†—è–¥–æ–∫ {idx}, SKU {sku}: —É—Å–ø—ñ—à–Ω–æ –æ–Ω–æ–≤–ª–µ–Ω–æ {len(wc_images_update)} –∑–æ–±—Ä–∞–∂–µ–Ω—å")
                    updated_count += len(wc_images_update)
                else:
                    logging.warning(f"–†—è–¥–æ–∫ {idx}, SKU {sku}: WooCommerce PUT –Ω–µ—É—Å–ø—ñ—à–Ω–∏–π: {resp_put.status_code}")
                    failed_count += len(wc_images_update)
            except Exception as e:
                logging.error(f"–†—è–¥–æ–∫ {idx}, SKU {sku}: WooCommerce PUT exception: {e}")
                failed_count += len(wc_images_update)

    logging.info(f"üéØ –û–Ω–æ–≤–ª–µ–Ω–Ω—è SEO (UA) –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –£—Å–ø—ñ—à–Ω–æ –æ–Ω–æ–≤–ª–µ–Ω–æ: {updated_count}, –Ω–µ –≤–¥–∞–ª–æ—Å—è: {failed_count}.")

def translate_and_prepare_new_prod_csv():
    """
    –°—Ç–≤–æ—Ä—é—î –Ω–æ–≤–∏–π RU CSV –Ω–∞ –æ—Å–Ω–æ–≤—ñ UA:
    - –ø–µ—Ä–µ–∫–ª–∞–¥ name ‚Üí Title_ru, content ‚Üí Content_ru, excerpt ‚Üí Excerpt_ru
    - rank_math_focus_keyword —Ç–∞ categories –∫–æ–ø—ñ—é—é—Ç—å—Å—è
    - WPML Translation ID –ø—ñ–¥—Ç—è–≥–Ω—É—Ç–æ –∑—ñ —Å—Ç–∞—Ä–æ–≥–æ –ø–µ—Ä–µ–∫–ª–∞–¥—É
    - —Ä–µ—à—Ç–∞ –ø–æ–ª—ñ–≤ –∑–∞–ø–æ–≤–Ω—é—é—Ç—å—Å—è –∑ —ñ—Å–Ω—É—é—á–æ–≥–æ RU CSV –∞–±–æ –∑–∞–ª–∏—à–∞—é—Ç—å—Å—è –ø–æ—Ä–æ–∂–Ω—ñ–º–∏
    """
    log_message_to_existing_file()
    logging.info("üöÄ –ü–æ—á–∞—Ç–æ–∫ —Ñ–æ—Ä–º—É–≤–∞–Ω–Ω—è –Ω–æ–≤–æ–≥–æ RU CSV...")

    settings = load_settings()
    input_path = settings["paths"].get("csv_path_sl_new_prod")
    output_path = settings["paths"].get("csv_path_sl_new_prod_ru")
    api_key = settings.get("deepl_api_key")
    api_url = settings.get("DEEPL_API_URL", "https://api-free.deepl.com/v2/translate")

    if not all([input_path, output_path, api_key]):
        logging.error("‚ùå –ù–µ –≤–∫–∞–∑–∞–Ω—ñ –≤—Å—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ —É settings.json")
        return

    # üî∏ –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∑–∞–ª–∏—à–æ–∫ —Å–∏–º–≤–æ–ª—ñ–≤ –ø–µ—Ä–µ–¥ –ø–æ—á–∞—Ç–∫–æ–º
    get_deepl_usage(api_key)

    # --- –ó—á–∏—Ç—É–≤–∞–Ω–Ω—è —ñ—Å–Ω—É—é—á–∏—Ö RU –ø–µ—Ä–µ–∫–ª–∞–¥—ñ–≤ ---
    existing_translations = {}
    try:
        with open(output_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                sku = row.get("Sku") or row.get("sku")
                if sku:
                    existing_translations[sku] = row
        logging.info(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ —ñ—Å–Ω—É—é—á—ñ –ø–µ—Ä–µ–∫–ª–∞–¥–∏: {len(existing_translations)}")
    except FileNotFoundError:
        logging.warning(f"–§–∞–π–ª –ø–µ—Ä–µ–∫–ª–∞–¥—ñ–≤ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {output_path}. –°—Ç–≤–æ—Ä–∏–º–æ –Ω–æ–≤–∏–π.")

    # --- –û—Å–Ω–æ–≤–Ω–∞ –ª–æ–≥—ñ–∫–∞ ---
    try:
        with open(input_path, 'r', encoding='utf-8') as f_in, \
             open(output_path, 'w', encoding='utf-8', newline='') as f_out:

            reader = csv.DictReader(f_in)
            output_headers = [
                "Sku","Title_ru","Content_ru","Excerpt_ru","Date","categories","Post Type","Permalink","WPML Translation ID",
                "WPML Language Code","Parent Product ID","_wpml_import_language_code","_wpml_import_source_language_code",
                "_wpml_import_translation_group","Price","Regular Price","Sale Price","Stock Status","Stock",
                "External Product URL","Total Sales","Product Type","Shipping Class","Product Visibility","Image URL",
                "Image Filename","Image Path","Image ID","Image Title","Image Caption","Image Description","Image Alt Text",
                "Image Featured","–ë—Ä–µ–Ω–¥–∏","–ö–∞—Ç–µ–≥–æ—Ä—ñ—ó —Ç–æ–≤–∞—Ä—ñ–≤","Product Tags","Translation Priorities",
                "rank_math_internal_links_processed","_low_stock_amount","rank_math_focus_keyword"
            ]
            writer = csv.DictWriter(f_out, fieldnames=output_headers)
            writer.writeheader()

            for idx, row in enumerate(reader, start=2):
                sku = row.get("sku")
                if not sku:
                    logging.warning(f"–†—è–¥–æ–∫ {idx}: –ø—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ–º–∞—î SKU)")
                    continue

                new_row = {col: "" for col in output_headers}
                new_row["Sku"] = sku

                # üîπ –ü–µ—Ä–µ–∫–ª–∞–¥–∏ —É –Ω–æ–≤—ñ –∫–æ–ª–æ–Ω–∫–∏
                new_row["Title_ru"] = translate_text_deepl(row.get("name", "").strip(), "RU", api_key, api_url)
                new_row["Content_ru"] = translate_text_deepl(row.get("content", "").strip(), "RU", api_key, api_url)
                new_row["Excerpt_ru"] = translate_text_deepl(row.get("excerpt", "").strip(), "RU", api_key, api_url)

                # üîπ –ö–æ–ø—ñ—é–≤–∞–Ω–Ω—è –±–µ–∑ –ø–µ—Ä–µ–∫–ª–∞–¥—É
                new_row["rank_math_focus_keyword"] = row.get("rank_math_focus_keyword", "")
                new_row["categories"] = row.get("categories", "")

                # üîπ WPML
                ru_row = existing_translations.get(sku, {})
                new_row["WPML Language Code"] = "ru"
                new_row["WPML Translation ID"] = ru_row.get("WPML Translation ID", "")

                # üîπ –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ –º–æ–≤–∏ –¥–ª—è —ñ–º–ø–æ—Ä—Ç—É
                new_row["_wpml_import_language_code"] = "ru"
                new_row["_wpml_import_source_language_code"] = "uk"

                # üîπ –ü—ñ–¥—Ç—è–≥—É—î–º–æ —Ä–µ—à—Ç—É –ø–æ–ª—ñ–≤
                for key, value in ru_row.items():
                    if key in output_headers and key not in ["Sku","Title_ru","Content_ru","Excerpt_ru","categories","rank_math_focus_keyword","WPML Language Code","WPML Translation ID"]:
                        new_row[key] = value

                writer.writerow(new_row)
                logging.info(f"‚úÖ –†—è–¥–æ–∫ {idx}: SKU {sku} –ø–µ—Ä–µ–∫–ª–∞–¥–µ–Ω–æ")

        logging.info(f"üéØ –ì–æ—Ç–æ–≤–æ! –§–∞–π–ª –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {output_path}")

        # üîπ –ü—ñ–¥—Ç—è–≥—É—î–º–æ _wpml_import_translation_group
        logging.info("üîÑ –û–Ω–æ–≤–ª—é—î–º–æ –∫–æ–ª–æ–Ω–∫—É _wpml_import_translation_group...")
        fill_wpml_translation_group()
        logging.info("üèÅ –û–Ω–æ–≤–ª–µ–Ω–Ω—è _wpml_import_translation_group –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")

        # üî∏ –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∑–∞–ª–∏—à–æ–∫ —Å–∏–º–≤–æ–ª—ñ–≤ –ø—ñ—Å–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è
        get_deepl_usage(api_key)

    except Exception as e:
        logging.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å —Ñ–æ—Ä–º—É–≤–∞–Ω–Ω—è CSV: {e}")

def upload_ru_translation_to_wp():
    """
    –°—Ç–≤–æ—Ä—é—î RU –ø–µ—Ä–µ–∫–ª–∞–¥ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç—É UA —á–µ—Ä–µ–∑ WPML.
    –ë–µ—Ä–µ—Ç—å—Å—è SKU –∑ –æ—Ä–∏–≥—ñ–Ω–∞–ª—É, –≤—Å—ñ –¥–∞–Ω—ñ (images, attributes, categories) –∫–æ–ø—ñ—é—é—Ç—å—Å—è –∑ –æ—Ä–∏–≥—ñ–Ω–∞–ª—É,
    –∞ –ø–æ–ª—è name/content/excerpt –±–µ—Ä—É—Ç—å—Å—è –∑ CSV.
    WPML –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –ø—ñ–¥—Å—Ç–∞–≤–ª—è—î SKU —Ç–∞ –∑–≤'—è–∑–æ–∫ –ø–µ—Ä–µ–∫–ª–∞–¥—É.
    """
    log_message_to_existing_file()
    logging.info("üöÄ –ü–æ—á–∞—Ç–æ–∫ —ñ–º–ø–æ—Ä—Ç—É RU –ø–µ—Ä–µ–∫–ª–∞–¥—ñ–≤ —á–µ—Ä–µ–∑ WPML...")

    settings = load_settings()
    csv_path = settings["paths"].get("csv_path_sl_new_prod_ru")
    if not csv_path:
        logging.error("‚ùå –ù–µ –≤–∫–∞–∑–∞–Ω–æ —à–ª—è—Ö –¥–æ csv_path_sl_new_prod_ru —É settings.json")
        return

    try:
        wcapi = get_wc_api(settings)

        with open(csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)

            for idx, row in enumerate(reader, start=2):
                sku = row.get("Sku")
                trid = row.get("_wpml_import_translation_group")
                if not sku or not trid:
                    logging.warning(f"–†—è–¥–æ–∫ {idx}: –ø—Ä–æ–ø—É—â–µ–Ω–æ —á–µ—Ä–µ–∑ –≤—ñ–¥—Å—É—Ç–Ω—ñ–π SKU –∞–±–æ trid")
                    continue

                # --- 1Ô∏è‚É£ –û—Ç—Ä–∏–º–∞—Ç–∏ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π –ø—Ä–æ–¥—É–∫—Ç UA ---
                resp = wcapi.get("products", params={"sku": sku, "lang": "uk"})
                if not resp.ok:
                    logging.warning(f"‚ö†Ô∏è SKU {sku}: –Ω–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π –ø—Ä–æ–¥—É–∫—Ç UA: {resp.text}")
                    continue

                products = resp.json()
                if not products:
                    logging.warning(f"‚ö†Ô∏è SKU {sku}: –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π –ø—Ä–æ–¥—É–∫—Ç UA –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
                    continue

                original = products[0]
                original_id = original.get("id")

                # --- 2Ô∏è‚É£ –ö–∞—Ç–µ–≥–æ—Ä—ñ—ó —Ç–∞ –∞—Ç—Ä–∏–±—É—Ç–∏ –≤ RU ---
                categories_ru = []
                for cat in original.get("categories", []):
                    cat_id = cat.get("id")
                    if cat_id:
                        cat_resp = wcapi.get(f"products/categories/{cat_id}", params={"lang": "ru"})
                        if cat_resp.ok:
                            cat_data = cat_resp.json()
                            categories_ru.append({"id": cat_data["id"], "name": cat_data["name"]})
                        else:
                            categories_ru.append({"name": cat.get("name")})
                    else:
                        categories_ru.append({"name": cat.get("name")})

                attributes_ru = []
                for attr in original.get("attributes", []):
                    attr_id = attr.get("id")
                    if attr_id:
                        attr_resp = wcapi.get(f"products/attributes/{attr_id}/terms", params={"lang": "ru"})
                        if attr_resp.ok:
                            options_ru = [v.get("name") for v in attr_resp.json()]
                        else:
                            options_ru = [v for v in attr.get("options", [])]
                        attributes_ru.append({
                            "id": attr_id,
                            "name": attr.get("name"),
                            "position": attr.get("position", 0),
                            "visible": attr.get("visible", True),
                            "variation": attr.get("variation", False),
                            "options": options_ru
                        })
                    else:
                        attributes_ru.append(attr)

                # --- 3Ô∏è‚É£ –î–∞–Ω—ñ –ø–µ—Ä–µ–∫–ª–∞–¥—É ---
                translated_data = {
                    "lang": "ru",  # <-- –û–ë–û–í'–Ø–ó–ö–û–í–û
                    "translation_of": original_id,  # –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π –ø—Ä–æ–¥—É–∫—Ç UA
                    "name": row.get("Title_ru", ""),
                    "description": row.get("Content_ru", ""),
                    "short_description": row.get("Excerpt_ru", ""),
                    "meta_data": original.get("meta_data", []) + [
                        {"key": "_wpml_import_translation_group", "value": trid},
                        {"key": "_wpml_import_language_code", "value": "ru"},
                        {"key": "_wpml_import_source_language_code", "value": "ua"}
                    ],
                    "categories": categories_ru,
                    "attributes": attributes_ru,
                    "images": original.get("images", []),
                    "type": original.get("type", "simple"),
                    "stock_status": original.get("stock_status", "instock"),
                    "regular_price": original.get("regular_price", ""),
                    "sale_price": original.get("sale_price", "")
                    # SKU –Ω–µ –ø–µ—Ä–µ–¥–∞—î—Ç—å—Å—è, WPML –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –ø—ñ–¥—Å—Ç–∞–≤–∏—Ç—å –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π
                }

                # --- 4Ô∏è‚É£ –°—Ç–≤–æ—Ä—é—î–º–æ –ø–µ—Ä–µ–∫–ª–∞–¥ RU ---
                post_resp = wcapi.post("products", translated_data)
                if post_resp.ok:
                    new_id = post_resp.json().get("id")
                    logging.info(f"üÜï SKU {sku}: —Å—Ç–≤–æ—Ä–µ–Ω–æ RU –ø–µ—Ä–µ–∫–ª–∞–¥ (ID {new_id})")
                else:
                    logging.warning(f"‚ö†Ô∏è SKU {sku}: –Ω–µ –≤–¥–∞–ª–æ—Å—è —Å—Ç–≤–æ—Ä–∏—Ç–∏ RU –ø–µ—Ä–µ–∫–ª–∞–¥. {post_resp.text}")

        logging.info("‚úÖ –Ü–º–ø–æ—Ä—Ç RU –ø–µ—Ä–µ–∫–ª–∞–¥—ñ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ!")

    except Exception as e:
        logging.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å —ñ–º–ø–æ—Ä—Ç—É –ø–µ—Ä–µ–∫–ª–∞–¥—ñ–≤: {e}")

def update_image_seo_ru_from_csv():
    """
    –ö–æ—Å—Ç–∏–ª—å. –ü–æ—Ç—Ä—ñ–±–Ω–æ –æ–Ω–æ–≤–ª—é–≤–∞—Ç–∏ –≤—Å—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è, –∞–ª–µ —Ñ—É–Ω–∫—Ü—ñ—è –∑–∞—Ç–∏—Ä–∞—î —É–∫—Ä–∞—ó–Ω—Å—å–∫—ñ —Ç–µ–≥–∏.
    –û–Ω–æ–≤–ª—é—î SEO-–∞—Ç—Ä–∏–±—É—Ç–∏ —Ç—ñ–ª—å–∫–∏ –ø–µ—Ä—à–æ–≥–æ (–≥–æ–ª–æ–≤–Ω–æ–≥–æ) –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
    —Ä–æ—Å—ñ–π—Å—å–∫–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ —É csv_path_sl_new_prod_ru.
    –õ–æ–≥—É–≤–∞–Ω–Ω—è –≤–∏–∫–æ–Ω–∞–Ω–æ —É —Å—Ç–∏–ª—ñ upload_ru_translation_to_wp().
    """
    log_message_to_existing_file()
    logging.info("üöÄ –ü–æ—á–∞—Ç–æ–∫ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è SEO-–∞—Ç—Ä–∏–±—É—Ç—ñ–≤ (RU) —Ç—ñ–ª—å–∫–∏ –¥–ª—è –≥–æ–ª–æ–≤–Ω–∏—Ö –∑–æ–±—Ä–∞–∂–µ–Ω—å...")

    settings = load_settings()
    if not settings:
        logging.critical("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è.")
        return

    csv_path = settings["paths"].get("csv_path_sl_new_prod_ru")
    seo_tag_path = settings["paths"].get("seo_tag")
    if not csv_path or not seo_tag_path:
        logging.critical("‚ùå –ù–µ –≤–∫–∞–∑–∞–Ω—ñ —à–ª—è—Ö–∏ –¥–æ CSV –∞–±–æ seo_tag —É settings.json.")
        return

    # --- –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —à–∞–±–ª–æ–Ω —Ç–µ–≥—ñ–≤ ---
    seo_tags_map = {}
    try:
        with open(seo_tag_path, mode='r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                category = row['category'].strip()
                seo_tags_map[category] = row
    except Exception as e:
        logging.critical(f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ñ–∞–π–ª SEO —Ç–µ–≥—ñ–≤: {e}")
        return

    # --- –û—Ç—Ä–∏–º—É—î–º–æ —Å–ø–∏—Å–æ–∫ SKU ---
    skus = []
    try:
        with open(csv_path, mode='r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for idx, row in enumerate(reader, start=2):
                sku = row.get('Sku') or row.get('sku')
                category = row.get('categories') or ""
                name = row.get('Title_ru') or ""
                if not sku:
                    logging.warning(f"–†—è–¥–æ–∫ {idx}: –ø—Ä–æ–ø—É—â–µ–Ω–æ (–≤—ñ–¥—Å—É—Ç–Ω—ñ–π SKU)")
                    continue
                skus.append({"sku": sku.strip(), "category": category.strip(), "name": name.strip(), "row_idx": idx})
    except Exception as e:
        logging.critical(f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –ø—Ä–æ—á–∏—Ç–∞—Ç–∏ CSV: {e}")
        return

    # --- –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ WooCommerce ---
    try:
        wcapi = get_wc_api(settings)
    except Exception as e:
        logging.critical(f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è —Å—Ç–≤–æ—Ä–∏—Ç–∏ –æ–±'—î–∫—Ç WooCommerce API: {e}")
        return

    base_url = settings.get("url", "").rstrip("/")
    wp_login = settings.get("login")
    wp_pass = settings.get("pass")

    updated_count = 0
    failed_count = 0

    for item in skus:
        sku = item["sku"]
        product_name = item["name"]
        category = item["category"]
        idx = item["row_idx"]

        # --- –§–æ—Ä–º—É—î–º–æ SEO-–¥–∞–Ω—ñ ---
        seo_row = seo_tags_map.get(category)
        if seo_row:
            alt = seo_row.get("alt_ru", "").replace("{product_name}", product_name)
            caption = seo_row.get("caption_ru", "").replace("{product_name}", product_name)
            description = seo_row.get("desc_ru", "").replace("{product_name}", product_name)
            title = seo_row.get("name_ru", "").replace("{product_name}", product_name)
        else:
            alt = f"–ö—É–ø–∏—Ç—å {product_name} –≤ —Å–µ–∫—Å-—à–æ–ø–µ Eros.in.ua"
            caption = f"{product_name} ‚Äì –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–∞—è —Å–µ–∫—Å-–∏–≥—Ä—É—à–∫–∞"
            description = f"{product_name} –∫—É–ø–∏—Ç—å –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω–µ Eros.in.ua."
            title = product_name

        # --- –û—Ç—Ä–∏–º—É—î–º–æ —Ç–æ–≤–∞—Ä ---
        try:
            resp = wcapi.get("products", params={"sku": sku, "lang": "ru"})
            if resp.status_code != 200 or not resp.json():
                logging.warning(f"–†—è–¥–æ–∫ {idx}, SKU {sku}: —Ç–æ–≤–∞—Ä –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∞–±–æ –ø–æ–º–∏–ª–∫–∞ GET ({resp.status_code})")
                failed_count += 1
                continue
            product = resp.json()[0]
        except Exception as e:
            logging.error(f"–†—è–¥–æ–∫ {idx}, SKU {sku}: WooCommerce GET exception: {e}")
            failed_count += 1
            continue

        product_id = product.get("id")
        images = product.get("images", [])

        if not images:
            logging.warning(f"–†—è–¥–æ–∫ {idx}, SKU {sku}: —Ç–æ–≤–∞—Ä –Ω–µ –º–∞—î –∑–æ–±—Ä–∞–∂–µ–Ω—å")
            failed_count += 1
            continue

        # --- –ë–µ—Ä–µ–º–æ —Ç—ñ–ª—å–∫–∏ –ø–µ—Ä—à–µ (–≥–æ–ª–æ–≤–Ω–µ) –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è ---
        main_image = images[0]
        img_id = main_image.get("id")
        src = main_image.get("src")

        if not img_id and src and wp_login and wp_pass:
            filename = os.path.basename(src)
            try:
                media_search = requests.get(
                    f"{base_url}/wp-json/wp/v2/media",
                    params={"search": filename},
                    auth=(wp_login, wp_pass)
                )
                if media_search.status_code == 200 and media_search.json():
                    img_id = media_search.json()[0].get("id")
            except Exception as e:
                logging.warning(f"–†—è–¥–æ–∫ {idx}, SKU {sku}: –Ω–µ –≤–¥–∞–ª–æ—Å—è –∑–Ω–∞–π—Ç–∏ –º–µ–¥—ñ–∞ —á–µ—Ä–µ–∑ WP API: {e}")

        # --- –û–Ω–æ–≤–ª—é—î–º–æ –ª–∏—à–µ –≥–æ–ª–æ–≤–Ω–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è ---
        if img_id:
            try:
                media_endpoint = f"{base_url}/wp-json/wp/v2/media/{img_id}"
                wp_resp = requests.put(
                    media_endpoint,
                    auth=(wp_login, wp_pass),
                    json={
                        "title": title,
                        "alt_text": alt,
                        "caption": caption,
                        "description": description
                    }
                )
                if wp_resp.status_code == 200:
                    logging.info(f"‚úÖ –†—è–¥–æ–∫ {idx}, SKU {sku}: –æ–Ω–æ–≤–ª–µ–Ω–æ –≥–æ–ª–æ–≤–Ω–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è (img_id {img_id})")
                    updated_count += 1
                else:
                    logging.warning(f"‚ö†Ô∏è –†—è–¥–æ–∫ {idx}, SKU {sku}: WP PUT {wp_resp.status_code} ‚Äî {wp_resp.text}")
                    failed_count += 1
            except Exception as e:
                logging.warning(f"‚ùå –†—è–¥–æ–∫ {idx}, SKU {sku}, img_id {img_id}: –ø–æ–º–∏–ª–∫–∞ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è WP media: {e}")
                failed_count += 1
        else:
            logging.warning(f"‚ö†Ô∏è –†—è–¥–æ–∫ {idx}, SKU {sku}: –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ ID –≥–æ–ª–æ–≤–Ω–æ–≥–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è")
            failed_count += 1

    logging.info(f"üéØ –ó–∞–≤–µ—Ä—à–µ–Ω–æ. –û–Ω–æ–≤–ª–µ–Ω–æ –≥–æ–ª–æ–≤–Ω–∏—Ö –∑–æ–±—Ä–∞–∂–µ–Ω—å: {updated_count}, –ø–æ–º–∏–ª–æ–∫: {failed_count}")
