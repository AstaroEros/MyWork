import csv
import os
import time
import requests
import shutil
import re
import pandas as pd
import mimetypes
from bs4 import BeautifulSoup
import random 
from PIL import Image
import logging
from typing import Dict, Tuple, List, Optional, Any
from scr.base_function import get_wc_api, load_settings, setup_new_log_file, log_message_to_existing_file, load_attributes_csv, \
                                save_attributes_csv, load_category_csv, save_category_csv, load_poznachky_csv, \
                                _process_batch_update, find_media_ids_for_sku, _process_batch_create, clear_directory, \
                                download_product_images, move_gifs, convert_to_webp_square, sync_webp_column, copy_to_site
from datetime import datetime, timedelta


def find_new_products():
    """
    –ü–æ—Ä—ñ–≤–Ω—é—î –∞—Ä—Ç–∏–∫—É–ª–∏ —Ç–æ–≤–∞—Ä—ñ–≤ –∑ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç–∞ –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ –∑ –∞—Ä—Ç–∏–∫—É–ª–∞–º–∏,
    —â–æ —î –Ω–∞ —Å–∞–π—Ç—ñ, —ñ –∑–∞–ø–∏—Å—É—î –Ω–æ–≤—ñ —Ç–æ–≤–∞—Ä–∏ –≤ –æ–∫—Ä–µ–º–∏–π —Ñ–∞–π–ª.
    """
    # --- 1. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ª–æ–≥—É–≤–∞–Ω–Ω—è ---
    log_message_to_existing_file()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 1. –ü–æ—á–∏–Ω–∞—é –ø–æ—à—É–∫ –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤...")
    
    # --- 2. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å –∑ settings.json ---
    settings = load_settings()
    
    # --- 3. –û—Ç—Ä–∏–º–∞–Ω–Ω—è —à–ª—è—Ö—ñ–≤ –¥–æ –ø–æ—Ç—Ä—ñ–±–Ω–∏—Ö —Ñ–∞–π–ª—ñ–≤ ---
    zalishki_path = settings['paths']['csv_path_zalishki']                   # –ë–∞–∑–∞ —ñ—Å–Ω—É—é—á–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤
    supliers_new_path = settings['paths']['csv_path_supliers_1_new']         # –§–∞–π–ª, –∫—É–¥–∏ –±—É–¥–µ –∑–∞–ø–∏—Å–∞–Ω–æ –Ω–æ–≤—ñ —Ç–æ–≤–∞—Ä–∏
    supliers_csv_path = settings['suppliers']['1']['csv_path']               # –ü—Ä–∞–π—Å-–ª–∏—Å—Ç –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ 1
    delimiter = settings['suppliers']['1']['delimiter']                      # –†–æ–∑–¥—ñ–ª—å–Ω–∏–∫ —É CSV
    
    # --- 4. –û—Ç—Ä–∏–º–∞–Ω–Ω—è –¥–æ–ø–æ–º—ñ–∂–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ ---
    sku_prefix = settings['suppliers']['1']['search']                        # –ü—Ä–µ—Ñ—ñ–∫—Å –¥–ª—è –ø–æ—à—É–∫—É
    bad_words = [word.lower() for word in settings['suppliers']['1'].get('bad_words', [])]  # –ó–∞–±–æ—Ä–æ–Ω–µ–Ω—ñ —Å–ª–æ–≤–∞ (—Ñ—ñ–ª—å—Ç—Ä)
    
    # --- 5. –û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤ –Ω–æ–≤–æ–≥–æ —Ñ–∞–π–ª—É ---
    new_product_headers = [
        settings['column_supliers_1_new_name'][str(i)]
        for i in range(len(settings['column_supliers_1_new_name']))
    ]
    num_new_columns = len(new_product_headers)

    logging.info("–ó—á–∏—Ç—É—é —ñ—Å–Ω—É—é—á—ñ –∞—Ä—Ç–∏–∫—É–ª–∏ –∑ —Ñ–∞–π–ª—É, –≤–∫–∞–∑–∞–Ω–æ–≥–æ –∑–∞ –∫–ª—é—á–µ–º 'csv_path_zalishki'.")

    try:
        # --- 6. –ó—á–∏—Ç—É–≤–∞–Ω–Ω—è —ñ—Å–Ω—É—é—á–∏—Ö –∞—Ä—Ç–∏–∫—É–ª—ñ–≤ —ñ–∑ –±–∞–∑–∏ (zalishki.csv) ---
        with open(zalishki_path, mode='r', encoding='utf-8') as zalishki_file:
            zalishki_reader = csv.reader(zalishki_file)
            next(zalishki_reader, None)  # –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫
            existing_skus = {row[9].strip().lower() for row in zalishki_reader if len(row) > 9}
            logging.info(f"–ó—á–∏—Ç–∞–Ω–æ {len(existing_skus)} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –∞—Ä—Ç–∏–∫—É–ª—ñ–≤ —ñ–∑ –±–∞–∑–∏.")

        # --- 7. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –Ω–æ–≤–æ–≥–æ —Ñ–∞–π–ª—É –¥–ª—è –∑–∞–ø–∏—Å—É –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ ---
        logging.info("–í—ñ–¥–∫—Ä–∏–≤–∞—é —Ñ–∞–π–ª –¥–ª—è –∑–∞–ø–∏—Å—É –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤...")
        with open(supliers_new_path, mode='w', encoding='utf-8', newline='') as new_file:
            writer = csv.writer(new_file)
            writer.writerow(new_product_headers)  # –∑–∞–ø–∏—Å—É—î–º–æ –∑–∞–≥–æ–ª–æ–≤–∫–∏
            
            # --- 8. –ó—á–∏—Ç—É–≤–∞–Ω–Ω—è –ø—Ä–∞–π—Å-–ª–∏—Å—Ç–∞ –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ ---
            logging.info("–ü–æ—Ä—ñ–≤–Ω—é—é –¥–∞–Ω—ñ –∑ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç–æ–º –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ 1...")
            with open(supliers_csv_path, mode='r', encoding='utf-8') as supliers_file:
                supliers_reader = csv.reader(supliers_file, delimiter=delimiter)
                next(supliers_reader, None)  # –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫
                
                # --- 9. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ª—ñ—á–∏–ª—å–Ω–∏–∫—ñ–≤ ---
                new_products_count = 0
                filtered_out_count = 0

                # --- 10. –ì–æ–ª–æ–≤–Ω–∏–π —Ü–∏–∫–ª: –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∫–æ–∂–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä—É ---
                for row in supliers_reader:
                    if not row:
                        continue
                    
                    sku = row[0].strip().lower()
                    
                    # --- 11. –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Ç–æ–≤–∞—Ä –Ω–æ–≤–∏–π (–≤—ñ–¥—Å—É—Ç–Ω—ñ–π —É –±–∞–∑—ñ) ---
                    if sku and sku not in existing_skus:
                        
                        # --- 12. –§–æ—Ä–º—É—î–º–æ –Ω–æ–≤–∏–π —Ä—è–¥–æ–∫ –∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ—é SL_new.csv ---
                        new_row = [''] * num_new_columns
                        
                        # –î–æ–¥–∞—î–º–æ –ø—Ä–µ—Ñ—ñ–∫—Å –¥–æ SKU
                        sku_with_prefix = sku_prefix + row[0]
                        new_row[0] = sku_with_prefix

                        # --- 13. –ú–∞–ø—É–≤–∞–Ω–Ω—è –∫–æ–ª–æ–Ω–æ–∫ –∑ –ø—Ä–∞–π—Å—É —É –Ω–æ–≤–∏–π CSV ---
                        column_mapping = [
                            (0, 5),   # a(0) -> f(5)
                            (1, 6),   # b(1) -> g(6)
                            (2, 7),   # c(2) -> h(7)
                            (3, 8),   # d(3) -> i(8)
                            (6, 9),   # g(6) -> j(9)
                            (7, 10),  # h(7) -> k(10)
                            (8, 11),  # i(8) -> l(11)
                            (9, 12),  # j(9) -> m(12)
                            (10, 13), # k(10) -> n(13)
                            (11, 14), # l(11) -> o(14)
                        ]
                        for source_index, dest_index in column_mapping:
                            if len(row) > source_index:
                                new_row[dest_index] = row[source_index]
                                
                        # --- 14. –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –∑–∞–±–æ—Ä–æ–Ω–µ–Ω–∏—Ö —Å–ª—ñ–≤ ---
                        should_skip = False
                        check_columns_indices = [6, 7, 10]  # –∫–æ–ª–æ–Ω–∫–∏, –¥–µ —à—É–∫–∞—î–º–æ –∑–∞–±–æ—Ä–æ–Ω–µ–Ω—ñ —Å–ª–æ–≤–∞
                        
                        for index in check_columns_indices:
                            if len(new_row) > index:
                                cell_content = new_row[index].lower()
                                for bad_word in bad_words:
                                    if bad_word in cell_content:
                                        logging.info(
                                            f"–ü—Ä–æ–ø—É—Å–∫–∞—é —Ç–æ–≤–∞—Ä '{row[0]}' —á–µ—Ä–µ–∑ —Å–ª–æ–≤–æ '{bad_word}' "
                                            f"–≤ –∫–æ–ª–æ–Ω—Ü—ñ {index} –Ω–æ–≤–æ–≥–æ —Ñ–∞–π–ª—É."
                                        )
                                        should_skip = True
                                        filtered_out_count += 1
                                        break
                                if should_skip:
                                    break
                        
                        # --- 15. –Ø–∫—â–æ —Ç–æ–≤–∞—Ä –º–∞—î –∑–∞–±–æ—Ä–æ–Ω–µ–Ω–µ —Å–ª–æ–≤–æ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ ---
                        if should_skip:
                            continue
                        
                        # --- 16. –Ø–∫—â–æ –Ω—ñ ‚Äî –¥–æ–¥–∞—î–º–æ —É —Ñ–∞–π–ª –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ ---
                        new_products_count += 1
                        writer.writerow(new_row)

        # --- 17. –ü—ñ–¥—Å—É–º–∫–æ–≤–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è ---
        logging.info(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {new_products_count} –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤.")
        logging.info(f"üö´ –í—ñ–¥—Ñ—ñ–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {filtered_out_count} —Ç–æ–≤–∞—Ä—ñ–≤ –∑–∞ –∑–∞–±–æ—Ä–æ–Ω–µ–Ω–∏–º–∏ —Å–ª–æ–≤–∞–º–∏.")
        logging.info(f"–î–∞–Ω—ñ –∑–∞–ø–∏—Å–∞–Ω–æ —É —Ñ–∞–π–ª csv 'supliers_new_path'.")

    # --- 18. –û–±—Ä–æ–±–∫–∞ –ø–æ–º–∏–ª–æ–∫ ---
    except FileNotFoundError as e:
        logging.info(f"‚ùå –ü–æ–º–∏–ª–∫–∞: –§–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ - {e}")
    except Exception as e:
        logging.info(f"‚ùå –í–∏–Ω–∏–∫–ª–∞ –Ω–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")

def find_product_data():
    """
    –ó—á–∏—Ç—É—î —Ñ–∞–π–ª –∑ –Ω–æ–≤–∏–º–∏ —Ç–æ–≤–∞—Ä–∞–º–∏, –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç—å –∑–∞ URL-–∞–¥—Ä–µ—Å–æ—é,
    –∑–Ω–∞—Ö–æ–¥–∏—Ç—å URL-–∞–¥—Ä–µ—Å—É –ø—Ä–æ—Å—Ç–æ–≥–æ –∞–±–æ –≤–∞—Ä—ñ–∞—Ç–∏–≤–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä—É,
    —ñ –∑–∞–ø–∏—Å—É—î –∑–Ω–∞–π–¥–µ–Ω—É URL-–∞–¥—Ä–µ—Å—É –≤ –∫–æ–ª–æ–Ω–∫—É B(1) –≤ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª.
    """

    # --- 1. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ª–æ–≥—É–≤–∞–Ω–Ω—è (–ø—ñ–¥–∫–ª—é—á–∞—î–º–æ —ñ—Å–Ω—É—é—á–∏–π –ª–æ–≥-—Ñ–∞–π–ª) ---
    log_message_to_existing_file()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 2. –ü–æ—á–∏–Ω–∞—é –ø–æ—à—É–∫ URL-–∞–¥—Ä–µ—Å —Ç–æ–≤–∞—Ä—ñ–≤...")
    
    # --- 2. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å —Ç–∞ —Ñ–æ—Ä–º—É–≤–∞–Ω–Ω—è —à–ª—è—Ö—ñ–≤/—Ç–∏–º—á–∞—Å–æ–≤–æ–≥–æ —Ñ–∞–π–ª—É ---
    settings = load_settings()
    supliers_new_path = settings['paths']['csv_path_supliers_1_new']  # –≤—Ö—ñ–¥–Ω–∏–π CSV (1.csv)
    site_url = settings['suppliers']['1']['site']                    # –±–∞–∑–æ–≤–∏–π URL —Å–∞–π—Ç—É (—â–æ–± –¥–æ–¥–∞–≤–∞—Ç–∏ –≤—ñ–¥–Ω–æ—Å–Ω—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è)
    temp_file_path = supliers_new_path + '.temp'                     # —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª –ø—ñ–¥ —á–∞—Å –∑–∞–ø–∏—Å—É

    # --- –õ—ñ—á–∏–ª—å–Ω–∏–∫–∏ —ñ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ---
    total_rows = 0
    found_variant_count = 0
    found_simple_count = 0
    not_found_count = 0
    found_variant_rows = []
    not_found_rows = []

    try:
        # --- 3. –í—ñ–¥–∫—Ä–∏–≤–∞—î–º–æ –≤—Ö—ñ–¥–Ω–∏–π —Ñ–∞–π–ª –¥–ª—è —á–∏—Ç–∞–Ω–Ω—è ---
        with open(supliers_new_path, mode='r', encoding='utf-8') as input_file:
            reader = csv.reader(input_file)
            headers = next(reader)  # —á–∏—Ç–∞—î–º–æ —ñ –∑–±–µ—Ä—ñ–≥–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–∫–∏ (—â–æ–± –ø–µ—Ä–µ–ø–∏—Å–∞—Ç–∏ –≤ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª)

            # --- 4. –í—ñ–¥–∫—Ä–∏–≤–∞—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª –¥–ª—è –ø–æ—Å—Ç—É–ø–æ–≤–æ–≥–æ –∑–∞–ø–∏—Å—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ ---
            with open(temp_file_path, mode='w', encoding='utf-8', newline='') as output_file:
                writer = csv.writer(output_file)
                writer.writerow(headers) # –∑–∞–ø–∏—Å—É—î–º–æ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —É —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª

                # --- 5. –Ü—Ç–µ—Ä–∞—Ü—ñ—è –ø–æ —Ä—è–¥–∫–∞—Ö –≤—Ö—ñ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª—É ---
                for idx, row in enumerate(reader):
                    total_rows += 1
                    # 5.1. –í–∏—Ç—è–≥—É—î–º–æ –∫–ª—é—á–æ–≤—ñ –ø–æ–ª—è —ñ–∑ —Ä—è–¥–∫–∞
                    search_url = row[0].strip()    # —É –≤–∏—Ö—ñ–¥–Ω–æ–º—É —Ñ–∞–π–ª—ñ —É –∫–æ–ª–æ–Ω—Ü—ñ A –º–æ–∂–µ –±—É—Ç–∏ "–ø–æ—Å–∏–ª–∞–Ω–Ω—è –¥–ª—è –ø–æ—à—É–∫—É"
                    file_sku = row[5].strip()      # –∞—Ä—Ç–∏–∫—É–ª (SKU) –∑ –∫–æ–ª–æ–Ω–∫–∏, —è–∫–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î —ñ–Ω–¥–µ–∫—Å—É 5

                    # --- 6. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –≤–∞–ª—ñ–¥–Ω–æ—Å—Ç—ñ URL –¥–ª—è –ø–æ—à—É–∫—É ---
                    # –Ø–∫—â–æ URL –ø—É—Å—Ç–∏–π –∞–±–æ –≤–∂–µ –ø–æ–∑–Ω–∞—á–µ–Ω–∏–π —è–∫ –ø–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Ç—É, –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ —Ä—è–¥–æ–∫
                    if not search_url or search_url.startswith('–ü–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Ç—É'):
                        writer.writerow(row)
                        continue

                    try:
                        # --- 7. –í–∏–∫–æ–Ω–∞–Ω–Ω—è HTTP-–∑–∞–ø–∏—Ç—É –¥–æ search_url —ñ –ø–∞—Ä—Å–∏–Ω–≥ HTML ---
                        response = requests.get(search_url)
                        response.raise_for_status()
                        soup = BeautifulSoup(response.text, 'html.parser')
                        found_type = None  # 'variant' –∞–±–æ 'simple'
                        found_url = None  # —Å—é–¥–∏ –∑–∞–ø–∏—à–µ–º–æ –∑–Ω–∞–π–¥–µ–Ω—É —Ä–µ–∞–ª—å–Ω—É URL-–∞–¥—Ä–µ—Å—É —Ç–æ–≤–∞—Ä—É
                        
                        # --- 8. –ü–æ—à—É–∫ –≤–∞—Ä—ñ–∞—Ç–∏–≤–Ω–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ (input.variant_control[data-code]) ---
                        # –®—É–∫–∞—î–º–æ input —Ç–µ–≥–∏ –∑ –∫–ª–∞—Å–æ–º variant_control —Ç–∞ –∞—Ç—Ä–∏–±—É—Ç–æ–º data-code,
                        # –ø–æ—Ä—ñ–≤–Ω—é—î–º–æ data-code –∑ file_sku ‚Äî —è–∫—â–æ —Å–ø—ñ–≤–ø–∞–¥—ñ–Ω–Ω—è, –±–µ—Ä–µ–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è —É –±–∞—Ç—å–∫—ñ–≤—Å—å–∫–æ–º—É –±–ª–æ—Ü—ñ.
                        variant_inputs = soup.find_all('input', class_='variant_control', attrs={'data-code': True})
                        for input_tag in variant_inputs:
                            site_sku = input_tag.get('data-code', '').strip()
                            if file_sku == site_sku:
                                parent_div = input_tag.find_parent('div', class_='card-block')
                                if parent_div:
                                    link_tag = parent_div.find('h4', class_='card-title').find('a')
                                    if link_tag and link_tag.has_attr('href'):
                                        # –§–æ—Ä–º—É—î–º–æ –ø–æ–≤–Ω–∏–π URL (–¥–æ–¥–∞—î–º–æ site_url –¥–æ –≤—ñ–¥–Ω–æ—Å–Ω–æ–≥–æ —à–ª—è—Ö—É)
                                        found_url = site_url + link_tag['href']
                                        found_type = 'variant'
                                        break

                        # --- 9. –Ø–∫—â–æ –Ω–µ –∑–Ω–∞–π—à–ª–∏ —Å–µ—Ä–µ–¥ –≤–∞—Ä—ñ–∞–Ω—Ç—ñ–≤ ‚Äî —à—É–∫–∞—î–º–æ –ø—Ä–æ—Å—Ç—ñ —Ç–æ–≤–∞—Ä–∏ ---
                        if not found_url:
                            # –î–ª—è –ø—Ä–æ—Å—Ç–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ —à—É–∫–∞—î–º–æ div –∑ –∫–ª–∞—Å–æ–º 'radio', –±–µ—Ä–µ–º–æ —Ç–µ–∫—Å—Ç —è–∫ SKU,
                            # —ñ –∑–∞ —Ç–∞–∫–∏–º –∂–µ –ø—ñ–¥—Ö–æ–¥–æ–º –∑–Ω–∞—Ö–æ–¥–∏–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è —É –±–ª–æ—Ü—ñ card-block.
                            simple_divs = soup.find_all('div', class_='radio')
                            for div_tag in simple_divs:
                                site_sku = div_tag.get_text(strip=True).strip()
                                if file_sku == site_sku:
                                    parent_div = div_tag.find_parent('div', class_='card-block')
                                    if parent_div:
                                        link_tag = parent_div.find('h4', class_='card-title').find('a')
                                        if link_tag and link_tag.has_attr('href'):
                                            found_url = site_url + link_tag['href']
                                            found_type = 'simple'
                                            break

                        # --- 10. –ó–∞–ø–∏—Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É —É –∫–æ–ª–æ–Ω–∫—É B (—ñ–Ω–¥–µ–∫—Å 1) –∞–±–æ –ª–æ–≥—É–≤–∞–Ω–Ω—è —è–∫—â–æ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ ---
                        if found_url:
                            row[1] = found_url
                            if found_type == 'variant':
                                found_variant_count += 1
                                found_variant_rows.append(idx + 2)  # +2, –±–æ —Ä—è–¥–∫–∏ CSV —Ä–∞—Ö—É—é—Ç—å—Å—è –∑ 1 + –∑–∞–≥–æ–ª–æ–≤–æ–∫
                            elif found_type == 'simple':
                                found_simple_count += 1
                        else:
                            not_found_count += 1
                            not_found_rows.append(idx + 2)

                        # –ó–∞–ø–∏—Å—É—î–º–æ (–∑–Ω–∞–π–¥–µ–Ω–∏–π –∞–±–æ –Ω–µ–∑–º—ñ–Ω–µ–Ω–∏–π) —Ä—è–¥–æ–∫ —É —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª
                        writer.writerow(row)

                    except requests.RequestException as e:
                        # --- 11. –û–±—Ä–æ–±–∫–∞ –ø–æ–º–∏–ª–æ–∫ HTTP-–∑–∞–ø–∏—Ç—É: –ª–æ–≥—É–≤–∞–Ω–Ω—è —Ç–∞ –º–∞—Ä–∫—É–≤–∞–Ω–Ω—è —Ä—è–¥–∫–∞ ---
                        logging.error(f"–†—è–¥–æ–∫ {idx + 2}: –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Ç—ñ –¥–æ —É—Ä–ª: {e}")
                        row[0] = f'–ü–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Ç—É: {e}'  # –ø–æ–∑–Ω–∞—á–∞—î–º–æ –ø–æ–ª–µ –ø–æ—à—É–∫—É —è–∫ –ø–æ–º–∏–ª–∫–æ–≤–µ
                        writer.writerow(row)
                    
                    # --- 12. –î–æ–¥–∞—Ç–∫–æ–≤–∞ –ø–∞—É–∑–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏ (—Ä–∞–Ω–¥–æ–º—ñ–∑–æ–≤–∞–Ω–∞) –¥–ª—è —É–Ω–∏–∫–Ω–µ–Ω–Ω—è –±–∞–Ω–∞/DDOS ---
                    time.sleep(random.uniform(1, 3))
  
        # --- 13. –ü—ñ—Å–ª—è —É—Å–ø—ñ—à–Ω–æ—ó –æ–±—Ä–æ–±–∫–∏: –∑–∞–º—ñ–Ω–∞ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª—É —Ç–∏–º—á–∞—Å–æ–≤–∏–º ---
        os.replace(temp_file_path, supliers_new_path)

        # --- 14. –ó–≤–µ–¥–µ–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ---
        logging.info("=== –ü–Ü–î–°–£–ú–ö–û–í–ê –Ü–ù–§–û–†–ú–ê–¶–Ü–Ø ===")
        logging.info(f"–í—Å—å–æ–≥–æ —Ä—è–¥–∫—ñ–≤ –∑ —Ç–æ–≤–∞—Ä–∞–º–∏: {total_rows}")
        logging.info(
            f"–ó–Ω–∞–π–¥–µ–Ω–æ URL –≤–∞—Ä—ñ–∞—Ç–∏–≤–Ω–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤: {found_variant_count}"
            + (f" (–†—è–¥–∫–∏ {', '.join(map(str, found_variant_rows))})" if found_variant_rows else "")
        )
        logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ URL –ø—Ä–æ—Å—Ç–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤: {found_simple_count}")
        logging.info(
            f"–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ URL: {not_found_count}"
            + (f" (–†—è–¥–∫–∏ {', '.join(map(str, not_found_rows))})" if not_found_rows else "")
        )

    except FileNotFoundError as e:
        # --- 15. –û–±—Ä–æ–±–∫–∞ –ø–æ–º–∏–ª–∫–∏: –≤—Ö—ñ–¥–Ω–∏–π —Ñ–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ ---
        logging.error(f"–ü–æ–º–∏–ª–∫–∞: –§–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ - {e}")
    except Exception as e:
        # --- 16. –ì–∞—Ä–∞–Ω—Ç—ñ–π–Ω–µ –ø—Ä–∏–±–∏—Ä–∞–Ω–Ω—è: –≤–∏–¥–∞–ª—è—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª –ø—Ä–∏ –ø–æ–º–∏–ª—Ü—ñ, —â–æ–± –Ω–µ –∑–∞–ª–∏—à–∏—Ç–∏ —Å–º—ñ—Ç—Ç—è ---
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)
        logging.error(f"–í–∏–Ω–∏–∫–ª–∞ –Ω–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")

def parse_product_attributes():
    """
    –ü–∞—Ä—Å–∏—Ç—å —Å—Ç–æ—Ä—ñ–Ω–∫–∏ —Ç–æ–≤–∞—Ä—ñ–≤, –∑–∞—Å—Ç–æ—Å–æ–≤—É—î –∑–∞–º—ñ–Ω—É –∑ attribute.csv (–±–ª–æ—á–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞) 
    —ñ –¥–æ–¥–∞—î –Ω–æ–≤—ñ –Ω–µ–≤—ñ–¥–æ–º—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –æ–¥—Ä–∞–∑—É –ø–µ—Ä–µ–¥ –Ω–∞—Å—Ç—É–ø–Ω–∏–º –±–ª–æ–∫–æ–º-–∑–∞–≥–æ–ª–æ–≤–∫–æ–º.
    –õ–æ–≥—É–≤–∞–Ω–Ω—è –≤–∫–ª—é—á–∞—î –ø—ñ–¥—Å—É–º–æ–∫ –¥–æ–¥–∞–Ω–∏—Ö –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ –ø–æ –∫–æ–ª–æ–Ω–∫–∞—Ö.
    """
    log_message_to_existing_file()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 3. –ü–æ—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç–æ—Ä—ñ–Ω–æ–∫ —Ç–æ–≤–∞—Ä—ñ–≤ –¥–ª—è –≤–∏–ª—É—á–µ–Ω–Ω—è –∞—Ç—Ä–∏–±—É—Ç—ñ–≤...")

    # --- 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å ---
    settings = load_settings()
    try:
        supliers_new_path = settings['paths']['csv_path_supliers_1_new']
        product_data_map = settings['suppliers']['1']['product_data_columns']
        other_attrs_index = settings['suppliers']['1']['other_attributes_column']
    except (TypeError, KeyError) as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –¥–æ—Å—Ç—É–ø—É –¥–æ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å. –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ settings.json: {e}")
        return

    # --- 2. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–∞–ø–∏ –¥–ª—è –æ–±—Ä–æ–±–∫–∏ (–±–µ–∑ –®—Ç—Ä–∏—Ö-–∫–æ–¥—É) ---
    processing_map = {k: v for k, v in product_data_map.items() if k != "–®—Ç—Ä–∏—Ö-–∫–æ–¥"}

    # --- 3. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø—Ä–∞–≤–∏–ª –∑–∞–º—ñ–Ω–∏ —Ç–∞ —Å–∏—Ä–∏—Ö –¥–∞–Ω–∏—Ö ---
    replacements_map, raw_data = load_attributes_csv()
    changes_made = False
    max_raw_row_len = len(raw_data[0]) if raw_data and raw_data[0] else 10

    # --- 4. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–æ—á–æ–∫ –≤—Å—Ç–∞–≤–∫–∏ –Ω–æ–≤–∏—Ö –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ ---
    insertion_points = {}
    current_col_index = None
    for i, row in enumerate(raw_data[1:], start=1):
        if row and row[0].strip().isdigit():
            col_index = int(row[0].strip())
            if current_col_index is not None and current_col_index not in insertion_points:
                insertion_points[current_col_index] = i
            current_col_index = col_index
            insertion_points[col_index] = i + 1
        elif current_col_index is not None:
            insertion_points[current_col_index] = i + 1

    logging.debug(f"–¢–æ—á–∫–∏ –≤—Å—Ç–∞–≤–∫–∏ (insertion_points): {insertion_points}")

    # --- 5. –°–ª–æ–≤–Ω–∏–∫ –¥–ª—è –ø—ñ–¥—Ä–∞—Ö—É–Ω–∫—É –Ω–æ–≤–∏—Ö –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ –ø–æ –∫–æ–ª–æ–Ω–∫–∞—Ö ---
    new_attributes_counter = {}  # {col_index: count}

    # --- 5.1 –°–ø–∏—Å–æ–∫ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—é —Ä—è–¥–∫—ñ–≤ –±–µ–∑ —à—Ç—Ä–∏—Ö–∫–æ–¥—ñ–≤ ---
    missing_shk_rows = []  # [—Ä—è–¥–æ–∫_—É_csv]

    # --- 6. –û–±—Ä–æ–±–∫–∞ CSV –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ ---
    temp_file_path = supliers_new_path + '.temp'
    try:
        with open(supliers_new_path, mode='r', encoding='utf-8') as input_file, \
             open(temp_file_path, mode='w', encoding='utf-8', newline='') as output_file:

            reader = csv.reader(input_file)
            writer = csv.writer(output_file)
            headers = next(reader)
            writer.writerow(headers)

            for idx, row in enumerate(reader):
                product_url = row[1].strip()
                file_sku = row[5].strip()

                # –†–æ–∑—à–∏—Ä–µ–Ω–Ω—è —Ä—è–¥–∫–∞, —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
                max_index = max(max(product_data_map.values(), default=0), other_attrs_index)
                if len(row) <= max_index:
                    row.extend([''] * (max_index + 1 - len(row)))

                if not product_url or product_url.startswith('–ü–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Ç—É'):
                    writer.writerow(row)
                    continue

                # --- 6.1 –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ ---
                try:
                    response = requests.get(product_url)
                    response.raise_for_status()
                    soup = BeautifulSoup(response.text, 'html.parser')
                    characteristics_div = soup.find('div', id='w0-tab0')
                    parsed_attributes = {}
                    if characteristics_div and characteristics_div.find('table'):
                        for tr in characteristics_div.find('table').find_all('tr'):
                            cells = tr.find_all('td')
                            if len(cells) == 2:
                                key = cells[0].get_text(strip=True).replace(':', '')
                                value = cells[1].get_text(strip=True)
                                parsed_attributes[key] = value

                    other_attributes = []

                    # --- 6.2 –û–±—Ä–æ–±–∫–∞ –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ ---
                    for attr_name, attr_value in parsed_attributes.items():
                        target_col_index = processing_map.get(attr_name)
                        original_value_lower = attr_value.strip().lower()

                        if target_col_index is not None:
                            replacement_rules = replacements_map.get(target_col_index, {})
                            new_value = replacement_rules.get(original_value_lower)

                            if new_value is not None and new_value != "":
                                row[target_col_index] = new_value
                            else:
                                if original_value_lower not in replacement_rules:
                                    insert_index = insertion_points.get(target_col_index)
                                    if insert_index is None:
                                        logging.error(f"–ê—Ç—Ä–∏–±—É—Ç '{attr_value}' (I={target_col_index}) –Ω–µ –¥–æ–¥–∞–Ω–æ: –≤—ñ–¥—Å—É—Ç–Ω—è —Ç–æ—á–∫–∞ –≤—Å—Ç–∞–≤–∫–∏.")
                                        row[target_col_index] = attr_value
                                        continue

                                    # –î–æ–¥–∞—î–º–æ –Ω–æ–≤–∏–π –∞—Ç—Ä–∏–±—É—Ç —É raw_data
                                    new_raw_row = [''] * max_raw_row_len
                                    new_raw_row[2] = original_value_lower
                                    raw_data.insert(insert_index, new_raw_row)
                                    replacements_map.setdefault(target_col_index, {})[original_value_lower] = ""
                                    changes_made = True

                                    # –ó—Å—É–≤–∞—î–º–æ —Ç–æ—á–∫–∏ –≤—Å—Ç–∞–≤–∫–∏
                                    for col, point in insertion_points.items():
                                        if point >= insert_index:
                                            insertion_points[col] += 1

                                    # –ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ –Ω–æ–≤–∏—Ö –∞—Ç—Ä–∏–±—É—Ç—ñ–≤
                                    new_attributes_counter[target_col_index] = new_attributes_counter.get(target_col_index, 0) + 1

                                row[target_col_index] = attr_value

                        elif attr_name == "–®—Ç—Ä–∏—Ö-–∫–æ–¥":
                            shk_index = product_data_map.get("–®—Ç—Ä–∏—Ö-–∫–æ–¥")
                            if shk_index is not None:
                                row[shk_index] = attr_value.strip()

                        else:
                            other_attributes.append(f"{attr_name}:{attr_value}")

                    # --- 6.3 –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ —à—Ç—Ä–∏—Ö–∫–æ–¥—É ---
                    shk_index = product_data_map.get("–®—Ç—Ä–∏—Ö-–∫–æ–¥")
                    if shk_index is not None:
                        if not row[shk_index].strip():
                            missing_shk_rows.append(idx + 2)  # +2, –±–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ = —Ä—è–¥–æ–∫ 1

                    if other_attributes:
                        row[other_attrs_index] = ', '.join(other_attributes)

                    writer.writerow(row)

                except requests.RequestException as req_err:
                    logging.error(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Ç—É –¥–ª—è URL {product_url}: {req_err}")
                    writer.writerow(row)
                except Exception as e:
                    logging.error(f"–ù–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É –¥–ª—è URL {product_url}: {e}")
                    writer.writerow(row)

                time.sleep(random.uniform(1, 3))

        os.replace(temp_file_path, supliers_new_path)
        logging.info("–ü–∞—Ä—Å–∏–Ω–≥ –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –§–∞–π–ª 1.csv –æ–Ω–æ–≤–ª–µ–Ω–æ.")

        # --- 7. –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è attribute.csv —Ç–∞ –ø—ñ–¥—Å—É–º–∫–æ–≤–∏–π –ª–æ–≥ ---
        if changes_made:
            save_attributes_csv(raw_data)
        else:
            logging.info("–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è attribute.csv –Ω–µ –ø–æ—Ç—Ä—ñ–±–Ω–µ. –ó–º—ñ–Ω: False.")

        # --- 7.1 –ü—ñ–¥—Å—É–º–∫–æ–≤–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è –Ω–æ–≤–∏—Ö –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ ---
        if new_attributes_counter:
            logging.info("–ü—ñ–¥—Å—É–º–æ–∫ –¥–æ–¥–∞–Ω–∏—Ö –Ω–æ–≤–∏—Ö –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ –ø–æ –∫–æ–ª–æ–Ω–∫–∞—Ö:")
            for col_index, count in sorted(new_attributes_counter.items()):
                logging.info(f"–ê—Ç—Ä–∏–±—É—Ç {col_index}, –¥–æ–¥–∞–Ω–æ {count} –Ω–æ–≤–∏—Ö –∞—Ç—Ä–∏–±—É—Ç—ñ–≤")
        else:
            logging.info("–ù–æ–≤—ñ –∞—Ç—Ä–∏–±—É—Ç–∏ –Ω–µ –¥–æ–¥–∞–Ω—ñ —É –∂–æ–¥–Ω—É –∫–æ–ª–æ–Ω–∫—É.")

        # --- 7.2 –õ–æ–≥—É–≤–∞–Ω–Ω—è –≤—ñ–¥—Å—É—Ç–Ω—ñ—Ö —à—Ç—Ä–∏—Ö–∫–æ–¥—ñ–≤ ---
        if missing_shk_rows:
            rows_str = ', '.join(map(str, missing_shk_rows))
            logging.warning(f"–£–í–ê–ì–ê! –ù–µ–º–∞—î —à—Ç—Ä–∏—Ö–∫–æ–¥—ñ–≤: {len(missing_shk_rows)} —à—Ç—É–∫–∏ (—Ä—è–¥–∫–∏ {rows_str})")
        else:
            logging.info("–£—Å—ñ —Ç–æ–≤–∞—Ä–∏ –º–∞—é—Ç—å —à—Ç—Ä–∏—Ö–∫–æ–¥–∏.")

    except Exception as e:
        logging.error(f"–í–∏–Ω–∏–∫–ª–∞ –Ω–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)

def apply_final_standardization():
    """
    –ó–∞—Å—Ç–æ—Å–æ–≤—É—î —Ñ—ñ–Ω–∞–ª—å–Ω—ñ –ø—Ä–∞–≤–∏–ª–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü—ñ—ó –∑ attribute.csv –¥–æ —Ñ–∞–π–ª—É 1.csv.
    –ó–∞–º—ñ–Ω—é—î –∞—Ç—Ä–∏–±—É—Ç–∏ –Ω–∞ –∑–Ω–∞—á–µ–Ω–Ω—è –∑ –∫–æ–ª–æ–Ω–∫–∏ 'attr_site_name', —è–∫—â–æ –≤–æ–Ω–æ —ñ—Å–Ω—É—î.
    –ü—Ä–æ—ñ–≥–Ω–æ—Ä–æ–≤–∞–Ω—ñ –∞—Ç—Ä–∏–±—É—Ç–∏ (–∑ –ø–æ—Ä–æ–∂–Ω—ñ–º 'attr_site_name') –æ—á–∏—â–∞—é—Ç—å—Å—è.
    –ê—Ç—Ä–∏–±—É—Ç–∏, –¥–ª—è —è–∫–∏—Ö –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –ø—Ä–∞–≤–∏–ª, –∑–∞–ª–∏—à–∞—é—Ç—å—Å—è –±–µ–∑ –∑–º—ñ–Ω.
    –õ–æ–≥—É–≤–∞–Ω–Ω—è –≤–∫–ª—é—á–∞—î —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–º—ñ–Ω —Ç–∞ –æ—á–∏—â–µ–Ω—å.
    """
    log_message_to_existing_file()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 4. –ü–æ—á–∏–Ω–∞—é —Ñ—ñ–Ω–∞–ª—å–Ω—É —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü—ñ—é –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ —É 1.csv...")

    # --- 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å ---
    settings = load_settings()
    try:
        csv_path = settings['paths']['csv_path_supliers_1_new']
        product_map = settings['suppliers']['1']['product_data_columns']
    except TypeError as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –¥–æ—Å—Ç—É–ø—É –¥–æ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å: {e}")
        return

    # --- 2. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–∞–ø–∏ –¥–ª—è –æ–±—Ä–æ–±–∫–∏ (–±–µ–∑ –®—Ç—Ä–∏—Ö-–∫–æ–¥—É) ---
    processing_map = {k: v for k, v in product_map.items() if k != "–®—Ç—Ä–∏—Ö-–∫–æ–¥"}

    # --- 3. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø—Ä–∞–≤–∏–ª –∑–∞–º—ñ–Ω–∏ ---
    replacements_map, _ = load_attributes_csv()

    # --- 4. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∑–∞–º—ñ–Ω ---
    replacement_counter = {}  # {col_index: count}
    cleared_counter = {}      # {col_index: count}

    # --- 5. –û–±—Ä–æ–±–∫–∞ CSV ---
    temp_file_path = csv_path + '.final_temp'
    try:
        with open(csv_path, 'r', encoding='utf-8') as infile, \
             open(temp_file_path, 'w', encoding='utf-8', newline='') as outfile:

            reader = csv.reader(infile)
            writer = csv.writer(outfile)
            headers = next(reader)
            writer.writerow(headers)

            # –°–ª–æ–≤–Ω–∏–∫ –¥–ª—è –ª–æ–≥—É–≤–∞–Ω–Ω—è –Ω–∞–∑–≤ –∫–æ–ª–æ–Ω–æ–∫
            column_names = {index: name for name, index in processing_map.items()}

            for idx, row in enumerate(reader):
                max_index = max(product_map.values(), default=0)
                if len(row) <= max_index:
                    row.extend([''] * (max_index + 1 - len(row)))

                for col_index, rules in replacements_map.items():
                    if col_index >= len(row):
                        continue

                    current_value = row[col_index].strip()
                    if not current_value:
                        continue

                    current_lower = current_value.lower()
                    col_name = column_names.get(col_index, f"I={col_index}")
                    new_value = rules.get(current_lower)

                    if new_value is not None:
                        if new_value:
                            if new_value != current_value:
                                row[col_index] = new_value
                                replacement_counter[col_index] = replacement_counter.get(col_index, 0) + 1
                                logging.info(f"–†—è–¥–æ–∫ {idx + 2}: –ó–ê–ú–Ü–ù–ê ({col_name}): '{current_value}' -> '{new_value}'")
                        else:
                            row[col_index] = ""
                            cleared_counter[col_index] = cleared_counter.get(col_index, 0) + 1
                            logging.warning(f"–†—è–¥–æ–∫ {idx + 2}: –Ü–ì–ù–û–†–£–í–ê–ù–ù–Ø/–û–ß–ò–©–ï–ù–ù–Ø ({col_name}): '{current_value}' –æ—á–∏—â–µ–Ω–æ")

                writer.writerow(row)

        os.replace(temp_file_path, csv_path)
        logging.info("–§—ñ–Ω–∞–ª—å–Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü—ñ—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. csv –æ–Ω–æ–≤–ª–µ–Ω–æ.")

        # --- 6. –ü—ñ–¥—Å—É–º–∫–æ–≤–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è ---
        if replacement_counter:
            for col, count in sorted(replacement_counter.items()):
                logging.info(f"–ê—Ç—Ä–∏–±—É—Ç {col}: –≤–∏–∫–æ–Ω–∞–Ω–æ {count} –∑–∞–º—ñ–Ω")
        if cleared_counter:
            for col, count in sorted(cleared_counter.items()):
                logging.info(f"–ê—Ç—Ä–∏–±—É—Ç {col}: –æ—á–∏—â–µ–Ω–æ {count} –∑–Ω–∞—á–µ–Ω—å")

    except FileNotFoundError as e:
        logging.error(f"–§–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {e}")
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)
    except Exception as e:
        logging.error(f"–ù–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü—ñ—ó: {e}")
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)

def fill_product_category():
    """
    –ó–∞–ø–æ–≤–Ω—é—î —Å–ª—É–∂–±–æ–≤—ñ –∫–æ–ª–æ–Ω–∫–∏ —É csv:
    - Q (–∫–∞—Ç–µ–≥–æ—Ä—ñ—è) –Ω–∞ –æ—Å–Ω–æ–≤—ñ M, N, O
    - T (–ø–æ–∑–Ω–∞—á–∫–∏) –Ω–∞ –æ—Å–Ω–æ–≤—ñ –Ω–∞–∑–≤–∏ —Ç–æ–≤–∞—Ä—É G
    - U (Rank Math) –Ω–∞ –æ—Å–Ω–æ–≤—ñ –Ω–∞–∑–≤–∏ —Ç–æ–≤–∞—Ä—É G
    - AV (pa_used) –Ω–∞ –æ—Å–Ω–æ–≤—ñ category.csv
    - V, W, X, Y, AZ —Ñ—ñ–∫—Å–æ–≤–∞–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏
    - Z (–∫–æ—Ä–æ—Ç–∫–∏–π –æ–ø–∏—Å) –∑ H
    - AX (–¥–∞—Ç–∞)
    –ü—Ä–∞—Ü—é—î —Ç—ñ–ª—å–∫–∏ –¥–ª—è –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ –∑ ID=1
    """
    log_message_to_existing_file()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 5. –ü–æ—á–∏–Ω–∞—é –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó —Ç–∞ —Å–ª—É–∂–±–æ–≤–∏—Ö –∫–æ–ª–æ–Ω–æ–∫...")

    settings = load_settings()
    try:
        csv_path = settings['paths']['csv_path_supliers_1_new']
        supplier_id = 1
        name_ukr = settings['suppliers']['1']['name_ukr']
    except (TypeError, KeyError) as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å: {e}")
        return

    # –Ü–Ω–¥–µ–∫—Å–∏ –∫–æ–ª–æ–Ω–æ–∫
    M, N, O = 12, 13, 14
    G, H = 6, 7
    Q, T, U = 16, 19, 20
    Z, V, W, X, Y = 25, 21, 22, 23, 24
    AV, AX, AZ = 47, 49, 51

    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø—Ä–∞–≤–∏–ª –∫–∞—Ç–µ–≥–æ—Ä—ñ–π —ñ –ø–æ–∑–Ω–∞—á–æ–∫
    category_map, raw_category = load_category_csv()
    rules_category = category_map.get(supplier_id, {})
    poznachky_list = load_poznachky_csv()
    changes_category = False
    max_row_len_category = len(raw_category[0]) if raw_category else 5

    # –°—Ç–≤–æ—Ä—é—î–º–æ –º–∞–ø—É –¥–ª—è pa_used
    pa_used_map = {}
    for row in raw_category:
        if len(row) > 5 and (row[0].strip() == str(supplier_id) or row[0].strip() == ''):
            key = tuple(v.strip().lower() for v in row[1:4])
            pa_used_map[key] = row[5].strip()

    logging.info(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(pa_used_map)} –ø—Ä–∞–≤–∏–ª pa_used")

    current_date = datetime.now().strftime('%Y-%m-%dT00:00:00')

    # –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –Ω–æ–≤–æ–≥–æ —Ä—è–¥–∫–∞ —É category.csv
    def get_insert_index(supplier_id, raw_data):
        insert_index = len(raw_data)
        found_block = False
        for i, r in enumerate(raw_data):
            if r and r[0].strip().isdigit():
                try:
                    cur_id = int(r[0].strip())
                    if cur_id == supplier_id:
                        found_block = True
                        insert_index = i + 1
                    elif cur_id > supplier_id and found_block:
                        return i
                except ValueError:
                    continue
            elif found_block:
                insert_index = i + 1
        return insert_index

    temp_path = csv_path + '.category_temp'
    try:
        with open(csv_path, 'r', encoding='utf-8') as infile, \
             open(temp_path, 'w', encoding='utf-8', newline='') as outfile:

            reader = csv.reader(infile)
            writer = csv.writer(outfile)
            headers = next(reader)
            writer.writerow(headers)

            for idx, row in enumerate(reader):
                # –†–æ–∑—à–∏—Ä—é—î–º–æ —Ä—è–¥–æ–∫ –∑–∞ –ø–æ—Ç—Ä–µ–±–∏
                max_col = max(M, N, O, Q, T, U, V, W, X, Y, Z, AV, AX, AZ, G, H)
                if len(row) <= max_col:
                    row.extend([''] * (max_col + 1 - len(row)))

                product_name = row[G].strip()
                product_desc = row[H]

                key = tuple(row[i].strip().lower() for i in (M, N, O))

                # --- –ö–∞—Ç–µ–≥–æ—Ä—ñ—è Q ---
                category_val = rules_category.get(key)
                if category_val is not None:
                    row[Q] = category_val or ""
                else:
                    # –î–æ–¥–∞—î–º–æ –Ω–æ–≤–∏–π —Ä—è–¥–æ–∫ —É category.csv
                    insert_idx = get_insert_index(supplier_id, raw_category)
                    new_row = [''] + list(row[M:O+1]) + [''] * (max_row_len_category - 4)
                    raw_category.insert(insert_idx, new_row)
                    rules_category[key] = ""
                    changes_category = True
                    logging.warning(f"–†—è–¥–æ–∫ {idx + 2}: –î–æ–¥–∞–Ω–∞ –Ω–æ–≤–∞ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—è –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó {key}")

                # --- –ü–æ–∑–Ω–∞—á–∫–∏ T ---
                if product_name and poznachky_list:
                    found_tags = []
                    covered = []
                    name_lower = product_name.lower()
                    for tag in poznachky_list:
                        if tag in name_lower:
                            start, end = name_lower.find(tag), name_lower.find(tag) + len(tag)
                            if not any(s <= start and end <= e for s, e in covered):
                                found_tags.append(tag.capitalize())
                                covered.append((start, end))
                                covered.sort(key=lambda x: x[1]-x[0], reverse=True)
                    if found_tags:
                        row[T] = ', '.join(found_tags)

                # --- Rank Math U ---
                if product_name:
                    cleaned = re.sub(r'[–∞-—è–ê-–Ø0-9]', '', product_name)
                    cleaned = re.sub(r'[^a-zA-Z\s]', '', cleaned)
                    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
                    row[U] = cleaned

                # --- pa_used AV ---
                pa_val = pa_used_map.get(key)
                if pa_val:
                    row[AV] = pa_val


                # --- –§—ñ–∫—Å–æ–≤–∞–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏ ---
                row[V] = name_ukr
                row[W] = "0"
                row[X] = "yes"
                row[Y] = "none"
                row[AZ] = "simple"
                row[AX] = current_date

                # --- –ö–æ—Ä–æ—Ç–∫–∏–π –æ–ø–∏—Å Z ---
                if product_desc:
                    row[Z] = product_desc.split('\\n', 1)[0].strip()
                else:
                    row[Z] = ""

                writer.writerow(row)

        os.replace(temp_path, csv_path)
        logging.info("–ó–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–π —Ç–∞ —Å–ª—É–∂–±–æ–≤–∏—Ö –∫–æ–ª–æ–Ω–æ–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")

        if changes_category:
            save_category_csv(raw_category)
        else:
            logging.info("–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è category.csv –Ω–µ –ø–æ—Ç—Ä—ñ–±–Ω–µ. –ó–º—ñ–Ω: False.")

    except Exception as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—ñ –∫–æ–ª–æ–Ω–æ–∫: {e}")
        if os.path.exists(temp_path):
            os.remove(temp_path)

def refill_product_category():
    """
    –ü–æ–≤—Ç–æ—Ä–Ω–æ –∑–∞–ø–æ–≤–Ω—é—î –∫–æ–ª–æ–Ω–∫–∏ Q (–ö–∞—Ç–µ–≥–æ—Ä—ñ—è) —Ç–∞ AV (pa_used) —É 1.csv
    –Ω–∞ –æ—Å–Ω–æ–≤—ñ –æ–Ω–æ–≤–ª–µ–Ω–∏—Ö –ø—Ä–∞–≤–∏–ª —É category.csv.
    –ù–ï –¥–æ–¥–∞—î –Ω–æ–≤—ñ —Ä—è–¥–∫–∏ —É category.csv.
    –õ–æ–≥—É–≤–∞–Ω–Ω—è –ø–æ–∫–∞–∑—É—î, —è–∫—ñ —Ä—è–¥–∫–∏ –æ–Ω–æ–≤–ª–µ–Ω—ñ.
    """
    log_message_to_existing_file()
    logging.info("–§—É–Ω–∫—Ü—ñ—è 6. –ü–æ—á–∏–Ω–∞—é –ø–æ–≤—Ç–æ—Ä–Ω–µ –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–π —Ç–∞ pa_used —É 1.csv...")

    # --- 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å ---
    settings = load_settings()
    try:
        csv_path = settings['paths']['csv_path_supliers_1_new']
        supplier_id = 1
    except (TypeError, KeyError) as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –¥–æ—Å—Ç—É–ø—É –¥–æ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å: {e}")
        return

    # --- 2. –Ü–Ω–¥–µ–∫—Å–∏ –∫–æ–ª–æ–Ω–æ–∫ CSV ---
    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –æ–¥—Ä–∞–∑—É —á–∏—Å–ª–∞, –±–µ–∑ –¥–æ–≤–≥–∏—Ö –∑–º—ñ–Ω–Ω–∏—Ö
    M, N, O = 12, 13, 14        # name_1, name_2, name_3
    Q, AV = 16, 47              # –ö–∞—Ç–µ–≥–æ—Ä—ñ—è —Ç–∞ pa_used
    max_index = max(M, N, O, Q, AV)
    missing_category_rows = []  # —Å–ø–∏—Å–æ–∫ —Ä—è–¥–∫—ñ–≤ –∑ –ø–æ—Ä–æ–∂–Ω—å–æ—é –∫–∞—Ç–µ–≥–æ—Ä—ñ—î—é

    # --- 3. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø—Ä–∞–≤–∏–ª –∫–∞—Ç–µ–≥–æ—Ä—ñ–π —Ç–∞ pa_used ---
    category_map, raw_category = load_category_csv()
    rules_category = {}
    pa_used_map = {}
    supplier_str = str(supplier_id)

    for row in raw_category:
        if len(row) > 5:
            supplier_value = row[0].strip()
            if supplier_value == supplier_str or supplier_value == '':
                key = tuple(v.strip().lower() for v in row[1:4])  # –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—è M,N,O
                rules_category[key] = row[4].strip() if len(row) > 4 else ""
                pa_used_map[key] = row[5].strip() if len(row) > 5 else ""

    logging.info(f"–ó—á–∏—Ç–∞–Ω–æ {len(rules_category)} –ø—Ä–∞–≤–∏–ª –¥–ª—è –ö–∞—Ç–µ–≥–æ—Ä—ñ—ó (Q) —Ç–∞ {len(pa_used_map)} –ø—Ä–∞–≤–∏–ª –¥–ª—è pa_used (AV)")

    # --- 4. –û–±—Ä–æ–±–∫–∞ CSV ---
    temp_path = csv_path + '.refill_temp'
    updated_rows = 0

    try:
        with open(csv_path, 'r', encoding='utf-8') as infile, \
             open(temp_path, 'w', encoding='utf-8', newline='') as outfile:

            reader = csv.reader(infile)
            writer = csv.writer(outfile)

            headers = next(reader)
            writer.writerow(headers)

            for idx, row in enumerate(reader):
                # –†–æ–∑—à–∏—Ä—é—î–º–æ —Ä—è–¥–æ–∫, —â–æ–± –Ω–µ –≤–∏—Ö–æ–¥–∏—Ç–∏ –∑–∞ –º–µ–∂—ñ
                if len(row) <= max_index:
                    row.extend([''] * (max_index + 1 - len(row)))

                # --- 4.1 –ö–ª—é—á –ø–æ—à—É–∫—É ---
                key = tuple(row[i].strip().lower() for i in (M, N, O))
                initial_category = row[Q].strip()
                initial_pa_used = row[AV].strip()
                row_changed = False

                # --- 4.2 –ü–æ–≤—Ç–æ—Ä–Ω–µ –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –ö–∞—Ç–µ–≥–æ—Ä—ñ—ó Q ---
                category_val = rules_category.get(key)
                if category_val and category_val != initial_category:
                    row[Q] = category_val
                    row_changed = True
                    logging.info(f"–†—è–¥–æ–∫ {idx + 2}: Q (–ö–∞—Ç–µ–≥–æ—Ä—ñ—è) –æ–Ω–æ–≤–ª–µ–Ω–æ. –ö–ª—é—á: {key}, –ó–Ω–∞—á–µ–Ω–Ω—è: '{category_val}'")

                # --- 4.3 –ü–æ–≤—Ç–æ—Ä–Ω–µ –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è pa_used AV ---
                pa_val = pa_used_map.get(key)
                if pa_val and pa_val != initial_pa_used:
                    row[AV] = pa_val
                    row_changed = True
                    logging.info(f"–†—è–¥–æ–∫ {idx + 2}: AV (pa_used) –æ–Ω–æ–≤–ª–µ–Ω–æ. –ö–ª—é—á: {key}, –ó–Ω–∞—á–µ–Ω–Ω—è: '{pa_val}'")

                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø–æ—Ä–æ–∂–Ω—å–æ—ó –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó –ø—ñ—Å–ª—è –æ–Ω–æ–≤–ª–µ–Ω–Ω—è
                if not row[Q].strip():
                    missing_category_rows.append(idx + 2)  # –∑–±–µ—Ä—ñ–≥–∞—î–º–æ –Ω–æ–º–µ—Ä —Ä—è–¥–∫–∞ —É —Ñ–∞–π–ª—ñ

                if row_changed:
                    updated_rows += 1

                writer.writerow(row)

        # --- 5. –ó–∞–º—ñ–Ω—é—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π CSV ---
        os.replace(temp_path, csv_path)
        logging.info(f"–ü–æ–≤—Ç–æ—Ä–Ω–µ –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –û–Ω–æ–≤–ª–µ–Ω–æ {updated_rows} —Ä—è–¥–∫—ñ–≤.")

        # --- –õ–æ–≥—É–≤–∞–Ω–Ω—è —Ä—è–¥–∫—ñ–≤ –∑ –ø–æ—Ä–æ–∂–Ω—å–æ—é –∫–∞—Ç–µ–≥–æ—Ä—ñ—î—é ---
        for row_num in missing_category_rows:
            logging.warning(f"–£–í–ê–ì–ê —Ä—è–¥–æ–∫ {row_num} –Ω–µ –∑–∞–ø–æ–≤–Ω–µ–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è!")

    except Exception as e:
        logging.error(f"–ù–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ–º—É –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—ñ: {e}")
        if os.path.exists(temp_path):
            os.remove(temp_path)

def separate_existing_products():
    """
    –ó–≤—ñ—Ä—è—î —à—Ç—Ä–∏—Ö–∫–æ–¥–∏ 1.csv –∑ –±–∞–∑–æ—é (zalishki.csv),
    –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—å –∑–Ω–∞–π–¥–µ–Ω—ñ —Ç–æ–≤–∞—Ä–∏ —É old_prod_new_SHK.csv,
    –≤–∏–¥–∞–ª—è—î —ó—Ö –∑ 1.csv —Ç–∞ —Ñ–æ—Ä–º—É—î –ø—ñ–¥—Å—É–º–∫–æ–≤—É —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É.
    –ö–æ–ª–æ–Ω–∫–∏ —Ç–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ—Å—Ç—ñ old -> new –≤–∏–Ω–µ—Å–µ–Ω—ñ —É settings.json.
    """
    log_message_to_existing_file()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 7. –ü–æ—á–∏–Ω–∞—é –∑–≤—ñ—Ä–∫—É 1.csv –∑—ñ —à—Ç—Ä–∏—Ö–∫–æ–¥–∞–º–∏ –±–∞–∑–∏ (zalishki.csv)...")

    settings = load_settings()
    try:
        sl_new_path = settings['paths']['csv_path_supliers_1_new']
        zalishki_path = settings['paths']['csv_path_zalishki']
        sl_old_prod_shk_path = settings['paths']['csv_path_sl_old_prod_new_shk']
        column_mapping = settings['suppliers']['1']['column_mapping_sl_old_to_sl_new']
    except KeyError as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó. –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —à–ª—è—Ö –∞–±–æ –º–∞–ø—É –∫–æ–ª–æ–Ω–æ–∫: {e}")
        return

    # --- 0. –ó—á–∏—Ç—É–≤–∞–Ω–Ω—è —ñ—Å–Ω—É—é—á–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞ old_prod_new_SHK.csv ---
    sl_old_header = []
    try:
        if os.path.exists(sl_old_prod_shk_path):
            with open(sl_old_prod_shk_path, mode='r', encoding='utf-8') as f:
                reader = csv.reader(f)
                sl_old_header = next(reader, [])
        else:
            logging.warning("–§–∞–π–ª old_prod_new_SHK.csv –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ ‚Äî —Å—Ç–≤–æ—Ä—é—é –Ω–æ–≤–∏–π —ñ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º.")
            sl_old_header_base = [
                'id', 'sku', '–ú–µ—Ç–∞: url_lutsk', '–ú–µ—Ç–∞: shtrih_cod', '–ú–µ—Ç–∞: artykul_lutsk', '–ü–æ–∑–Ω–∞—á–∫–∏',
                'rank_math_focus_keyword', '–ú–µ—Ç–∞: postachalnyk', 'manage_stock', 'tax_status', 'excerpt'
            ]
            # –î–æ–¥–∞—î–º–æ –∞—Ç—Ä–∏–±—É—Ç–∏ —Ç–∞ –¥–æ–¥–∞—Ç–∫–æ–≤—ñ –∫–æ–ª–æ–Ω–∫–∏ (–±–µ–∑ attribute_none)
            sl_old_header = sl_old_header_base + [f'attribute_{i}' for i in range(1, 24)] + [
                'content', 'post_date', 'product_type'
            ]

        # –û—á–∏—â–∞—î–º–æ —Ñ–∞–π–ª, –∞–ª–µ –∑–∞–ª–∏—à–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫
        with open(sl_old_prod_shk_path, mode='w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(sl_old_header)

        logging.info("–§–∞–π–ª old_prod_new_SHK.csv –æ—á–∏—â–µ–Ω–æ, –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∑–∞–ª–∏—à–µ–Ω–æ –±–µ–∑ –∑–º—ñ–Ω.")
    except Exception as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó old_prod_new_SHK.csv: {e}")
        return

    # --- 1. –ó—á–∏—Ç—É–≤–∞–Ω–Ω—è –±–∞–∑–∏ —à—Ç—Ä–∏—Ö–∫–æ–¥—ñ–≤ ---
    zalishki_map = {}  # {shk: (id, sku)}
    try:
        with open(zalishki_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            next(reader)  # –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫
            for row in reader:
                if len(row) > 7:
                    shk = row[7].strip()
                    if shk:
                        zalishki_map[shk] = (row[0].strip(), row[1].strip())
        logging.info(f"–ó—á–∏—Ç–∞–Ω–æ {len(zalishki_map)} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —à—Ç—Ä–∏—Ö–∫–æ–¥—ñ–≤ –∑ –±–∞–∑–∏.")
    except Exception as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —á–∏—Ç–∞–Ω–Ω—ñ –±–∞–∑–∏: {e}")
        return

    # --- 2. –û–±—Ä–æ–±–∫–∞ 1.csv —Ç–∞ —Ñ–æ—Ä–º—É–≤–∞–Ω–Ω—è —Å–ø–∏—Å–∫—ñ–≤ ---
    items_to_keep = []
    items_to_move = []

    try:
        with open(sl_new_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            header = next(reader)
            items_to_keep.append(header)

            for row in reader:
                # –†–æ–∑—à–∏—Ä—é—î–º–æ —Ä—è–¥–æ–∫ –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —ñ–Ω–¥–µ–∫—Å—É —É –º–∞–ø—ñ
                max_index = max(column_mapping.values())
                if len(row) <= max_index:
                    row.extend([''] * (max_index + 1 - len(row)))

                shk_value = row[2].strip()  # C (–®—Ç—Ä–∏—Ö–∫–æ–¥)
                if shk_value in zalishki_map:
                    item_id, item_sku = zalishki_map[shk_value]

                    # –§–æ—Ä–º—É—î–º–æ –Ω–æ–≤–∏–π —Ä—è–¥–æ–∫ –¥–ª—è old_prod_new_SHK.csv
                    new_row = [''] * len(sl_old_header)
                    new_row[0] = item_id
                    new_row[1] = item_sku

                    for sl_old_idx_str, sl_new_idx in column_mapping.items():
                        sl_old_idx = int(sl_old_idx_str)  # –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ –∫–ª—é—á —É int
                        if sl_new_idx < len(row):
                            new_row[sl_old_idx] = row[sl_new_idx]

                    items_to_move.append(new_row)
                else:
                    items_to_keep.append(row)

        # --- 3. –ó–∞–ø–∏—Å –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ ---
        if items_to_move:
            with open(sl_old_prod_shk_path, 'a', encoding='utf-8', newline='') as f:
                writer = csv.writer(f)
                writer.writerows(items_to_move)
            logging.info(f"–ü–µ—Ä–µ–Ω–µ—Å–µ–Ω–æ {len(items_to_move)} —ñ—Å–Ω—É—é—á–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ —É old_prod_new_SHK.csv.")
        else:
            logging.info("–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∂–æ–¥–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä—É –∑ —ñ—Å–Ω—É—é—á–∏–º —à—Ç—Ä–∏—Ö–∫–æ–¥–æ–º —É –±–∞–∑—ñ.")

        # --- 4. –ó–∞–ø–∏—Å –æ–Ω–æ–≤–ª–µ–Ω–æ–≥–æ 1.csv ---
        temp_path = sl_new_path + '.temp'
        with open(temp_path, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerows(items_to_keep)
        os.replace(temp_path, sl_new_path)
        logging.info(f"1.csv –æ–Ω–æ–≤–ª–µ–Ω–æ. –ó–∞–ª–∏—à–∏–ª–æ—Å—å {len(items_to_keep)-1} –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ –¥–ª—è —ñ–º–ø–æ—Ä—Ç—É.")

    except Exception as e:
        logging.error(f"–ù–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –æ–±—Ä–æ–±–∫–∏ 1.csv: {e}")
        if 'temp_path' in locals() and os.path.exists(temp_path):
            os.remove(temp_path)

def assign_new_sku_to_products():
    """
    –ó–Ω–∞—Ö–æ–¥–∏—Ç—å –Ω–∞–π–±—ñ–ª—å—à–∏–π SKU —É zalishki.csv (—Å–æ—Ä—Ç—É—î –ø–æ –∫–æ–ª–æ–Ω—Ü—ñ B(1))
    —ñ –ø—Ä–∏—Å–≤–æ—é—î –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ SKU —Ç–æ–≤–∞—Ä–∞–º –±–µ–∑ SKU —É –∫–æ–ª–æ–Ω—Ü—ñ P(15) —Ñ–∞–π–ª—É 1.csv.
    """
    log_message_to_existing_file()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 8. –ü–æ—á–∏–Ω–∞—é –ø—Ä–∏—Å–≤–æ—î–Ω–Ω—è –Ω–æ–≤–∏—Ö SKU —Ç–æ–≤–∞—Ä–∞–º —É 1.csv...")

    # --- 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å ---
    settings = load_settings()
    try:
        sl_new_path = settings['paths']['csv_path_supliers_1_new']
        zalishki_path = settings['paths']['csv_path_zalishki']
    except KeyError as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó. –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —à–ª—è—Ö: {e}")
        return

    # --- 2. –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è —ñ–Ω–¥–µ–∫—Å—É SKU —É 1.csv ---
    SKU_COL_INDEX = 15  # P
    ZALISHKI_SKU_INDEX = 1  # B

    # --- 3. –ó–Ω–∞—Ö–æ–¥–∏–º–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π SKU —É zalishki.csv ---
    try:
        with open(zalishki_path, mode='r', encoding='utf-8') as f:
            reader = csv.reader(f)
            next(reader, None)  # –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫
            sku_list = []
            for row in reader:
                if len(row) > ZALISHKI_SKU_INDEX:
                    val = row[ZALISHKI_SKU_INDEX].strip()
                    if val.isdigit():
                        sku_list.append(int(val))

            if not sku_list:
                logging.warning("–£ –±–∞–∑—ñ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∂–æ–¥–Ω–æ–≥–æ —á–∏—Å–ª–æ–≤–æ–≥–æ SKU. –ü—Ä–∏—Å–≤–æ—î–Ω–Ω—è –Ω–µ–º–æ–∂–ª–∏–≤–µ.")
                return

            sku_list.sort()
            last_sku = sku_list[-1]
            logging.info(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π SKU —É –±–∞–∑—ñ: {last_sku}")

    except FileNotFoundError:
        logging.error(f"–§–∞–π–ª –±–∞–∑–∏ zalishki.csv –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∑–∞ —à–ª—è—Ö–æ–º: {zalishki_path}")
        return
    except Exception as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —á–∏—Ç–∞–Ω–Ω—ñ zalishki.csv: {e}")
        return

    # --- 4. –ü—Ä–∏—Å–≤–æ—î–Ω–Ω—è –Ω–æ–≤–∏—Ö SKU —É 1.csv ---
    next_sku = last_sku + 1
    assigned_count = 0
    temp_path = sl_new_path + '.temp'

    try:
        with open(sl_new_path, mode='r', encoding='utf-8', newline='') as input_file:
            reader = csv.reader(input_file)
            header = next(reader, None)
            rows = [header] if header else []

            for row in reader:
                if len(row) <= SKU_COL_INDEX:
                    row.extend([''] * (SKU_COL_INDEX + 1 - len(row)))

                current_sku = row[SKU_COL_INDEX].strip()
                if not current_sku:
                    row[SKU_COL_INDEX] = str(next_sku)
                    assigned_count += 1
                    next_sku += 1

                rows.append(row)

        # --- 5. –ó–∞–ø–∏—Å –æ–Ω–æ–≤–ª–µ–Ω–æ–≥–æ CSV ---
        if assigned_count > 0:
            with open(temp_path, mode='w', encoding='utf-8', newline='') as f:
                writer = csv.writer(f)
                writer.writerows(rows)
            os.replace(temp_path, sl_new_path)
            logging.info(f"‚úÖ –£—Å–ø—ñ—à–Ω–æ –ø—Ä–∏—Å–≤–æ—î–Ω–æ {assigned_count} –Ω–æ–≤–∏—Ö SKU. –ù–∞—Å—Ç—É–ø–Ω–∏–π SKU –±—É–¥–µ {next_sku}.")
        else:
            logging.info("–£—Å—ñ —Ç–æ–≤–∞—Ä–∏ –≤–∂–µ –º–∞—é—Ç—å SKU. –ó–º—ñ–Ω –Ω–µ –≤–Ω–µ—Å–µ–Ω–æ.")

    except FileNotFoundError:
        logging.error(f"–§–∞–π–ª 1.csv –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∑–∞ —à–ª—è—Ö–æ–º")
    except Exception as e:
        logging.error(f"–ù–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –ø—Ä–∏—Å–≤–æ—î–Ω–Ω—è SKU: {e}")
        if os.path.exists(temp_path):
            os.remove(temp_path)

def download_images_for_product():
    """
    6 –µ—Ç–∞–ø—ñ–≤:
      1. –û—á–∏—Å—Ç–∫–∞ –ø–∞–ø–æ–∫ JPG —Ç–∞ WEBP
      2. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω—å —É –ø–∞–ø–∫—É JPG
      3. –ü–µ—Ä–µ–º—ñ—â–µ–Ω–Ω—è GIF —É WEBP
      4. –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è JPG —É WEBP
      5. –û–Ω–æ–≤–ª–µ–Ω–Ω—è CSV
      6. –ö–æ–ø—ñ—é–≤–∞–Ω–Ω—è –Ω–∞ —Å–∞–π—Ç
    """
    log_message_to_existing_file()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 9. –ü–æ—á–∞—Ç–æ–∫ –ø—Ä–æ—Ü–µ—Å—É –æ–±—Ä–æ–±–∫–∏ –∑–æ–±—Ä–∞–∂–µ–Ω—å...")

    settings = load_settings()
    try:
        sl_new = settings['paths']['csv_path_supliers_1_new']
        jpg_path = settings['paths']['img_path_jpg']
        webp_path = settings['paths']['img_path_webp']
        cat_map = settings['categories']
        site_path = settings['paths']['site_path_images']
    except KeyError as e:
        logging.error(f"‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —à–ª—è—Ö —É settings.json: {e}")
        return

    URL, SKU, CAT, IMG_LIST, WEBP_LIST = 1, 15, 16, 17, 18

    # 1Ô∏è‚É£ –û—á–∏—Å—Ç–∫–∞
    clear_directory(jpg_path)
    clear_directory(webp_path)

    # 2Ô∏è‚É£ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
    rows = []
    with open(sl_new, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        header = next(reader)
        rows.append(header)
        for row in reader:
            if len(row) <= IMG_LIST:
                row.extend([''] * (IMG_LIST - len(row) + 1))
            url, sku, cat = row[URL].strip(), row[SKU].strip(), row[CAT].strip()
            if url and sku and cat:
                imgs = download_product_images(url, sku, cat, jpg_path, cat_map)
                row[IMG_LIST] = ', '.join(imgs) if imgs else ''
            rows.append(row)
            time.sleep(random.uniform(0.5, 1.5))

    with open(sl_new, 'w', encoding='utf-8', newline='') as f:
        csv.writer(f).writerows(rows)
    logging.info(f"üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω—å –∑–∞–≤–µ—Ä—à–µ–Ω–æ ({len(rows)-1} —Ä—è–¥–∫—ñ–≤).")

    # 3Ô∏è‚É£ GIF
    move_gifs(jpg_path, webp_path)

    # 4Ô∏è‚É£ WEBP
    convert_to_webp_square(jpg_path, webp_path)

    # 5Ô∏è‚É£ CSV sync
    sync_webp_column(sl_new, webp_path, WEBP_LIST, SKU)

    # 6Ô∏è‚É£ –ö–æ–ø—ñ—é–≤–∞–Ω–Ω—è
    copy_to_site(webp_path, site_path)

    logging.info("‚úÖ –£—Å—ñ 6 –µ—Ç–∞–ø—ñ–≤ –æ–±—Ä–æ–±–∫–∏ –∑–æ–±—Ä–∞–∂–µ–Ω—å –≤–∏–∫–æ–Ω–∞–Ω–æ —É—Å–ø—ñ—à–Ω–æ.")

def create_new_products_import_file():
    """
    –°—Ç–≤–æ—Ä—é—î –æ–Ω–æ–≤–ª–µ–Ω–∏–π —Ñ–∞–π–ª `new_prod.csv` –¥–ª—è —ñ–º–ø–æ—Ä—Ç—É –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤.

    üß© –õ–æ–≥—ñ–∫–∞ —Ä–æ–±–æ—Ç–∏:
    1Ô∏è‚É£ –ó—á–∏—Ç—É—î –ø–æ—Ç–æ—á–Ω–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ñ–∞–π–ª—É new_prod.csv (–≤—ñ–Ω –ó–ê–õ–ò–®–ê–Ñ–¢–¨–°–Ø –±–µ–∑ –∑–º—ñ–Ω).
    2Ô∏è‚É£ –û—á–∏—â—É—î —Ñ–∞–π–ª –≤—ñ–¥ —Å—Ç–∞—Ä–∏—Ö –¥–∞–Ω–∏—Ö.
    3Ô∏è‚É£ –ó—á–∏—Ç—É—î new.csv.
    4Ô∏è‚É£ –ü–µ—Ä–µ–Ω–æ—Å–∏—Ç—å –¥–∞–Ω—ñ –∑–≥—ñ–¥–Ω–æ –∑ COLUMN_MAP.
    5Ô∏è‚É£ –ó–∞–ø–∏—Å—É—î –æ–Ω–æ–≤–ª–µ–Ω—ñ –¥–∞–Ω—ñ —É new_prod.csv, –∑–∞–ª–∏—à–∞—é—á–∏ —Å—Ç–∞—Ä–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫.

    ‚ö†Ô∏è –Ø–∫—â–æ —Ñ–∞–π–ª—É new_prod.csv —â–µ –Ω–µ–º–∞—î, –π–æ–≥–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ —Å—Ç–≤–æ—Ä–∏—Ç–∏ –≤—Ä—É—á–Ω—É –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏.
    """

    # -----------------------------------------------------------
    # üî¢ –ú–∞–ø–∞ –∫–æ–ª–æ–Ω–æ–∫: {—ñ–Ω–¥–µ–∫—Å —É new.csv ‚Üí —ñ–Ω–¥–µ–∫—Å —É new_prod.csv}
    # -----------------------------------------------------------
    COLUMN_MAP = {
        15: 0, 1: 1, 2: 2, 5: 3, 6: 4, 7: 5, 8: 6, 9: 7, 16: 8, 18: 9,
        19: 10, 20: 11, 21: 12, 22: 13, 23: 14, 24: 15, 25: 16, 26: 17,
        27: 18, 28: 19, 29: 20, 30: 21, 31: 22, 32: 23, 33: 24, 34: 25,
        35: 26, 36: 27, 37: 28, 38: 29, 39: 30, 40: 31, 41: 32, 42: 33,
        43: 34, 44: 35, 45: 36, 46: 37, 47: 38, 48: 39, 49: 40, 51: 41
    }
    log_message_to_existing_file()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 10. –ü–æ—á–∏–Ω–∞—é —Ñ–æ—Ä–º—É–≤–∞–Ω–Ω—è —Ñ–∞–π–ª—É new_prod.csv –¥–ª—è —ñ–º–ø–æ—Ä—Ç—É –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤.")

    # -----------------------------------------------------------
    # üß© –ö—Ä–æ–∫ 1: –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å —ñ —à–ª—è—Ö—ñ–≤
    # -----------------------------------------------------------
    settings = load_settings()
    try:
        sl_new_path = settings['paths']['csv_path_supliers_1_new']
        sl_new_prod_path = settings['paths']['csv_path_sl_new_prod']
    except KeyError as e:
        logging.critical(f"‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —à–ª—è—Ö —É settings.json: {e}")
        return

    temp_file = sl_new_prod_path + '.temp'

    # -----------------------------------------------------------
    # üìÑ –ö—Ä–æ–∫ 2: –ó—á–∏—Ç—É—î–º–æ —ñ—Å–Ω—É—é—á–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ new_prod.csv
    # -----------------------------------------------------------
    if not os.path.exists(sl_new_prod_path):
        logging.critical(
            f"‚ö†Ô∏è –§–∞–π–ª new_prod_path –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ!\n"
            "–°—Ç–≤–æ—Ä—ñ—Ç—å –π–æ–≥–æ –≤—Ä—É—á–Ω—É –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º."
        )
        return

    try:
        with open(sl_new_prod_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            header = next(reader, None)
            if not header:
                logging.critical(f"‚ùå –£ —Ñ–∞–π–ª—ñ {sl_new_prod_path} –≤—ñ–¥—Å—É—Ç–Ω—ñ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫.")
                return
        logging.info("‚úÖ –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∑–±–µ—Ä–µ–∂–µ–Ω–æ, —Å—Ç–∞—Ä—ñ –¥–∞–Ω—ñ –±—É–¥–µ –æ—á–∏—â–µ–Ω–æ.")
    except Exception as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —á–∏—Ç–∞–Ω–Ω—ñ –∑–∞–≥–æ–ª–æ–≤–∫–∞: {e}", exc_info=True)
        return

    # -----------------------------------------------------------
    # üì¶ –ö—Ä–æ–∫ 3: –ó—á–∏—Ç—É—î–º–æ SL_new.csv
    # -----------------------------------------------------------
    if not os.path.exists(sl_new_path):
        logging.critical(f"‚ùå –§–∞–π–ª new_path –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
        return

    try:
        with open(sl_new_path, 'r', encoding='utf-8') as f:
            reader = list(csv.reader(f))
    except Exception as e:
        logging.critical(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —á–∏—Ç–∞–Ω–Ω—ñ new_path: {e}", exc_info=True)
        return

    if len(reader) <= 1:
        logging.warning("‚ö†Ô∏è SL_new.csv –ø–æ—Ä–æ–∂–Ω—ñ–π –∞–±–æ –º—ñ—Å—Ç–∏—Ç—å –ª–∏—à–µ –∑–∞–≥–æ–ª–æ–≤–æ–∫.")
        # –ó–∞–ø–∏—Å—É—î–º–æ –ª–∏—à–µ –∑–∞–≥–æ–ª–æ–≤–æ–∫ —É –Ω–æ–≤–∏–π —Ñ–∞–π–ª
        with open(sl_new_prod_path, 'w', encoding='utf-8', newline='') as f:
            csv.writer(f).writerow(header)
        return

    source_rows = reader[1:]  # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫

    # -----------------------------------------------------------
    # üîÑ –ö—Ä–æ–∫ 4: –§–æ—Ä–º—É—î–º–æ –Ω–æ–≤—ñ —Ä—è–¥–∫–∏ –∑–≥—ñ–¥–Ω–æ –∑ –º–∞–ø–æ—é COLUMN_MAP
    # -----------------------------------------------------------
    rows_to_write = [header]
    processed = 0
    max_src = max(COLUMN_MAP.keys())

    for i, src_row in enumerate(source_rows, start=2):  # —Ä—è–¥–∫–∏ –ø–æ—á–∏–Ω–∞—é—Ç—å—Å—è –∑ 2 (1 ‚Äî –∑–∞–≥–æ–ª–æ–≤–æ–∫)
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –¥–æ–≤–∂–∏–Ω—É —Ä—è–¥–∫–∞ (—â–æ–± —É–Ω–∏–∫–Ω—É—Ç–∏ IndexError)
        if len(src_row) <= max_src:
            logging.warning(f"‚ö†Ô∏è –†—è–¥–æ–∫ {i}: –ø—Ä–æ–ø—É—â–µ–Ω–æ ‚Äî –Ω–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –∫–æ–ª–æ–Ω–æ–∫ ({len(src_row)}/{max_src+1}).")
            continue

        # –°—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ–≤–∏–π —Ä—è–¥–æ–∫ —ñ–∑ —Ç–∞–∫–æ—é —Å–∞–º–æ—é –∫—ñ–ª—å–∫—ñ—Å—Ç—é –∫–æ–ª–æ–Ω–æ–∫, —è–∫ —É –∑–∞–≥–æ–ª–æ–≤–∫—É
        tgt_row = [''] * len(header)

        # –ü–µ—Ä–µ–Ω–æ—Å–∏–º–æ –∑–Ω–∞—á–µ–Ω–Ω—è –∑–≥—ñ–¥–Ω–æ –∑ COLUMN_MAP
        for src_idx, tgt_idx in COLUMN_MAP.items():
            if tgt_idx < len(tgt_row):
                tgt_row[tgt_idx] = src_row[src_idx].strip()

        rows_to_write.append(tgt_row)
        processed += 1

    logging.info(f"üîÅ –û–±—Ä–æ–±–ª–µ–Ω–æ {processed} —Ä—è–¥–∫—ñ–≤ –¥–ª—è –∑–∞–ø–∏—Å—É —É new_prod.csv.")

    # -----------------------------------------------------------
    # üíæ –ö—Ä–æ–∫ 5: –ó–∞–ø–∏—Å —É —Ñ–∞–π–ª (–æ—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä–∏—Ö –¥–∞–Ω–∏—Ö, –∞–ª–µ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∑–±–µ—Ä—ñ–≥–∞—î—Ç—å—Å—è)
    # -----------------------------------------------------------
    try:
        with open(temp_file, 'w', encoding='utf-8', newline='') as f:
            csv.writer(f).writerows(rows_to_write)
        os.replace(temp_file, sl_new_prod_path)
        logging.info(f"‚úÖ –§–∞–π–ª new_prod_path –æ–Ω–æ–≤–ª–µ–Ω–æ ({processed} —Ä—è–¥–∫—ñ–≤).")
    except Exception as e:
        logging.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Å—É new_prod_path: {e}", exc_info=True)
        if os.path.exists(temp_file):
            os.remove(temp_file)

def update_existing_products_batch():
    """
    –û–Ω–æ–≤–ª—é—î —ñ—Å–Ω—É—é—á—ñ —Ç–æ–≤–∞—Ä–∏ —É WooCommerce –Ω–∞ –æ—Å–Ω–æ–≤—ñ CSV-—Ñ–∞–π–ª—É old_prod_new_SHK.csv.

    –§—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª:
    1. –ó—á–∏—Ç—É—î –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Ç–∞ –ø—ñ–¥–∫–ª—é—á–∞—î—Ç—å—Å—è –¥–æ WooCommerce API.
    2. –ß–∏—Ç–∞—î CSV —Ç–∞ —Ñ–æ—Ä–º—É—î —Å–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä—ñ–≤ –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è.
    3. –î–ª—è –∫–æ–∂–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä—É:
       - –ü–µ—Ä–µ–≤—ñ—Ä—è—î ID
       - –ó–±–∏—Ä–∞—î —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ñ –ø–æ–ª—è (sku, post_date, product_type, tax_status)
       - –§–æ—Ä–º—É—î meta_data (ACF, Rank Math)
       - –û–±—Ä–æ–±–ª—è—î –∞—Ç—Ä–∏–±—É—Ç–∏:
            * –∑–±–µ—Ä—ñ–≥–∞—î —Å—Ç–∞—Ä—ñ –∞—Ç—Ä–∏–±—É—Ç–∏, —è–∫–∏—Ö –Ω–µ–º–∞—î –≤ CSV
            * –æ–Ω–æ–≤–ª—é—î —ñ—Å–Ω—É—é—á—ñ —Ç–∞ –¥–æ–¥–∞—î –Ω–æ–≤—ñ
            * –≥–ª–æ–±–∞–ª—å–Ω–∏–π –∞—Ç—Ä–∏–±—É—Ç pa_manufacturer –ø—Ä–∏–≤'—è–∑—É—î—Ç—å—Å—è –¥–æ WooCommerce
       - –û–Ω–æ–≤–ª—é—î —Ç–µ–≥–∏ (tags)
       - –û–Ω–æ–≤–ª—é—î excerpt —Ç–∞ content
    4. –ù–∞–¥—Å–∏–ª–∞—î —Ç–æ–≤–∞—Ä–∏ –ø–∞–∫–µ—Ç–∞–º–∏ –ø–æ BATCH_SIZE.
    5. –õ–æ–≥—É—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ç–∞ –ø–æ–º–∏–ª–∫–∏.
    """

    log_message_to_existing_file()
    logging.info("–§–£–ù–ö–¶–Ü–Ø 11. –ü–æ—á–∏–Ω–∞—é –ø–∞–∫–µ—Ç–Ω–µ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è —ñ—Å–Ω—É—é—á–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ –∑ old_prod_new_SHK.csv...")

    # --- –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å ---
    settings = load_settings()
    if not settings:
        logging.critical("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è.")
        return

    try:
        csv_path = settings['paths']['csv_path_sl_old_prod_new_shk']
    except KeyError:
        logging.error("‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —à–ª—è—Ö –¥–æ CSV —É –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è—Ö.")
        return

    # --- –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ WooCommerce API ---
    wcapi = get_wc_api(settings)
    if not wcapi:
        logging.critical("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è —Å—Ç–≤–æ—Ä–∏—Ç–∏ –æ–±'—î–∫—Ç WooCommerce API.")
        return

    # --- –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è ---
    BATCH_SIZE = 50
    products_to_update = []
    total_products_read = 0
    total_updated = 0
    total_skipped = 0
    errors_list = []
    start_time = time.time()

    try:
        with open(csv_path, mode='r', encoding='utf-8') as f:
            reader = csv.reader(f)
            headers = next(reader)
            logging.info(f"–ó—á–∏—Ç–∞–Ω–æ {len(headers)} –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤: {', '.join(headers[:5])}...")

            STANDARD_FIELDS = ['sku', 'post_date', 'product_type', 'tax_status']
            ACF_PREFIX = '–ú–µ—Ç–∞: '
            ATTRIBUTE_PREFIX = 'attribute:'
            field_map = {header: idx for idx, header in enumerate(headers)}

            for row in reader:
                total_products_read += 1

                # --- –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ ID ---
                product_id_str = row[field_map.get('id', -1)].strip()
                if not product_id_str.isdigit():
                    errors_list.append(f"–†—è–¥–æ–∫ {total_products_read}: –ü—Ä–æ–ø—É—â–µ–Ω–æ. –ù–µ–∫–æ—Ä–µ–∫—Ç–Ω–∏–π ID.")
                    total_skipped += 1
                    continue

                product_id = int(product_id_str)
                product_data = {"id": product_id}
                meta_data = []
                new_attributes = []
                tags = []

                # --- –ü—Ä–æ—Ö–æ–¥–∏–º–æ –ø–æ –≤—Å—ñ—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö ---
                for key, index in field_map.items():
                    if index >= len(row):
                        continue
                    value = row[index].strip()
                    if not value:
                        continue

                    # --- –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ñ –ø–æ–ª—è ---
                    if key in STANDARD_FIELDS:
                        if key == 'post_date':
                            product_data['date_created'] = value
                            try:
                                dt = datetime.fromisoformat(value)
                                product_data['date_created_gmt'] = (dt - timedelta(hours=3)).isoformat()
                            except ValueError:
                                errors_list.append(
                                    f"‚ö†Ô∏è –†—è–¥–æ–∫ {total_products_read}: –Ω–µ–∫–æ—Ä–µ–∫—Ç–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç post_date '{value}'"
                                )
                        else:
                            product_data[key] = value

                    # --- Meta Data ---
                    elif key.startswith(ACF_PREFIX) or key == 'rank_math_focus_keyword':
                        meta_key = key.replace(ACF_PREFIX, '') if key.startswith(ACF_PREFIX) else key
                        meta_data.append({"key": meta_key, "value": value})

                    # --- –¢–µ–≥–∏ ---
                    elif key == '–ü–æ–∑–Ω–∞—á–∫–∏':
                        tag_names = [t.strip() for t in value.split(',') if t.strip()]
                        tags.extend([{"name": t} for t in tag_names])

                    # --- –ù–µ –¥–æ–¥–∞—é –ö–æ—Ä–æ—Ç–∫–∏–π —Ç–∞ –ø–æ–≤–Ω–∏–π –æ–ø–∏—Å ---
                    # elif key == 'excerpt':
                    #    product_data['excerpt'] = value
                    # elif key == 'content':
                    #    product_data['description'] = value  # WooCommerce API

                    # --- –ê—Ç—Ä–∏–±—É—Ç–∏ ---
                    elif key.startswith(ATTRIBUTE_PREFIX):
                        attr_name = key.replace(ATTRIBUTE_PREFIX, '')
                        options = [v.strip() for v in value.split(',') if v.strip()]
                        if options:
                            attr_dict = {
                                "name": attr_name,
                                "position": len(new_attributes),
                                "visible": True,
                                "variation": False,
                                "options": options
                            }
                            # –ì–ª–æ–±–∞–ª—å–Ω–∏–π –∞—Ç—Ä–∏–±—É—Ç manufacturer
                            if attr_name == "pa_manufacturer":
                                attr_dict["id"] = 1  # ID –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –∞—Ç—Ä–∏–±—É—Ç–∞ manufacturer —É WooCommerce
                            new_attributes.append(attr_dict)

                # --- –î–æ–¥–∞—î–º–æ meta_data —Ç–∞ —Ç–µ–≥–∏ –¥–æ product_data ---
                if meta_data:
                    product_data['meta_data'] = meta_data
                if tags:
                    product_data['tags'] = tags

                # --- –û–±—Ä–æ–±–∫–∞ –∞—Ç—Ä–∏–±—É—Ç—ñ–≤: merge –∑ —ñ—Å–Ω—É—é—á–∏–º–∏ ---
                if new_attributes:
                    try:
                        # –û—Ç—Ä–∏–º—É—î–º–æ —ñ—Å–Ω—É—é—á—ñ –∞—Ç—Ä–∏–±—É—Ç–∏ —Ç–æ–≤–∞—Ä—É
                        existing_attributes = wcapi.get(f"products/{product_id}").json().get("attributes", [])
                        attr_map = {attr['name']: attr for attr in existing_attributes}

                        # –û–Ω–æ–≤–ª—é—î–º–æ —ñ—Å–Ω—É—é—á—ñ –∞–±–æ –¥–æ–¥–∞—î–º–æ –Ω–æ–≤—ñ
                        for new_attr in new_attributes:
                            name = new_attr['name']
                            if name in attr_map:
                                attr_map[name]['options'] = new_attr['options']
                                attr_map[name]['position'] = new_attr['position']
                                attr_map[name]['visible'] = new_attr['visible']
                                attr_map[name]['variation'] = new_attr['variation']
                                if 'id' in new_attr:
                                    attr_map[name]['id'] = new_attr['id']
                            else:
                                attr_map[name] = new_attr

                        product_data['attributes'] = list(attr_map.values())
                    except Exception as e:
                        logging.error(f"–†—è–¥–æ–∫ {total_products_read}: –ü–æ–º–∏–ª–∫–∞ merge –∞—Ç—Ä–∏–±—É—Ç—ñ–≤: {e}")

                products_to_update.append(product_data)

                # --- –ù–∞–¥—Å–∏–ª–∞–Ω–Ω—è –ø–∞–∫–µ—Ç—É –Ω–∞ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è ---
                if len(products_to_update) >= BATCH_SIZE:
                    total_updated += _process_batch_update(wcapi, products_to_update, errors_list)
                    products_to_update = []

            # --- –û–±—Ä–æ–±–∫–∞ –∑–∞–ª–∏—à–∫—É ---
            if products_to_update:
                total_updated += _process_batch_update(wcapi, products_to_update, errors_list)

    except Exception as e:
        logging.critical(f"‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—Ä–æ–±—Ü—ñ CSV: {e}", exc_info=True)
        return

    # --- –ü—ñ–¥—Å—É–º–æ–∫ ---
    elapsed_time = int(time.time() - start_time)
    logging.info("--- üèÅ –ü—ñ–¥—Å—É–º–æ–∫ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è —ñ—Å–Ω—É—é—á–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ ---")
    logging.info(f"–í—Å—å–æ–≥–æ —Ä—è–¥–∫—ñ–≤: {total_products_read}")
    logging.info(f"–£—Å–ø—ñ—à–Ω–æ –æ–Ω–æ–≤–ª–µ–Ω–æ: {total_updated}")
    logging.info(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ/–∑ –ø–æ–º–∏–ª–∫–∞–º–∏: {total_products_read - total_updated}")
    logging.info(f"–ó–∞–≥–∞–ª—å–Ω–∞ —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å: {elapsed_time} —Å–µ–∫.")

    if errors_list:
        logging.warning(f"‚ö†Ô∏è –ó–Ω–∞–π–¥–µ–Ω–æ {len(errors_list)} –ø–æ–º–∏–ª–æ–∫. –ü–µ—Ä—à—ñ 5:")
        for err in errors_list[:5]:
            logging.warning(f"-> {err}")
    else:
        logging.info("‚úÖ –û–Ω–æ–≤–ª–µ–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ –±–µ–∑ –ø–æ–º–∏–ª–æ–∫.")

def create_new_products_batch():
    """–ü–∞–∫–µ—Ç–Ω–µ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ —É WooCommerce –∑ CSV."""
    log_message_to_existing_file()
    logging.info("üöÄ –ü–æ—á–∏–Ω–∞—é –ø–∞–∫–µ—Ç–Ω–µ –°–¢–í–û–†–ï–ù–ù–Ø –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ –∑ SL_new_prod.csv...")

    settings = load_settings()
    if not settings:
        logging.critical("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è.")
        return

    try:
        csv_path = settings['paths']['csv_path_sl_new_prod']
        uploads_path = '/var/www/html/erosinua/public_html/wp-content/uploads/products'
    except KeyError as e:
        logging.error(f"‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —à–ª—è—Ö –¥–æ CSV –∞–±–æ uploads_path: {e}")
        return

    wcapi = get_wc_api(settings)
    if not wcapi:
        logging.critical("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è —Å—Ç–≤–æ—Ä–∏—Ç–∏ –æ–±'—î–∫—Ç WooCommerce API.")
        return

    BATCH_SIZE = 50
    products_to_create: List[Dict[str, Any]] = []
    total_products_read = 0
    total_created = 0
    total_skipped = 0
    errors_list: List[str] = []
    start_time = time.time()

    STANDARD_FIELDS = ['sku', 'post_date', 'excerpt', 'content', 'product_type']
    ACF_PREFIX = '–ú–µ—Ç–∞: '
    ATTRIBUTE_PREFIX = 'attribute:'

    try:
        with open(csv_path, mode='r', encoding='utf-8') as f:
            reader = csv.reader(f)
            headers = next(reader)
            field_map = {header: idx for idx, header in enumerate(headers)}

            for row in reader:
                total_products_read += 1
                sku = row[field_map.get('sku', -1)].strip()
                name = row[field_map.get('name', -1)].strip()

                if not sku or not name:
                    errors_list.append(f"–†—è–¥–æ–∫ {total_products_read}: –ü—Ä–æ–ø—É—â–µ–Ω–æ. –í—ñ–¥—Å—É—Ç–Ω—ñ–π SKU –∞–±–æ Name.")
                    total_skipped += 1
                    continue

                product_data: Dict[str, Any] = {"sku": sku, "name": name, "status": "draft"}
                meta_data: List[Dict[str, Any]] = []
                attributes: List[Dict[str, Any]] = []
                tags: List[Dict[str, Any]] = []
                images: List[Dict[str, Any]] = []

                for key, index in field_map.items():
                    if index >= len(row):
                        continue
                    value = row[index].strip()
                    if not value:
                        continue

                    # --- —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ñ –ø–æ–ª—è ---
                    if key in STANDARD_FIELDS:
                        if key == 'product_type':
                            product_data['type'] = value or 'simple'
                        elif key == 'excerpt':
                            product_data['short_description'] = value
                        elif key == 'content':
                            product_data['description'] = value
                        elif key == 'post_date':
                            product_data['date_created'] = value
                    # --- meta_data ---
                    elif key.startswith(ACF_PREFIX):
                        meta_data.append({"key": key.replace(ACF_PREFIX, ''), "value": value})
                    elif key == 'rank_math_focus_keyword':
                        meta_data.append({"key": key, "value": value})
                    # --- –∞—Ç—Ä–∏–±—É—Ç–∏ ---
                    elif key.startswith(ATTRIBUTE_PREFIX):
                        attr_name = key.replace(ATTRIBUTE_PREFIX, '')
                        options = [v.strip() for v in value.split(',') if v.strip()]
                        if options:
                            attr = {"name": attr_name, "position": len(attributes), "visible": True,
                                    "variation": False, "options": options}
                            if attr_name == "pa_manufacturer":
                                attr["id"] = 1
                            attributes.append(attr)
                    # --- –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó ---
                    elif key == 'categories':
                        category_names = [c.strip() for c in value.split('|') if c.strip()]
                        if category_names:
                            product_data['categories'] = [{"name": c} for c in category_names]
                    # --- —Ç–µ–≥–∏ ---
                    elif key == '–ü–æ–∑–Ω–∞—á–∫–∏':
                        tags.extend([{"name": t.strip()} for t in value.split(',') if t.strip()])
                    # --- –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è ---
                    elif key == 'image_name':
                        images = find_media_ids_for_sku(wcapi, sku, uploads_path)
                    # --- —Ü—ñ–Ω–∞, –∑–∞–ø–∞—Å–∏, –æ–ø–æ–¥–∞—Ç–∫—É–≤–∞–Ω–Ω—è, —Å—Ç–∞—Ç—É—Å ---
                    elif key == 'regular_price':
                        product_data['regular_price'] = str(value)
                    elif key == 'manage_stock':
                        product_data['manage_stock'] = value.strip() in ['1', 'yes', 'true']
                    elif key == 'tax_status':
                        product_data['tax_status'] = 'taxable' if value.strip() in ['1', 'yes', 'true'] else 'none'
                    elif key == 'status':
                        product_data['status'] = 'publish' if value.strip() in ['1', 'yes', 'true'] else 'draft'
                    else:
                        product_data[key] = value

                # --- –¥–æ–¥–∞–≤–∞–Ω–Ω—è —Å–ø–∏—Å–∫—ñ–≤ —É product_data ---
                if meta_data:
                    product_data['meta_data'] = meta_data
                if attributes:
                    product_data['attributes'] = attributes
                if tags:
                    product_data['tags'] = tags
                if images:
                    product_data['images'] = images

                products_to_create.append(product_data)

                # --- –ø–∞–∫–µ—Ç–Ω–∞ –≤—ñ–¥–ø—Ä–∞–≤–∫–∞ ---
                if len(products_to_create) >= BATCH_SIZE:
                    total_created += _process_batch_create(wcapi, products_to_create, errors_list)
                    products_to_create = []

            # --- –æ–±—Ä–æ–±–∫–∞ –∑–∞–ª–∏—à–∫—É ---
            if products_to_create:
                total_created += _process_batch_create(wcapi, products_to_create, errors_list)

    except FileNotFoundError:
        logging.critical(f"‚ùå –§–∞–π–ª {csv_path} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
        return
    except Exception as e:
        logging.critical(f"‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —á–∏—Ç–∞–Ω–Ω—ñ CSV: {e}", exc_info=True)
        return

    # --- –ø—ñ–¥—Å—É–º–æ–∫ ---
    elapsed_time = int(time.time() - start_time)
    logging.info("--- üèÅ –ü—ñ–¥—Å—É–º–æ–∫ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ ---")
    logging.info(f"–í—Å—å–æ–≥–æ –ø—Ä–æ—á–∏—Ç–∞–Ω–æ —Ä—è–¥–∫—ñ–≤: {total_products_read}")
    logging.info(f"–£—Å–ø—ñ—à–Ω–æ —Å—Ç–≤–æ—Ä–µ–Ω–æ —Ç–æ–≤–∞—Ä—ñ–≤: {total_created}")
    logging.info(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ/–∑ –ø–æ–º–∏–ª–∫–∞–º–∏: {total_products_read - total_created}")
    logging.info(f"–ó–∞–≥–∞–ª—å–Ω–∞ —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å: {elapsed_time} —Å–µ–∫.")

    if errors_list:
        logging.warning(f"‚ö†Ô∏è –ó–Ω–∞–π–¥–µ–Ω–æ {len(errors_list)} –ø–æ–º–∏–ª–æ–∫/–ø—Ä–æ–ø—É—Å–∫—ñ–≤. –ü–µ—Ä—à—ñ 5 –ø–æ–º–∏–ª–æ–∫:")
        for error in errors_list[:5]:
            logging.warning(f"-> {error}")
    else:
        logging.info("‚úÖ –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ.")